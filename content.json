{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://philia897.github.io","root":"/"},"pages":[{"title":"分类","date":"2025-02-25T03:17:22.228Z","updated":"2025-02-25T03:17:22.228Z","comments":false,"path":"categories/index.html","permalink":"http://philia897.github.io/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2025-02-25T03:17:22.240Z","updated":"2025-02-25T03:17:22.240Z","comments":false,"path":"repository/index.html","permalink":"http://philia897.github.io/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2025-02-25T03:17:22.233Z","updated":"2025-02-25T03:17:22.232Z","comments":true,"path":"links/index.html","permalink":"http://philia897.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2025-02-25T03:17:22.246Z","updated":"2025-02-25T03:17:22.246Z","comments":false,"path":"tags/index.html","permalink":"http://philia897.github.io/tags/index.html","excerpt":"","text":""},{"title":"关于 About","date":"2025-02-25T04:52:12.069Z","updated":"2025-02-25T04:52:12.069Z","comments":false,"path":"about/index.html","permalink":"http://philia897.github.io/about/index.html","excerpt":"","text":"详情请参见 简历 Please refer to the resume (updated on 2025&#x2F;02&#x2F;24):"},{"title":"","date":"2025-03-05T11:32:27.074Z","updated":"2025-03-05T11:32:27.074Z","comments":true,"path":"about/resume.html","permalink":"http://philia897.github.io/about/resume.html","excerpt":"","text":"pre code.hljs { display: block; overflow-x: auto; padding: 1em; } code.hljs { padding: 3px 5px; } /* Atom One Light by Daniel Gamage Original One Light Syntax theme from https://github.com/atom/one-light-syntax base: #fafafa mono-1: #383a42 mono-2: #686b77 mono-3: #a0a1a7 hue-1: #0184bb hue-2: #4078f2 hue-3: #a626a4 hue-4: #50a14f hue-5: #e45649 hue-5-2: #c91243 hue-6: #986801 hue-6-2: #c18401 */ .hljs { color: #383a42; background: #fafafa; } .hljs-comment, .hljs-quote { color: #a0a1a7; font-style: italic; } .hljs-doctag, .hljs-keyword, .hljs-formula { color: #a626a4; } .hljs-section, .hljs-name, .hljs-selector-tag, .hljs-deletion, .hljs-subst { color: #e45649; } .hljs-literal { color: #0184bb; } .hljs-string, .hljs-regexp, .hljs-addition, .hljs-attribute, .hljs-meta .hljs-string { color: #50a14f; } .hljs-attr, .hljs-variable, .hljs-template-variable, .hljs-type, .hljs-selector-class, .hljs-selector-attr, .hljs-selector-pseudo, .hljs-number { color: #986801; } .hljs-symbol, .hljs-bullet, .hljs-link, .hljs-meta, .hljs-selector-id, .hljs-title { color: #4078f2; } .hljs-built_in, .hljs-title.class_, .hljs-class .hljs-title { color: #c18401; } .hljs-emphasis { font-style: italic; } .hljs-strong { font-weight: bold; } .hljs-link { text-decoration: underline; } Presonal Resume Presonal Resume /* https://necolas.github.io/normalize.css/ */ html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0} article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible} pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects} b,strong{font-weight:bolder}small{font-size:80%}img{border-style:none} body { font-size: 15px; color: #32373F; word-wrap: break-word; line-height: 1.6em; background-color: #ffffff; font-family: 'Avenir Next', 'Avenir', 'Arial', sans-serif; padding-bottom: 0px; padding-top: 0px; } kbd { border: 1px solid rgb(220, 220, 220); box-shadow: inset 0 -1px 0 rgb(220, 220, 220); padding: 2px 4px; border-radius: 3px; background-color: rgb(243, 243, 243); } ::-webkit-scrollbar { width: 7px; height: 7px; } ::-webkit-scrollbar-corner { background: none; } ::-webkit-scrollbar-track { border: none; } ::-webkit-scrollbar-thumb { background: rgba(100, 100, 100, 0.3); border-radius: 5px; } ::-webkit-scrollbar-track:hover { background: rgba(0, 0, 0, 0.1); } ::-webkit-scrollbar-thumb:hover { background: rgba(100, 100, 100, 0.7); } /* Remove top padding and margin from first child so that top of rendered text is aligned to top of text editor text */ #rendered-md > h1:first-child, #rendered-md > h2:first-child, #rendered-md > h3:first-child, #rendered-md > h4:first-child, #rendered-md > ul:first-child, #rendered-md > ol:first-child, #rendered-md > table:first-child, #rendered-md > blockquote:first-child, #rendered-md > img:first-child, #rendered-md > p:first-child { margin-top: 0; padding-top: 0; } p, h1, h2, h3, h4, h5, h6, ul, table { margin-top: .6em; margin-bottom: 1.35em; /* Adds support for RTL text in the note body. It automatically detects the direction using the content. Issue: https://github.com/laurent22/joplin/issues/3991 */ unicode-bidi: plaintext; } h1, h2, h3, h4, h5, h6, ul, table { margin-bottom: 0.65em; } h1, h2, h3, h4, h5, h6 { line-height: 1.5em; } h1 { font-size: 1.5em; font-weight: bold; border-bottom: 1px solid #dddddd; padding-bottom: .3em; } h2 { font-size: 1.3em; font-weight: bold; padding-bottom: .1em; */ } h3 { font-size: 1.1em; font-weight: bold; } h4, h5, h6 { font-size: 1em; font-weight: bold; } .exported-note-title { font-size: 2em; font-weight: bold; margin-bottom: 0.8em; line-height: 1.5em; padding-bottom: .35em; border-bottom: 1px solid #dddddd; } a { color: #155BDA; } ul, ol { padding-left: 0; margin-left: 1.7em; } li { margin-bottom: .4em; } li p { margin-top: 0.2em; margin-bottom: 0; } .resource-icon { display: inline-block; position: relative; top: 0.3em; text-decoration: none; width: 1.2em; height: 1.4em; margin-right: 0.4em; background-color: #155BDA; } /* These icons are obtained from the wonderful ForkAwesome project by copying the src svgs * into the css classes below. * svgs are obtained from https://github.com/ForkAwesome/Fork-Awesome/tree/master/src/icons/svg * instead of the svg width, height property you must use a viewbox here, 0 0 1536 1792 is typically the actual size of the icon * each line begins with the pre-amble -webkit-mask: url(\"data:image/svg+xml;utf8, * and of course finishes with \"); * to prevent artifacts it is also necessary to include -webkit-mask-repeat: no-repeat; * on the following line * */ .fa-joplin { /* Awesome Font file */ -webkit-mask: url(\"data:image/svg+xml;utf8,\"); } .fa-file-image { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-pdf { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-word { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-powerpoint { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-excel { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-audio { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-video { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-archive { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-code { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file-alt, .fa-file-csv { /* fork-awesome doesn't have csv so we use the text icon */ -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } .fa-file { -webkit-mask: url(\"data:image/svg+xml;utf8,\"); -webkit-mask-repeat: no-repeat; } blockquote { border-left: 4px solid rgb(220, 220, 220); padding-left: 1.2em; margin-left: 0; opacity: 0.7; } .jop-tinymce table, table { text-align: left; border-collapse: collapse; border: 1px solid rgb(220, 220, 220); background-color: #ffffff; } .jop-tinymce table td, .jop-tinymce table th, table td, th { text-align: left; padding: .5em 1em .5em 1em; font-size: 15; color: #32373F; font-family: 'Avenir Next', 'Avenir', 'Arial', sans-serif; } .jop-tinymce table td, table td { border: 1px solid rgb(220, 220, 220); } .jop-tinymce table th, table th { border: 1px solid rgb(220, 220, 220); border-bottom: 2px solid rgb(220, 220, 220); background-color: rgb(247, 247, 247); } .jop-tinymce table tr:nth-child(even), table tr:nth-child(even) { background-color: rgb(247, 247, 247); } .jop-tinymce table tr:hover, table tr:hover { background-color: #e5e5e5; } hr { border: none; border-bottom: 2px solid #dddddd; } img { max-width: 100%; height: auto; } .inline-code, .mce-content-body code { border: 1px solid rgb(220, 220, 220); background-color: rgb(243, 243, 243); padding-right: .2em; padding-left: .2em; border-radius: .25em; color: rgb(0,0,0); font-size: .9em; } .highlighted-keyword { background-color: #F3B717; color: black; } .not-loaded-resource img { width: 1.15em; height: 1.15em; background: white; padding: 2px !important; border-radius: 2px; box-shadow: 0 1px 3px #000000aa; } a.not-loaded-resource img { margin-right: .2em; } a.not-loaded-resource { display: flex; flex-direction: row; align-items: center; } .md-checkbox input[type=checkbox]:checked { opacity: 0.7; } .jop-tinymce ul.joplin-checklist .checked, .md-checkbox .checkbox-label-checked { opacity: 0.5; } .exported-note { padding: 1em; } .joplin-editable .joplin-source { display: none; } mark { background: #F7D26E; color: black; } /* =============================================== */ /* For TinyMCE */ /* =============================================== */ .mce-content-body { /* Note: we give a bit more padding at the bottom, to allow scrolling past the end of the document */ padding: 5px 10px 10em 0; } /* .mce-content-body code { background-color: transparent; } */ .mce-content-body [data-mce-selected=inline-boundary] { background-color: transparent; } .mce-content-body .joplin-editable { cursor: pointer !important; } .mce-content-body.mce-content-readonly { opacity: 0.5; } /* We need that to make sure click events have the A has a target */ .katex a span { pointer-events: none; } .media-player { width: 100%; margin-top: 10px; } .media-player.media-pdf { min-height: 35rem; width: 100%; max-width: 1000px; margin: 0; border: 0; display: block; } /* Clear the CODE style if the element is within a joplin-editable block */ .mce-content-body .joplin-editable code { border: none; background: none; padding: 0; color: inherit; font-size: inherit; } /* To make code blocks horizontally scrollable */ /* https://github.com/laurent22/joplin/issues/5740 */ pre.hljs { overflow-x: auto; } .joplin-table-wrapper{ overflow-x: auto; overflow-y: hidden; } /* =============================================== */ /* For TinyMCE */ /* =============================================== */ @media print { body { height: auto !important; } pre { white-space: pre-wrap; } .code, .inline-code { border: 1px solid #CBCBCB; } #joplin-container-content { /* The height of the content is set dynamically by JavaScript (in updateBodyHeight) to go around various issues related to scrolling. However when printing we don't want this fixed size as that would crop the content. So we set it to auto here. \"important\" is needed to override the style set by JavaScript at the element-level. */ height: auto !important; } } /* FOR THE MARKDOWN EDITOR */ /* Remove the indentation from the checkboxes at the root of the document (otherwise they are too far right), but keep it for their children to allow nested lists. Make sure this value matches the UL margin. */ li.md-checkbox { list-style-type: none; } li.md-checkbox input[type=checkbox] { margin-left: -1.71em; margin-right: 0.7em; } ul.joplin-checklist { list-style:none; } /* FOR THE RICH TEXT EDITOR */ ul.joplin-checklist li::before { content:\"\\f14a\"; font-family:\"Font Awesome 5 Free\"; background-size: 16px 16px; pointer-events: all; cursor: pointer; width: 1em; height: 1em; margin-left: -1.3em; position: absolute; color: #32373F; } .joplin-checklist li:not(.checked)::before { content:\"\\f0c8\"; } .mermaid { width: 640px; } pre.mermaid[data-processed=true] { white-space: unset; } .mermaid-export-graph { opacity: 0; height: 0; z-index: 1; position: relative; } .joplin-editable:hover .mermaid-export-graph, .joplin-editable .mermaid-export-graph:has(:focus-visible) { opacity: 1; } .mermaid-export-graph > button:hover { background-color: #CBDAF1 !important; } /* For styling the rendered Markdown */ @media print { .exported-note-title { display: none !important; } } Zekun WANG wangzekun.felix@gmail.com LinkedIn | GitHub | Blog Education KTH Royal Institute of Technology (KTH) Stockholm, Sweden M.Sc. in Communication Systems GPA: 4.81 / 5.0 09/2021 - 11/2023 University of Tokyo (UTokyo) Tokyo, Japan Exchange study in Creative Informatics GPA: 4.0 / 4.0 09/2022 - 04/2023 Huazhong University of Science and Technology (HUST) Wuhan, China B.Eng. in Electrical Engineering GPA: 3.89 / 4.0 09/2018 - 06/2022 Researh Experience Confidential Federated Learning with Homomorphic Encryption Stockholm, Sweden Master Thesis conducted in CanaryBit AB supervised by Dr. Nicolae Paladi and Prof. Roberto Guanciale 03/2023 - 09/2023 Developed Heflp, a homomorphic encryption&ndash;enhanced confidential federated learning framework supporting both PyTorch and TensorFlow as ML backbones. Integrated both classic HE schemes (CKKS, BFV) and a self-developed innovative FL-dedicated scheme Flashev2. Designed a robust model aggregation mechanism MWAvg to mitigate risk of overflowing in large-scale client scenarios, maintaining efficiency without additional computational and memory overhead. Evaluated the framework across diverse settings&mdash;varying model architectures, client counts, ML tasks, and HE schemes&mdash;to benchmark and compare performance. Overcoming Environmental Challenges in CAVs through MEC-based Federated Learning Tokyo, Japan Research Project in Tsukada Lab, UTokyo supervised by Prof. Manabu Tsukada 11/2022-05/2023 Proposed a vehicle-focused FL framework called Flames that enhances task-specific model caching across various conditions. Designed mechanisms to manage cached models and control FL processes to reduce memory overhead and increase efficiency while retaining model performances. Evaluated it on the BDD100K dataset, achieving a 5% increase in overall accuracy against centralized training. ML-Stealer: Stealing Prediction Functionality of Machine Learning Models Wuhan, China National Undergraduate Training Program for Innovation supervised by Prof. Chen Wang 04/2020-05/2021 Implemented Generative Adversarial Network(GAN) to learn the prediction behavior of victim ML models. Improved sampling and data augmentation by Variational Autoencoder(VAE). Designed loss function Applying focal-loss strategy to relieve the effect of unevenly distributed training dataset. Achieved an averaged accuracy of 85.6% and up to 93.6% at best under the mere black-box access scenario. Publications Z. Wang, N. Paladi, R. Guanciale, &ldquo;Confidential Federated Learning with Homomorphic Encryption,&rdquo; KTH master&rsquo;s degree Thesis, Stockholm, Sweden, 2023. Z. Wang, J. Nakazato, M. Asad, E. Javanmardi and M. Tsukada, &quot;Overcoming Environmental Challenges in CAVs through MEC-based Federated Learning,&quot; 2023 Fourteenth International Conference on Ubiquitous and Future Networks (ICUFN), Paris, France, 2023. G. Liu, S. Wang, B. Wan, Z. Wang and C. Wang, &quot;ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access,&quot; 2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), Shenyang, China, 2021. M. Asad, S. Shaukat, D. Hu, Z, Wang, E, Javanmardi, J. Nakazato and M. Tsukada, &ldquo;Limitations and Future Aspects of Communication Costs in Federated Learning: A Survey,&rdquo; Sensors, vol. 23, no. 17, p. 7358, Aug. 2023. Work Experience CanaryBit 一 R&amp;D Intern Remote supervised by Dr. Nicolae Paladi 12/2024-present Optimize the Heflp framework by improving code quality and refactoring the project for higher robustness and performance. Evaluat Heflp&apos;s performance with comprehensive experiments&mdash;varying dataset slices, client counts, homomorphic encryption (HE) schemes, and training settings&mdash;using a large-scale threat intelligence dataset from a corporate partner. Research and develop improvements leveraging MWAvg, implementing innovative mechanisms to reduce computational and memory overhead across diverse HE schemes while balancing precision and security. Hubei Tongmao Auto Tech Co., Ltd 一 Technical Consultant (Part-Time) Hubei, China 04/2024-10/2024 Led a team of 3 developers to design and deploy an internal document management system using Qt/C++ (frontend) and MySQL/C++ (backend) with WebSocket communication. Developed an integrated LLM-based AI assistant with RAG architecture for Q&amp;A, document summarizatoin and inspirational suggestions, increasing employee efficiency and reducing task resolution time by over 35%. Ericsson 一 Summer Intern in BBI Section Kista, Sweden supervised by Sec. Manager Mladen Toncev 06/2022-09/2022 Engineered a Bazel Event Service application in Java to aggregate and process data from 50+ clients, manage collected data via OpenSearch, and generate dashboards for DevOps analytics. Modified the source code of Bazel to enhance the Bazel Event Protocol for comprehensive client information collection, fitting specific demands. Implemented Agile practices (daily standups, sprint planning) and collaborative development techniques like pair-programming, improving team productivity and reducing code defects. Skills Programming Languages: C/C++, Python, Java, Rust, JavaScript. Frameworks: React.js, Node.js, Flask, Django, gRPC, TensorFlow, PyTorch, Langchain, Qt, Tauri, etc. Tools: Git, Docker, Jira, VS Code, Figma/Penpot, Matlab, Ollama, ChatGPT/DeepSeek. Language: Chinese: Native English: Professional working proficiency Japanese: Visitor level Others Student Club Experience Taekwondo Association (HUST) 一 Performance Team Leader 10/2019 - 05/2020 Organized and performed in 2+ martial arts shows for campus events. Trained 30+ members in techniques and competition strategies. Improved team coordination for performances. Hobbies Programming, Try opensource projects, Linux fans Mathematics, History, Chinese poetry Ping-pong, Badminton, Soccer, Taekwondo Violin, Go Chess, Animation, Travelling Honors &amp; Awards JASSO Scholarship, UTokyo 10/2022 KTH Scholarship, KTH 03/2021 Outstanding Undergraduates in Term of Academic Performance, HUST (awarded to top 1\\%) 11/2020 Scholarship for Academic Excellence, HUST 10/2020 Excellent Student Association Cadre, HUST 06/2020 The Best Creativity Award, IEEE Library Remote Service Best Practice Contest 06/2020 Scholarship for Community Engagement, HUST 09/2019"}],"posts":[{"title":"Frontend scrollbar errors","slug":"Frontend scrollbar errors","date":"2025-02-26T06:41:14.000Z","updated":"2025-02-26T07:58:08.000Z","comments":true,"path":"2025/02/26/Frontend scrollbar errors/","permalink":"http://philia897.github.io/2025/02/26/Frontend%20scrollbar%20errors/","excerpt":"A very classic problem when I develop my application using Tauri. Here is how folloing chatgpt’s advice to solve it.","text":"A very classic problem when I develop my application using Tauri. Here is how folloing chatgpt’s advice to solve it. Scroll bar always showsIf the scrollbar always shows, check if height: 100vh is set in a wrong way, it will make the element’s height to be the same as the window, will cause conflict if margin is set as well, which will always make the whole element’s height slightly larger than its outer. Normaly set height: 100vh in the body {} Scroll bar not show even the content exceed the windowCheck if the outer of the element does not set overflow, they shall be set to auto or hidden or the overflow of the inner elements will not work. Extra info from ChatGPT:Here’s an improved and more detailed version of your note, with additional explanations and examples to help clarify the concepts: Scrollbar Always ShowsIf the scrollbar is always visible, the issue may be related to setting height: 100vh in an improper way. When you apply height: 100vh, the height of the element becomes equal to the viewport height (the window size), which can lead to unexpected behavior when combined with margin or padding. This is because the height will be slightly larger than the actual viewport height, causing an overflow and displaying the scrollbar. Cause: height: 100vh makes the element’s height the same as the viewport height. If margin is set (especially margin-top or margin-bottom), it can cause the element to extend beyond the viewport, forcing the browser to show the scrollbar. Solution:To prevent this issue, it is common to avoid setting height: 100vh directly on the body or main container. Instead, you can use min-height: 100vh, which ensures that the element’s height is at least the viewport height but can expand to fit the content, avoiding unnecessary scrollbars. Example:1234567891011/* Instead of this */body &#123; height: 100vh; margin: 0;&#125;/* Use this */body &#123; min-height: 100vh; margin: 0;&#125; In this example, min-height: 100vh ensures that the body will at least cover the entire viewport but will expand if the content exceeds the viewport height, preventing any unexpected scrollbars. Scrollbar Does Not Appear Even When Content Exceeds the WindowIf the content overflows the window but the scrollbar does not appear, it is often due to missing overflow properties on the container element. By default, many elements do not display scrollbars even if their content exceeds their set dimensions. Cause: The overflow property needs to be set on the parent container (overflow: auto or overflow: scroll) to enable scrolling. Without setting overflow, the browser won’t know to display a scrollbar, even if the content overflows. Solution:Ensure that the parent container of the overflowing content has the appropriate overflow property set. Use overflow: auto to display a scrollbar only when necessary (when the content exceeds the container size) or overflow: scroll to always show the scrollbar. Example:1234567891011/* Outer container */.container &#123; width: 100%; height: 500px; /* Set a specific height */ overflow: auto; /* Display scrollbars only when content overflows */&#125;/* Inner content */.inner-content &#123; height: 700px; /* Content that exceeds the container height */&#125; In this example, the .container has a fixed height of 500px and overflow: auto, so when the .inner-content exceeds the height, the browser will display a vertical scrollbar. Alternatively, if you want the scrollbar to always be visible (even if there is no overflow), you can use overflow: scroll instead of auto. By applying these adjustments, you can ensure proper handling of scrollbars for both cases: when you want them to always show or only when necessary.","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"},{"name":"javascript","slug":"javascript","permalink":"http://philia897.github.io/tags/javascript/"}]},{"title":"Test Pro","slug":"Test-Pro","date":"2025-02-25T07:16:20.000Z","updated":"2025-02-25T07:19:31.281Z","comments":true,"path":"2025/02/25/Test-Pro/","permalink":"http://philia897.github.io/2025/02/25/Test-Pro/","excerpt":"","text":"Hello world","categories":[],"tags":[],"author":"John Doe"},{"title":"Cryptography Learning","slug":"Cryptography Learning","date":"2025-02-25T02:11:07.000Z","updated":"2025-02-25T02:12:44.000Z","comments":true,"path":"2025/02/25/Cryptography Learning/","permalink":"http://philia897.github.io/2025/02/25/Cryptography%20Learning/","excerpt":"Record some meaningful chat history when chatgpt teaching me about cryptography.","text":"Record some meaningful chat history when chatgpt teaching me about cryptography. You:I’m learning cryptography, and I’m very curious about some technologies. Could you tell me what is secure multiparty computation? ChatGPT:Absolutely! Secure Multiparty Computation (SMPC or MPC) is a subfield of cryptography that focuses on enabling multiple parties to jointly compute a function over their inputs while keeping those inputs private. Here’s a breakdown of the key ideas: Core Concepts Multiple Parties, Private Inputs: Each party has its own private data that it does not want to share with others. Despite this, they wish to collaborate to compute a function that depends on all their inputs. Function Computation with Privacy: The parties compute the output of a function ( f(x_1, x_2, \\ldots, x_n) ), where ( x_1, x_2, \\ldots, x_n ) are their private inputs. The protocol ensures that no party learns anything about the other parties’ inputs except what can be inferred from the final output. Security Goals: Privacy: Inputs remain confidential. Correctness: The output is guaranteed to be correct according to the function specification. Fairness (optional in some protocols): All parties get the result, or none do. Independence of Inputs: Parties cannot choose their inputs based on others’ inputs. How Does It Work?SMPC relies on cryptographic techniques to achieve its goals. Some common methods include: Secret Sharing: Each party splits its input into random shares and distributes them among all parties. No single party holds enough information to reconstruct the original input. The computation is performed on these shares without revealing the actual values. Homomorphic Encryption: A special type of encryption that allows certain computations to be performed directly on encrypted data. The result, when decrypted, matches the output of the same computation performed on the plaintext data. Garbled Circuits: A method where a function is represented as a circuit, and parties securely evaluate it using encrypted or “garbled” representations of the data. Oblivious Transfer: A protocol used to enable one party to send data to another such that the receiver learns only part of the data (and the sender doesn’t know what part). Applications of SMPCSMPC has a wide range of practical applications where privacy is critical: Privacy-Preserving Data Analysis: Companies can jointly analyze datasets without exposing sensitive customer information. Secure Voting: Ensures voter privacy while tallying votes accurately. Fraud Detection: Banks can collaborate to detect fraud across financial institutions without exposing their transaction logs. Genomic Research: Enables researchers to compute over genomic data while protecting individual privacy. Auction Mechanisms: Conduct secure auctions without revealing bidders’ bids until the outcome. Advantages and ChallengesAdvantages: Protects sensitive data during computation. Reduces the need for trusted third parties. Challenges: High computational and communication costs compared to traditional computations. Scalability issues for large datasets or many parties. Complexity in designing secure protocols for arbitrary functions. In essence, SMPC is a powerful tool for enabling collaborative computations in a privacy-preserving way, and it’s becoming increasingly relevant in our data-driven world! Let me know if you want to dive deeper into any specific aspect! You:Could you give me some examples about how to make it using secret sharing or homomorphic encryption? ChatGPT:Sure! Let’s break down examples of Secure Multiparty Computation (SMPC) using secret sharing and homomorphic encryption. 1. Secret Sharing ExampleSecret sharing involves splitting a value into multiple shares and distributing them among participants. No single share reveals any information about the secret, but the secret can be reconstructed when a sufficient number of shares are combined. Example: Addition Using Secret SharingImagine Alice and Bob want to compute the sum ( x + y ) without revealing their respective inputs ( x ) and ( y ). Setup: Create Shares Alice has ( x &#x3D; 5 ) and splits it into two random shares, e.g., ( x_1 &#x3D; 2 ) and ( x_2 &#x3D; 3 ) such that ( x_1 + x_2 &#x3D; x ). Bob has ( y &#x3D; 7 ) and splits it into two random shares, e.g., ( y_1 &#x3D; 4 ) and ( y_2 &#x3D; 3 ) such that ( y_1 + y_2 &#x3D; y ). Distribute Shares Alice sends ( x_1 ) to Bob and keeps ( x_2 ). Bob sends ( y_1 ) to Alice and keeps ( y_2 ). Local Computation Alice computes ( s_A &#x3D; x_2 + y_1 &#x3D; 3 + 4 &#x3D; 7 ). Bob computes ( s_B &#x3D; x_1 + y_2 &#x3D; 2 + 3 &#x3D; 5 ). Reconstruct the Result Alice and Bob exchange their local results: ( s_A + s_B &#x3D; 7 + 5 &#x3D; 12 ), which is ( x + y ). This method ensures that neither party learns the other’s input! 2. Homomorphic Encryption ExampleHomomorphic encryption allows computations to be performed directly on encrypted data. The result of these computations, when decrypted, matches the output of the same computation on the plaintext data. Example: Multiplication Using Homomorphic EncryptionImagine Alice has ( x &#x3D; 3 ) and Bob has ( y &#x3D; 4 ). They want to compute ( x \\cdot y ) without sharing their inputs. Encryption Alice encrypts her input ( x &#x3D; 3 ) using a homomorphic encryption scheme and sends the encrypted value ( E(x) ) to Bob. Homomorphic Computation Bob performs the multiplication directly on the encrypted value: ( E(x \\cdot y) &#x3D; E(x)^y ) (multiplication in plaintext corresponds to repeated multiplication in encryption). Bob sends the result ( E(x \\cdot y) ) back to Alice. Decryption Alice decrypts ( E(x \\cdot y) ) using her private key to get ( x \\cdot y &#x3D; 12 ). Bob never learns ( x ), and Alice never learns ( y ), but they collaboratively compute the product. Comparison and Use Cases Method Advantages Challenges Use Case Secret Sharing - Lightweight computation- No need for encryption - High communication overhead- Limited to specific operations Privacy-preserving summation in distributed systems Homomorphic Encryption - Supports general computations- Minimal communication - Computationally expensive- Requires encryption keys Privacy-preserving data analysis in cloud environments Both methods achieve the goal of SMPC but are suited for different scenarios. Secret sharing is more efficient for simple computations, while homomorphic encryption offers greater flexibility at the cost of higher computational complexity. Let me know if you’d like more code-like examples or further exploration! You:So in my understanding. Secret sharing is like everyone split its secret into several pieces of shares, and sent these shares to other parties, while keep at least one share in it own side. So that every party does not know the whole value unless they collude. Then everyone did the part of computation on the data shares they have. Finally aggregate all the results to get the final result. It looks like parallel computing? Where different threads do not have data of other threads. ChatGPT:Yes, your understanding is correct, and you’re drawing an insightful analogy to parallel computing! Let’s break this down further to see the similarities and key differences: How Secret Sharing Relates to Parallel Computing Data Distribution: Secret Sharing: Each party splits its secret into shares and distributes them across multiple parties, including itself. Parallel Computing: In a distributed system, data is partitioned across different threads or nodes to process parts of a computation independently. Independent Computation: Secret Sharing: Each party performs computations on the shares they receive without access to the full original data. Parallel Computing: Each thread or node performs its computation independently, often without knowledge of other threads’ operations. Aggregation: Secret Sharing: At the end of the computation, the parties combine their intermediate results (computed on shares) to reconstruct the final result. Parallel Computing: The results from different threads or nodes are merged to produce the overall output. Key Differences Privacy vs. Performance: In secret sharing, the goal is privacy: ensuring no single party can reconstruct the original secret without collaboration. In parallel computing, the goal is performance: splitting the workload to achieve faster computation. Communication Overhead: Secret Sharing: Requires extensive communication between parties to share data and exchange partial results. This is necessary to ensure no party can independently deduce private inputs. Parallel Computing: Communication overhead is generally minimized to optimize performance, as threads or nodes typically don’t exchange sensitive information. Fault Tolerance: Secret Sharing: Many schemes are designed to tolerate a certain number of “faulty” or colluding parties without compromising the computation’s privacy. Parallel Computing: Fault tolerance is usually concerned with system crashes or failures, not malicious behavior. Security Assumptions: Secret sharing assumes parties will follow protocol rules (honest-but-curious) or attempts to handle malicious behavior (with additional cryptographic safeguards). Parallel computing generally doesn’t deal with adversaries; its focus is on efficient execution. Analogy VisualizedImagine you’re tasked with finding the total weight of a set of packages: Secret Sharing: Each package is split into random parts and distributed among workers. Each worker processes their part of the data and sends their partial results back. Finally, the total weight is reconstructed by aggregating the results. Parallel Computing: The packages are simply divided among workers. Each worker calculates the weight of their assigned packages and sends their results for aggregation. ConclusionYou’re absolutely right that secret sharing can feel like parallel computing in its distributed nature. However, its design prioritizes privacy and security, whereas parallel computing focuses on speed and efficiency. Despite their different goals, the conceptual framework of breaking down tasks and aggregating results provides a strong parallel between the two! You:Got it. Then next topic. What is private information retrieval, and give some examples ChatGPT: What is Private Information Retrieval (PIR)?Private Information Retrieval (PIR) is a cryptographic technique that allows a user to query a database and retrieve specific information without revealing to the database which item is being retrieved. The goal is to protect the privacy of the user’s query. How Does PIR Work?The main challenge in PIR is to minimize the communication overhead and computation cost while preserving privacy. There are two primary approaches: Single-Server PIR: The database resides on a single server. Techniques like homomorphic encryption or oblivious transfer are used to ensure the server cannot deduce which record the user is retrieving. Multi-Server PIR: The database is replicated across multiple servers. The user queries each server in a way that no individual server can learn which item is being retrieved, and the responses are combined to reconstruct the desired data. Examples of PIR1. Single-Server PIR Using Homomorphic Encryption Scenario: Alice wants to retrieve the ( i )-th record from a database ( DB &#x3D; [d_1, d_2, \\ldots, d_n] ), hosted on a single server, without revealing ( i ) to the server. Process: Alice creates an encrypted query vector ( Q ) using homomorphic encryption. This vector is ( Q &#x3D; [q_1, q_2, \\ldots, q_n] ), where ( q_i &#x3D; 1 ) (encrypted) and ( q_j &#x3D; 0 ) (encrypted) for all ( j \\neq i ). Alice sends ( Q ) to the server. The server computes the dot product of ( Q ) with the database ( DB ):[R &#x3D; Q \\cdot DB &#x3D; q_1 d_1 + q_2 d_2 + \\ldots + q_n d_n]Since only ( q_i ) is “1” and the rest are “0”, this gives ( R &#x3D; d_i ), but in an encrypted form. The server sends the encrypted ( R ) back to Alice. Alice decrypts ( R ) to obtain ( d_i ). Privacy: The server cannot learn ( i ), because all it sees are encrypted values. 2. Multi-Server PIR Scenario: A database is replicated across two servers ( S_1 ) and ( S_2 ). Bob wants to retrieve ( d_k ) without revealing ( k ). Process: Bob sends different queries to ( S_1 ) and ( S_2 ), designed so that neither ( S_1 ) nor ( S_2 ) can independently determine ( k ). Each server processes its query and sends a partial result back to Bob. Bob combines the results to retrieve ( d_k ). Example Scheme: Bob sends random subsets of indices to each server, ensuring that ( k ) appears in a way that only the combination of responses reveals the desired record. Applications of PIR Digital Privacy: Browsing anonymous search engines without revealing search queries. Retrieving sensitive data from cloud storage without exposing access patterns. Censorship-Resistant Information Access: Users can access information in restricted environments without revealing what they’re accessing. Genomic Privacy: Researchers can query genomic databases to retrieve specific data points without revealing which genetic marker they’re interested in. Cryptocurrency: Enhancing the privacy of blockchain operations by obscuring access to transaction records. Challenges and Practical Considerations Communication Overhead: Simple PIR schemes often require the user to download a large portion of the database, making them inefficient for large datasets. Computation Cost: Techniques like homomorphic encryption can be computationally expensive, especially for large-scale databases. Server Assumptions: Single-server schemes require strong cryptographic guarantees. Multi-server schemes assume that servers do not collude, which may not always be realistic. Variants of PIR Computational PIR: Relies on computational hardness assumptions (e.g., homomorphic encryption). Information-Theoretic PIR: Guarantees perfect privacy without cryptographic assumptions but usually requires multiple servers. Let me know if you’d like further examples, implementation details, or deeper insights into a specific aspect of PIR! You:What about threshold cryptography? ChatGPT: What is Threshold Cryptography?Threshold cryptography is a branch of cryptography that enables a group of parties to collaboratively perform cryptographic operations (e.g., signing, decryption, key generation) without any single party having access to the entire secret key. Instead, the key is divided into shares using techniques like secret sharing. Key Features of Threshold Cryptography Threshold Property: A cryptographic operation (e.g., signing or decryption) requires a minimum number (( t )) of parties to collaborate. If fewer than ( t ) parties are present, the operation cannot be performed, preserving security. Distributed Trust: The secret key is never stored in a single location or by a single entity. Instead, it’s split into shares and distributed among multiple parties. No single party or small group of malicious parties (fewer than ( t )) can compromise the system. Fault Tolerance: Threshold cryptosystems can tolerate some parties being unavailable or compromised, as long as the required ( t )-party threshold is met. How Does Threshold Cryptography Work?Threshold cryptography typically uses secret sharing schemes, like Shamir’s Secret Sharing: Shamir’s Secret Sharing Scheme Splitting the Secret: A secret ( S ) is encoded as the constant term of a polynomial ( P(x) ) of degree ( t-1 ), where ( t ) is the threshold. Random coefficients are chosen for the polynomial: ( P(x) &#x3D; S + a_1x + a_2x^2 + \\ldots + a_{t-1}x^{t-1} ). Each party ( i ) is given a share ( (i, P(i)) ), i.e., an evaluation of ( P(x) ) at point ( i ). Reconstruction: To reconstruct ( S ), any ( t ) or more parties use their shares to interpolate ( P(x) ) using Lagrange interpolation. If fewer than ( t ) shares are available, ( S ) cannot be reconstructed. Applications of Threshold Cryptography Threshold Signature Schemes: A digital signature is generated collaboratively by ( t ) or more parties without any of them knowing the entire private key. Example: Signing in blockchain consensus mechanisms (e.g., BFT protocols) where multiple validators collaborate to sign blocks. Threshold Decryption: Decryption is performed only when ( t ) or more parties provide their decryption shares. Example: Protecting encrypted votes in electronic voting systems, where decryption requires collaboration from a group of election officials. Key Management: Private keys for systems like SSL&#x2F;TLS certificates are split among multiple servers. Example: Cloud providers offering secure key management services can distribute trust across multiple servers to prevent compromise. Secure Multiparty Computation (SMPC): Threshold cryptography can be used to securely distribute keys for SMPC protocols. Cryptocurrency Wallets: Threshold cryptography enables secure multi-signature wallets where a group must collaborate to sign transactions. Example: A wallet requiring 2 out of 3 key holders to authorize transactions. Examples of Threshold Cryptography Protocols Threshold RSA: The RSA private key is split into shares. Decryption or signing requires collaboration among ( t ) or more parties who provide partial results that are combined to produce the final output. Threshold Elliptic Curve Digital Signature Algorithm (ECDSA): Used in cryptocurrency wallets and blockchain protocols. The private key for ECDSA is split, and signing operations are performed collaboratively. Verifiable Secret Sharing (VSS): A variant of secret sharing where the correctness of the shares can be verified without revealing the secret. Useful in preventing malicious parties from distributing invalid shares. Advantages of Threshold Cryptography Enhanced Security: The private key is never exposed in its entirety, reducing the risk of compromise. Fault Tolerance: The system can continue functioning even if some parties are unavailable. Distributed Trust: No single entity has full control over the secret, ensuring decentralization. Resilience Against Insider Threats: Even if some parties are malicious, they cannot compromise the system as long as the threshold ( t ) is not met. Challenges in Threshold Cryptography Efficiency: Cryptographic operations involving multiple parties can be computationally and communication-intensive. Setup Complexity: Initial setup (e.g., generating and distributing shares) can be complicated. Robustness: Handling malicious parties often requires additional protocols, like Verifiable Secret Sharing (VSS) or Byzantine fault tolerance. Threshold cryptography is a critical tool in decentralized and secure systems. If you’d like, I can provide more technical details or examples, like how threshold signatures work in blockchain! You:But when using Shamir’s secret sharing, how to achieve threshold cryptography? Giving one example ChatGPT:Let’s walk through an example of how Shamir’s Secret Sharing can be used to achieve threshold cryptography, such as threshold decryption. Scenario: Threshold DecryptionA secret key ( S ) is split among multiple parties using Shamir’s secret sharing. At least ( t ) out of ( n ) parties must collaborate to decrypt a message, but fewer than ( t ) parties cannot decrypt it. Setup: Sharing the Secret The secret ( S ) (e.g., a private key) is split using a polynomial of degree ( t-1 ):[P(x) &#x3D; S + a_1x + a_2x^2 + \\ldots + a_{t-1}x^{t-1}] The coefficients ( a_1, a_2, \\ldots, a_{t-1} ) are chosen randomly. Each party ( i ) is assigned a share ( (i, P(i)) ), where ( i ) is a unique identifier. The shares ( (i, P(i)) ) are securely distributed to ( n ) parties. Encryption and Decryption The public key (corresponding to the private key ( S )) is used for encryption, so anyone can encrypt messages. Decryption requires ( t ) or more parties to collaborate. Decryption ExampleSuppose the encrypted message is ( E(m) ), and we need ( t &#x3D; 3 ) out of ( n &#x3D; 5 ) parties to decrypt. Collect Shares: Parties ( P_1, P_2, ) and ( P_3 ) collaborate by providing their shares ( (1, P(1)) ), ( (2, P(2)) ), and ( (3, P(3)) ). Reconstruct the Secret: Using Lagrange interpolation, the parties reconstruct the secret ( S ). The formula for ( P(0) ), which corresponds to the secret, is:[S &#x3D; P(0) &#x3D; \\sum_{i&#x3D;1}^{t} P(i) \\cdot \\lambda_i]where ( \\lambda_i ) are the Lagrange coefficients:[\\lambda_i &#x3D; \\prod_{j&#x3D;1, j \\neq i}^{t} \\frac{-j}{i - j}] For example, if shares are: ( (1, P(1)) &#x3D; (1, 5) ) ( (2, P(2)) &#x3D; (2, 15) ) ( (3, P(3)) &#x3D; (3, 40) ) The Lagrange interpolation reconstructs ( S ). Decrypt: Once ( S ) (the private key) is reconstructed, it is used to decrypt the message ( E(m) ) into ( m ). Why is this Threshold Cryptography? The key ( S ) is never revealed in its entirety to any single party. Collaboration among at least ( t ) parties is required to reconstruct ( S ). Even if fewer than ( t ) parties collude, they cannot learn ( S ). Practical Applications Secure Voting Systems: Threshold decryption ensures that election results can only be decrypted if a quorum of election officials collaborates. Cryptocurrency Wallets: Threshold wallets require multiple key holders (e.g., 2 out of 3) to sign transactions without exposing the private key. Distributed Key Management: Organizations can distribute decryption rights across multiple servers, reducing the risk of a single point of failure. Let me know if you’d like to see the mathematical steps in more detail or code examples for this process! You:But when decryption, the S is revealed right? The one who did the Lagrange interpolation can know the secret S. So who will be the one to do this? ChatGPT:You’re absolutely correct that when Lagrange interpolation is used to reconstruct the secret ( S ), the entity performing the reconstruction learns the secret. This is a critical consideration in threshold cryptography designs. Different approaches address this concern depending on the application. Let’s explore the options. Who Reconstructs the Secret? Trusted Party: In some systems, a trusted entity (e.g., an election authority, a key management server) performs the interpolation. Limitation: Requires a trusted party, which contradicts the decentralized ethos of many threshold cryptography applications. Shared Decryption Without Revealing ( S ): Instead of reconstructing ( S ), parties use their shares to directly compute the decryption result without any single party learning ( S ). This approach is common in threshold decryption systems like Threshold RSA or Threshold ECDSA. Example: If the ciphertext ( C ) corresponds to ( E(m) &#x3D; m^e \\mod N ) (RSA encryption), each party can compute a partial decryption result ( C^{s_i} ), where ( s_i ) is their share of the private key ( S ). The partial results are combined (e.g., via modular multiplication) to recover ( m ), without ever reconstructing ( S ). Distributed Reconstruction Using Secure Multiparty Computation (SMPC): Instead of one party reconstructing ( S ), the parties engage in a secure multiparty computation (SMPC) protocol. The result of this protocol is the final output (e.g., the decrypted message) without any party learning ( S ). Public Verification (Non-Interactive Threshold Cryptography): In systems like threshold signature schemes, parties do not reconstruct the private key ( S ). Instead, each party provides a partial signature using their share, and these partial signatures are combined into a valid full signature. This can be publicly verified without revealing ( S ). Example: Threshold RSA DecryptionSuppose a message ( m ) is encrypted as ( C &#x3D; m^e \\mod N ), where ( N ) is the RSA modulus, and ( S ) is the private key. Setup: ( S ) is split into shares ( s_1, s_2, \\ldots, s_n ) using Shamir’s secret sharing. Each party holds one share. Decryption: Each party computes a partial decryption ( D_i &#x3D; C^{s_i} \\mod N ). The partial decryptions ( D_1, D_2, \\ldots, D_t ) are combined using a reconstruction formula (specific to RSA) to recover ( m ), without revealing ( S ) to any party. SummaryIn practical threshold cryptographic systems, the secret ( S ) itself is often not reconstructed explicitly. Instead: Computation is performed collaboratively on shares to produce the desired output (e.g., decryption or signature). This prevents any single party from learning ( S ), aligning with the goals of privacy and security. If you’d like, I can dive deeper into any of these approaches or show an example of distributed decryption! You:So it looks like threshold cryptography means that multiple parties shares one secret, aiming to do the cryptographic operation without revealing the secret. While secure multiparty computation means multiple parties have their own secrets, and they want to collaboratively execute one function on these secrets to get the result without revealing any party’s secret.But in the threshold cryptography, each party’s share of the private key is also a secret that shall not be shared with other parties. So can I consider threshold cryptography a particular kind of MPC? ChatGPT:Yes, you can absolutely consider threshold cryptography as a specific kind of secure multiparty computation (SMPC)! The distinction lies in their objectives and the type of secrets involved, but the mechanisms share many conceptual similarities. Let’s clarify this connection. Threshold Cryptography as a Subclass of SMPC Shared Secret in Threshold Cryptography: Goal: Multiple parties collectively perform cryptographic operations (e.g., signing, decryption) using a shared secret (e.g., a private key) without revealing the secret to any single party. Nature of the Secret: The secret (e.g., private key ( S )) is split and distributed using a secret-sharing scheme, so no individual party knows the full secret. Operation: Parties collaborate to reconstruct the result of the cryptographic operation (e.g., signature, plaintext) without reconstructing the full secret ( S ). Essentially, threshold cryptography focuses on cryptographic operations with a shared secret. Secrets in SMPC: Goal: Multiple parties collaboratively compute a function ( f(x_1, x_2, \\ldots, x_n) ) on their individually owned secrets ( x_1, x_2, \\ldots, x_n ), ensuring that no party learns any other party’s secret. Nature of the Secrets: Each party has its own secret input that is never revealed. Operation: The protocol ensures privacy while allowing the correct result ( f(x_1, x_2, \\ldots, x_n) ) to be computed. Similarities No Single Point of Knowledge: In threshold cryptography, no single party knows the full secret ( S ). In SMPC, no party learns another party’s secret ( x_i ). Collaboration: Both require multiple parties to participate in the protocol to compute a meaningful result. Mathematical Techniques: Both use tools like secret sharing, homomorphic encryption, and zero-knowledge proofs to preserve privacy during computation. Differences Aspect Threshold Cryptography SMPC Nature of Secrets A single shared secret (e.g., private key) split among parties. Each party has its own individual secret (e.g., private data). Objective Perform a cryptographic operation without reconstructing the secret. Compute a general function on private inputs without revealing them. Scope Focused on specific cryptographic tasks (e.g., signing, decryption). Broad, can compute any function (e.g., statistics, machine learning). Viewing Threshold Cryptography as a Special Case of SMPCIn threshold cryptography: The function being computed is tied to a cryptographic operation. For example: ( f(C, S) &#x3D; m ) (threshold decryption of ciphertext ( C ) with private key ( S )). ( f(M, S) &#x3D; \\text{signature} ) (threshold signing of message ( M ) with private key ( S )). The inputs to the computation are the shares of the private key, which are derived from a single shared secret. In this sense, threshold cryptography is a constrained form of SMPC, where the inputs are derived from a shared secret and the output is a cryptographic result. Key TakeawayYou are absolutely right to observe that threshold cryptography can be considered a specialized form of SMPC: Threshold cryptography focuses on cryptographic operations with a shared secret. SMPC is broader and focuses on computing any function over distributed private inputs. If you’d like, I can provide more examples or dive into how specific SMPC tools (like secret sharing or homomorphic encryption) directly enable threshold cryptographic protocols! You:Yes, give me some famous threshold cryptographic protocols ChatGPT:Here are some famous threshold cryptographic protocols, categorized based on the cryptographic operations they enable, such as decryption, signing, and key generation. These protocols are foundational to many secure distributed systems, including blockchains, secure voting, and cloud services. 1. Threshold RSA Objective: Perform RSA decryption or signing using a distributed private key without reconstructing the private key. Core Idea: The RSA private key ( S ) is split into shares using Shamir’s Secret Sharing or similar schemes. Each party computes a partial result (e.g., ( C^{s_i} \\mod N ), where ( s_i ) is their share of the private key). The partial results are combined to reconstruct the decryption or signature without revealing the private key. Applications: Distributed key management. Threshold signing in blockchain systems (e.g., signing blocks or transactions). Famous Protocols: Shoup’s Threshold RSA Protocol: Ensures robustness and security even with malicious participants. Gennaro, Jarecki, Krawczyk, Rabin (GJKR) Protocol: A more efficient and secure threshold RSA implementation. 2. Threshold ECDSA (Elliptic Curve Digital Signature Algorithm) Objective: Generate ECDSA signatures in a threshold setting without revealing the private key. Core Idea: The private key used in ECDSA is distributed among ( n ) parties. The parties collaboratively compute the signature (involving elliptic curve point multiplication) without any party learning the full private key. Challenges: Ensuring the security of elliptic curve point multiplication in a distributed manner. Applications: Threshold wallets for cryptocurrencies like Bitcoin and Ethereum. Secure signing in blockchain consensus mechanisms. Famous Protocols: GG18 Protocol (Goldfeder et al., 2018): A practical and secure threshold ECDSA implementation. GG20 Protocol: An improvement on GG18, designed for higher security and efficiency. 3. Verifiable Secret Sharing (VSS) Objective: Share a secret among parties with guarantees that malicious parties cannot distribute invalid or inconsistent shares. Core Idea: Extends Shamir’s Secret Sharing with a verification mechanism to ensure that: Distributed shares are valid and consistent. Malicious parties cannot introduce invalid shares during the reconstruction phase. Applications: Building robust threshold cryptographic protocols. Foundations for secure multiparty computation (SMPC). Famous Protocols: Pedersen’s VSS Protocol: Uses cryptographic commitments for verification. Feldman’s VSS Protocol: A non-interactive VSS scheme based on public key cryptography. 4. Threshold Schnorr Signatures Objective: Generate Schnorr signatures collaboratively without revealing the private key. Core Idea: Schnorr signatures involve proving knowledge of a discrete logarithm. The private key is split among parties, and they collectively compute a valid signature. Applications: Privacy-preserving authentication. Signing in blockchain consensus protocols. Famous Protocols: MULTISIG Schnorr: A threshold signature scheme widely used in blockchain systems like Bitcoin. 5. Threshold ElGamal Encryption Objective: Decrypt messages encrypted using ElGamal encryption in a distributed manner. Core Idea: The ElGamal private key is split into shares, and decryption is performed by combining partial decryptions from ( t ) or more parties. The private key itself is never reconstructed. Applications: Secure e-voting systems where votes are encrypted, and decryption requires collaboration from election authorities. Famous Protocols: Used in Helios, a secure e-voting system. 6. Distributed Key Generation (DKG) Protocols Objective: Generate a cryptographic key collaboratively among multiple parties without any single party controlling the full key. Core Idea: Each party contributes randomness during the key generation process. The resulting key is shared among the parties using secret sharing. Applications: Threshold key management. Secure setups for SMPC and other cryptographic protocols. Famous Protocols: Gennaro, Jarecki, Krawczyk, Rabin (GJKR) DKG Protocol: A robust protocol for RSA key generation. Pedersen’s DKG Protocol: A popular DKG for discrete logarithm-based systems. 7. Threshold Homomorphic Encryption Objective: Enable distributed decryption of homomorphically encrypted data. Core Idea: The secret key for a homomorphic encryption scheme is split among parties. Partial decryptions are combined to recover the plaintext without revealing the private key. Applications: Privacy-preserving machine learning. Secure e-voting systems. Famous Protocols: Paillier Threshold Encryption: Threshold decryption for Paillier’s additive homomorphic encryption scheme. 8. BLS (Boneh–Lynn–Shacham) Threshold Signatures Objective: Generate compact signatures in a threshold setting, useful for blockchains and consensus protocols. Core Idea: BLS signatures are short, based on pairings over elliptic curves. Parties collaborate to generate a BLS signature without exposing the private key. Applications: Blockchain protocols like Ethereum 2.0. Aggregated signatures for efficient verification. Famous Protocols: BLS threshold signatures in decentralized finance (DeFi) and consensus systems. Key Takeaways Threshold RSA and ECDSA are widely used in distributed systems like blockchains. Verifiable Secret Sharing (VSS) and Distributed Key Generation (DKG) are foundational for building robust cryptographic systems. Threshold Schnorr and BLS Signatures are gaining popularity in blockchain ecosystems for their efficiency and compactness. If you’re interested in a particular protocol, I can dive deeper into its technical details or real-world applications!","categories":[{"name":"Cryptography","slug":"Cryptography","permalink":"http://philia897.github.io/categories/Cryptography/"}],"tags":[{"name":"cryptography","slug":"cryptography","permalink":"http://philia897.github.io/tags/cryptography/"},{"name":"multiparty computation","slug":"multiparty-computation","permalink":"http://philia897.github.io/tags/multiparty-computation/"},{"name":"homomorphic encryption","slug":"homomorphic-encryption","permalink":"http://philia897.github.io/tags/homomorphic-encryption/"},{"name":"secret sharing","slug":"secret-sharing","permalink":"http://philia897.github.io/tags/secret-sharing/"},{"name":"threshold cryptography","slug":"threshold-cryptography","permalink":"http://philia897.github.io/tags/threshold-cryptography/"}]},{"title":"npm vs Yarn vs Deno vs Bun","slug":"npm vs Yarn vs Deno vs Bun","date":"2025-02-15T19:12:54.000Z","updated":"2025-02-15T19:13:09.000Z","comments":true,"path":"2025/02/16/npm vs Yarn vs Deno vs Bun/","permalink":"http://philia897.github.io/2025/02/16/npm%20vs%20Yarn%20vs%20Deno%20vs%20Bun/","excerpt":"Compare the javascript runtimes and package managers.","text":"Compare the javascript runtimes and package managers. Great question! These tools are all package managers or runtime environments used in the JavaScript ecosystem, but they have different purposes, features, and performance characteristics. Here’s a quick rundown of each: 1. npm (Node Package Manager) Purpose: npm is the default package manager for Node.js. It is used to install and manage libraries (dependencies) in JavaScript projects. Features: Large registry with millions of packages. Works seamlessly with the Node.js runtime. Has a wide ecosystem and community. npm install is used to install dependencies, and it has its own lockfile (package-lock.json) for ensuring consistent installs across different environments. Pros: Well-established, stable, and widely supported. Integrated directly into the Node.js environment. Cons: Historically slower than other package managers (though npm has been improving in recent versions). The default CLI can be a bit slow and cumbersome for larger projects. 2. Yarn Purpose: Yarn is an alternative package manager to npm, created by Facebook to address some of npm’s limitations, particularly around speed and determinism (consistency across installs). Features: Fast, thanks to parallelized operations and caching. yarn.lock ensures reproducible builds, similar to npm’s lockfile. Better handling of monorepos. Supports offline installs. Pros: Faster installs compared to older versions of npm (though npm has caught up with the release of v7 and beyond). More reliable dependency resolution and deterministic installs. Cons: Slightly less popular than npm in some communities (although it still has wide adoption). Not as tightly integrated with Node.js, so you need to install it separately. 3. Deno Purpose: Deno is a modern JavaScript and TypeScript runtime built by the creator of Node.js, Ryan Dahl. It aims to fix the shortcomings of Node.js, with a focus on security, simplicity, and TypeScript support out of the box. Features: Secure by default, with restricted access to the file system, network, etc., unless explicitly allowed. Native TypeScript support without needing a separate transpiler. No package manager like npm or Yarn; instead, it uses URLs to import modules directly from the web (similar to how browsers handle modules). Pros: More secure out of the box. Built-in TypeScript support. Modern features and design principles. Cons: Newer and less mature than Node.js. Smaller ecosystem, so fewer libraries and tools. Lacks the widespread community and package management features that npm&#x2F;Yarn provide. 4. Bun Purpose: Bun is a fast JavaScript runtime like Node.js and Deno, but it focuses heavily on performance. It also comes with its own package manager, bundler, and task runner. Features: Extremely fast startup times and execution. Built-in bundling and task running capabilities. Uses a new package manager (bun install), which claims to be much faster than npm and Yarn. Pros: Super fast (especially for bundling and installing dependencies). Native TypeScript and JSX support. Out-of-the-box tooling for bundling, running scripts, and testing. Cons: Still very new, so ecosystem support is limited. Some compatibility issues with older libraries or Node.js-specific APIs. Not as widely adopted as npm or Yarn yet. Which one is recommended? For Most Projects (especially if you’re using Node.js): npm or Yarn are the go-to choices. If you need speed and reliability, Yarn might be better, but npm has been catching up in terms of performance. For New Projects with TypeScript: If you’re starting fresh with a focus on modern features, you might want to try Deno, especially if security and native TypeScript support are key for you. For Maximum Performance: If you’re working on performance-sensitive projects, or if you’re curious about the newest tools, Bun might be a good experiment, especially for bundling or dev server setups. That said, npm remains the most recommended for general use, simply because of its maturity, vast ecosystem, and community support. Yarn is still a great alternative if you care more about speed and consistency.","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://philia897.github.io/tags/javascript/"},{"name":"npm","slug":"npm","permalink":"http://philia897.github.io/tags/npm/"}]},{"title":"Binding ip address v.s. firewall rules","slug":"Binding ip address v.s. firewall rules","date":"2025-01-08T22:33:15.000Z","updated":"2025-01-08T22:33:31.000Z","comments":true,"path":"2025/01/09/Binding ip address v.s. firewall rules/","permalink":"http://philia897.github.io/2025/01/09/Binding%20ip%20address%20v.s.%20firewall%20rules/","excerpt":"Just note the learning history with chatgpt regarding firewall and basic networking knowledge.","text":"Just note the learning history with chatgpt regarding firewall and basic networking knowledge. Exactly! You’ve got it right. Let’s break this down: Binding to an IP Address — Per-Service LevelBinding an IP address (e.g., 127.0.0.1, 0.0.0.0, or a specific network interface) is a service-level configuration. This defines where a service is “listening” for incoming connections. It’s up to each service to specify which IP addresses and ports it should accept requests from. Key Points: Binding to a Specific IP Address: When a service binds to 127.0.0.1 (localhost), it is only accessible from the local machine (i.e., processes running on the same machine). External services, including Docker containers or other machines on the network, cannot access it. When a service binds to 0.0.0.0, it listens on all network interfaces of the host machine, making it accessible both from the local machine (via 127.0.0.1) and from external sources, such as Docker containers or machines on the network. Per-Service Configuration: Every service (like a web server, database, etc.) typically has its own configuration file or startup argument where it defines the address (IP) and port it should listen on. For example, a web server (such as Apache or Nginx) might be configured to listen on port 80, but whether it’s listening on 127.0.0.1 or 0.0.0.0 is determined by the service’s binding configuration. Service Accessibility: If a service is bound to 127.0.0.1, only local applications can reach it, while external systems (like Docker containers) are blocked. For a service to be accessible from a container, it must bind to 0.0.0.0 or be reachable via the host’s network interface (e.g., host.docker.internal). Firewalls — Port and Address Filtering — Network LevelA firewall operates at a lower network level and focuses on controlling traffic based on network rules (IP addresses, ports, protocols, etc.). A firewall can filter traffic based on both the source&#x2F;destination IP and the port number, and it works independently of the services running on the system. Its job is to control access to the network by allowing or blocking traffic based on predefined rules. Key Points: Port and Address Filtering: Firewalls typically allow or block connections based on ports (e.g., HTTP on port 80, SSH on port 22) and source&#x2F;destination IP addresses. For instance, a firewall rule could block access to port 11434 from external sources, or it could block all inbound traffic to the host except for certain ports (e.g., port 80 or 443 for web traffic). Types of Firewalls: Host-based firewalls: These firewalls are installed directly on the machine and manage inbound and outbound traffic for that specific system. For example, iptables or firewalld on Linux. Network-based firewalls: These firewalls are often deployed between networks and control traffic coming into or leaving a network. Impact on Service Accessibility: Even if a service is bound to 0.0.0.0 and listening on all network interfaces, if the firewall blocks access to the port (e.g., port 11434), the service will not be reachable from external sources, including Docker containers. For example, the firewall could block all inbound connections on port 11434, or it could allow only certain trusted IP addresses to access that port. How They Work Together: Binding IP (Service-Level): Determines where a service is accessible (e.g., only locally or from all network interfaces). Controls which networks or devices can connect to the service. Firewall Rules (Network-Level): Determines whether a connection is allowed once a service has been bound to a specific IP and port. Controls which traffic can reach the service, based on the source&#x2F;destination IP address and port. These two components work on different levels but are complementary: The service binding specifies where the service is available (which IP and port). The firewall governs who can connect to that service, based on IP and port filtering rules. Example to Illustrate:Let’s say you have a web server (e.g., Ollama) running on your machine: Service Binding: The web server is configured to bind to 127.0.0.1:11434, meaning it will only accept connections from the local machine (localhost). If you try to access it from a Docker container or another machine, the connection will be refused, because it’s bound only to the localhost address (127.0.0.1), which is inaccessible from external systems. Firewall: The firewall might have rules that allow incoming connections to port 11434 from local connections (127.0.0.1) but block external connections to that port. If the firewall is set to block connections to port 11434 from external networks (e.g., your Docker container), even if the service is bound to 0.0.0.0, the firewall would still prevent the connection. Combined Scenario: If the service is bound to 127.0.0.1 but the firewall allows access from Docker’s internal network (such as host.docker.internal), the container won’t be able to connect to it because the service is not listening on 0.0.0.0 — it’s listening only on 127.0.0.1 for local traffic. If the service is bound to 0.0.0.0 but the firewall blocks port 11434, even though the service is listening on all interfaces, the firewall won’t allow the connection. Summary: Binding to IP (Service-Level): Determines where the service listens and whether it’s accessible from external machines or just the local host. 127.0.0.1: Only local access (not accessible from containers, other machines, or external networks). 0.0.0.0: Accessible from any interface (local machine, containers, and external machines, if permitted by firewall). Firewall Rules (Network-Level): Determines whether traffic can reach the service at all. It filters traffic based on IP address and port. Firewalls are independent of the service’s binding and could block access even if the service is bound to 0.0.0.0. To fix your issue, you need to make sure: The Ollama service binds to 0.0.0.0, allowing external access. The firewall permits connections to port 11434 from the Docker container’s network or other external networks. By correctly configuring both service binding and firewall rules, you can ensure the service is accessible from both the host and Docker containers.","categories":[],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"networking","slug":"networking","permalink":"http://philia897.github.io/tags/networking/"}]},{"title":"Compare Flatpak and AUR","slug":"Compare Flatpak and AUR","date":"2024-12-29T22:51:52.000Z","updated":"2024-12-29T22:52:58.000Z","comments":true,"path":"2024/12/30/Compare Flatpak and AUR/","permalink":"http://philia897.github.io/2024/12/30/Compare%20Flatpak%20and%20AUR/","excerpt":"Both AUR (Arch User Repository) and Flatpak are popular methods to install software on Linux systems, but they differ significantly in terms of how they work, their goals, and use cases.","text":"Both AUR (Arch User Repository) and Flatpak are popular methods to install software on Linux systems, but they differ significantly in terms of how they work, their goals, and use cases. Here’s a detailed comparison to help you decide which is better for your specific needs: 1. AUR (Arch User Repository)How it Works: AUR is a community-driven repository for Arch Linux users. Packages in AUR are distributed as PKGBUILDs, which are scripts that automate downloading the source code, compiling it, and packaging it for installation. Pros: Native Integration: AUR packages integrate directly with your Arch system, using pacman to manage dependencies and updates. Software installed via AUR behaves like a native package. Customizability: You can modify the PKGBUILD before building to tweak configurations or use different versions of software. Lightweight: Does not bundle runtime dependencies (shared libraries are used from the system). Large Selection: Contains software not officially available in Arch repositories, including development versions and niche software. Cons: Stability Risks: AUR packages are user-maintained and may occasionally break due to dependency issues or misconfigurations. Security Concerns: PKGBUILDs could include malicious code if the maintainer isn’t trustworthy (always review PKGBUILDs before building). Compiling Can Be Slow: For large packages, compilation time can be significant (e.g., browsers like Chromium or large games). 2. FlatpakHow it Works: Flatpak is a universal app distribution system designed to work on multiple Linux distributions. Applications are sandboxed and shipped with their runtime dependencies, isolating them from the host system. Pros: Cross-Distribution Compatibility: Works on virtually any Linux distribution, not just Arch. Sandboxing and Security: Flatpak apps are sandboxed, reducing the risk of them impacting the rest of your system. Dependency Independence: Flatpaks bundle their own dependencies, ensuring apps work regardless of your system libraries. Easy Installation: Flatpak offers a consistent installation process across distributions. Version Control: Flatpak makes it easy to install multiple versions of the same app (e.g., stable and beta versions). Cons: Larger Disk Usage: Flatpaks bundle dependencies, which can lead to larger installations compared to AUR. Performance Overhead: The sandboxing layer can cause slight performance degradation in some cases. Limited System Integration: Flatpak apps are isolated and may not integrate as seamlessly with the host system (e.g., themes, file system paths). Smaller Repository: While growing, Flatpak’s repository (Flathub) doesn’t have as wide a selection as AUR. Comparison Table Feature AUR Flatpak Scope Arch Linux only Cross-distribution Dependencies Uses system dependencies Bundled with the app Performance Native performance Slight overhead Security Depends on maintainer Sandboxed apps Disk Usage Minimal Larger due to bundling Ease of Use Requires paru or similar Simple, universal commands Customization Highly customizable Limited Repository Size Very large (community-driven) Growing (Flathub) Which One Is Recommended? Use AUR If: You are comfortable with Arch’s package management system and the occasional need to troubleshoot. You value performance, tight system integration, and lightweight installations. The software is available only in AUR or you want to customize the build process. Use Flatpak If: You prioritize security and isolation (e.g., when testing untrusted software). You need cross-distribution compatibility or want to run multiple versions of an app. You’re okay with larger installations and minor performance trade-offs. Hybrid ApproachYou don’t need to choose one over the other entirely: Use AUR for well-maintained and frequently updated packages that integrate with your system. Use Flatpak for sandboxing apps you don’t fully trust or for software with many dependencies that might conflict with your system libraries. This approach gives you the best of both worlds!","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"aur","slug":"aur","permalink":"http://philia897.github.io/tags/aur/"},{"name":"software","slug":"software","permalink":"http://philia897.github.io/tags/software/"},{"name":"flatpak","slug":"flatpak","permalink":"http://philia897.github.io/tags/flatpak/"}]},{"title":"Python decorator","slug":"Python decorator","date":"2024-12-20T22:54:22.000Z","updated":"2024-12-20T23:18:00.000Z","comments":true,"path":"2024/12/21/Python decorator/","permalink":"http://philia897.github.io/2024/12/21/Python%20decorator/","excerpt":"","text":"YouPlease explain how python decorator works? ChatGPT:A Python decorator is a design pattern that allows you to modify or extend the behavior of functions or methods without changing their code. It is a higher-order function, meaning it takes a function as an argument and returns a new function or the original function with some additional behavior. Key Concepts of Decorators Functions are First-Class Citizens:In Python, functions can be passed as arguments, returned from other functions, and assigned to variables. Wrapper Functions:A decorator often defines an inner function (a “wrapper”) that adds functionality to the original function before calling it. The @decorator Syntax:A decorator is typically applied to a function using the @decorator_name syntax. How Decorators WorkHere’s a step-by-step explanation: A decorator is a function that takes another function as input. Inside the decorator, you define a wrapper function that modifies the behavior of the original function. The wrapper function is returned and replaces the original function. Example of a Basic Decorator123456789101112131415# Decorator functiondef my_decorator(func): def wrapper(): print(&quot;Something is happening before the function is called.&quot;) func() print(&quot;Something is happening after the function is called.&quot;) return wrapper# Using the decorator@my_decoratordef say_hello(): print(&quot;Hello!&quot;)# Call the decorated functionsay_hello() Output: 123Something is happening before the function is called.Hello!Something is happening after the function is called. Explanation: @my_decorator is shorthand for say_hello = my_decorator(say_hello). When say_hello() is called, the wrapper function inside my_decorator executes instead. The wrapper adds behavior before and after calling the original say_hello(). Example with ArgumentsIf the decorated function takes arguments, the wrapper must also accept and pass those arguments. 12345678910111213def my_decorator(func): def wrapper(*args, **kwargs): print(&quot;Before the function call&quot;) result = func(*args, **kwargs) print(&quot;After the function call&quot;) return result return wrapper@my_decoratordef greet(name): print(f&quot;Hello, &#123;name&#125;!&quot;)greet(&quot;Alice&quot;) Output: 123Before the function callHello, Alice!After the function call Practical Use Cases Logging:Automatically log function calls and their results. Authorization:Check permissions before allowing access to certain functions. Memoization:Cache function results to optimize performance. Example: Built-in DecoratorsPython has several built-in decorators, such as: @staticmethod: Defines a static method in a class. @classmethod: Defines a class method. @property: Turns a method into a read-only property. Example of @property:12345678910class Circle: def __init__(self, radius): self._radius = radius @property def area(self): return 3.14 * (self._radius ** 2)circle = Circle(5)print(circle.area) # Automatically calls the method Decorators provide a powerful and flexible way to enhance functions and methods while keeping code readable and maintainable. YouPlease give me more examples, I saw some decorators allow arguments right? ChatGPT:Yes! Some decorators allow arguments, which makes them even more flexible. To understand this, let’s first revisit the structure of a decorator. Decorators with arguments are essentially functions that return a decorator function. This allows them to accept their own parameters. Here are more examples, including how to use decorators with arguments: Example 1: Decorator with ArgumentsLet’s create a decorator that takes arguments and customizes its behavior: 12345678910111213def repeat(times): # Decorator factory def decorator(func): def wrapper(*args, **kwargs): for _ in range(times): func(*args, **kwargs) return wrapper return decorator@repeat(times=3) # Pass arguments to the decoratordef greet(name): print(f&quot;Hello, &#123;name&#125;!&quot;)greet(&quot;Alice&quot;) Output: 123Hello, Alice!Hello, Alice!Hello, Alice! Explanation: repeat is a decorator factory that takes an argument (times). It returns the actual decorator function. The decorator defines the wrapper function that adds behavior to greet. Example 2: Timing a FunctionThis decorator logs how long a function takes to execute. 1234567891011121314151617import timedef timer(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f&quot;&#123;func.__name__&#125; executed in &#123;end_time - start_time:.2f&#125; seconds&quot;) return result return wrapper@timerdef slow_function(): time.sleep(2) print(&quot;Finished slow function!&quot;)slow_function() Output: 12Finished slow function!slow_function executed in 2.00 seconds Example 3: Restrict Access with ArgumentsCreate a decorator to restrict access based on a role. 12345678910111213141516def require_role(role): def decorator(func): def wrapper(user_role, *args, **kwargs): if user_role != role: print(&quot;Access denied!&quot;) return return func(user_role, *args, **kwargs) return wrapper return decorator@require_role(&quot;admin&quot;)def view_dashboard(user_role): print(&quot;Welcome to the admin dashboard!&quot;)view_dashboard(&quot;guest&quot;) # Access deniedview_dashboard(&quot;admin&quot;) # Welcome to the admin dashboard! Output: 12Access denied!Welcome to the admin dashboard! Example 4: Caching with ArgumentsA decorator with arguments can be used to cache results for a specified duration. 1234567891011121314151617181920212223242526272829import timefrom functools import lru_cachedef cache_results(duration): def decorator(func): cache = &#123;&#125; def wrapper(*args): current_time = time.time() if args in cache and (current_time - cache[args][1] &lt; duration): return cache[args][0] result = func(*args) cache[args] = (result, current_time) return result return wrapper return decorator@cache_results(duration=10) # Cache results for 10 secondsdef expensive_computation(x): print(f&quot;Computing &#123;x&#125;...&quot;) time.sleep(2) return x ** 2print(expensive_computation(4)) # Computes resultprint(expensive_computation(4)) # Returns cached resulttime.sleep(11)print(expensive_computation(4)) # Recomputes as cache expired Output: 12345Computing 4...1616Computing 4...16 Example 5: Chaining Multiple DecoratorsYou can apply multiple decorators to a single function. They are applied in a bottom-up order. 123456789101112131415161718def uppercase(func): def wrapper(*args, **kwargs): result = func(*args, **kwargs) return result.upper() return wrapperdef add_exclamation(func): def wrapper(*args, **kwargs): result = func(*args, **kwargs) return result + &quot;!&quot; return wrapper@add_exclamation@uppercasedef say_hello(): return &quot;hello&quot;print(say_hello()) # &quot;HELLO!&quot; Tips for Writing and Using Decorators Use functools.wraps:To preserve the original function’s name and docstring, use the functools.wraps decorator inside your decorator: 12345678from functools import wrapsdef my_decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(&quot;Decorating...&quot;) return func(*args, **kwargs) return wrapper Combining with Built-ins:Combine custom decorators with Python’s built-in ones like @staticmethod or @property. Decorators with arguments give you the flexibility to parameterize and customize behavior. You can build powerful tools for logging, access control, performance optimization, and more!","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"},{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"}]},{"title":"Data structure in Linux for app developing","slug":"Data structure in Linux for app developing","date":"2024-12-16T22:42:23.000Z","updated":"2024-12-16T22:42:55.000Z","comments":true,"path":"2024/12/17/Data structure in Linux for app developing/","permalink":"http://philia897.github.io/2024/12/17/Data%20structure%20in%20Linux%20for%20app%20developing/","excerpt":"About standards when developing Linux applications.","text":"About standards when developing Linux applications. If I’m developing an application, how to manage my data?I saw there are many dirs in Linux user home, like .config, .cache, .local, seems some dirs like &#x2F;var, &#x2F;tmp can also be used, give me some guidelines When developing an application on Linux, managing application data properly is crucial for maintaining system cleanliness, following best practices, and providing a smooth user experience. Linux has a set of standard directories where applications are expected to store different types of data. Below is a breakdown of the most commonly used directories for storing user and system data, along with guidelines for managing your application’s data: 1. User-specific Data Locationsa. ~/.config/ Purpose: This directory is typically used to store configuration files for applications. It’s meant for settings, preferences, and other configuration files that need to persist across sessions. Guideline: Store configuration files and user-specific settings for your application here. For example, if your app allows users to customize the interface or preferences, save those settings in a subdirectory under ~/.config/yourapp/. Example: ~/.config/yourapp/config.json, ~/.config/yourapp/settings.ini. b. ~/.local/share/ Purpose: This directory is for application data that is specific to the user. It’s often used for things like databases, log files, or other persistent data that needs to be stored between application runs. Guideline: Store your application’s primary data here, such as user-generated files, temporary working files, or data that should persist between runs. Create a subdirectory with your application’s name. Example: ~/.local/share/yourapp/ for persistent data files, such as ~/.local/share/yourapp/user_data.db. c. ~/.local/cache/ Purpose: This directory is for cached data that can be rebuilt if necessary, like temporary files, downloaded resources, or files that don’t need to persist across sessions. Guideline: Store temporary files here that your application can safely recreate or download again when needed. For example, downloaded images, files, or data fetched from the internet. Example: ~/.local/cache/yourapp/ for cached files like ~/.local/cache/yourapp/images/ or ~/.local/cache/yourapp/temp_data.json. d. ~/.local/bin/ Purpose: This directory is for user-specific executables that are not installed system-wide. Guideline: If your application has a command-line interface (CLI) or needs to install executables for the user, store those files here. Example: ~/.local/bin/yourapp for user-installed executables. 2. System-wide Data Locations (Requires root access)a. /etc/ Purpose: The /etc/ directory contains system-wide configuration files. These files control system behavior and the configuration of installed applications. Guideline: If your application requires system-wide configuration and must be shared by all users on the system, you can store configuration files in /etc/yourapp/. However, this is typically for system administrators and requires elevated privileges. Example: /etc/yourapp/config.conf. b. /var/ Purpose: The /var/ directory is for variable data that changes over time, such as logs, databases, caches, and application-specific files that need to persist but are constantly being modified. Guideline: If your application generates logs, databases, or other stateful data that needs to be preserved, use /var/lib/yourapp/ for persistent data or /var/log/yourapp/ for logs. This is mostly for system-wide data that is managed by a service or application running with root privileges. Example: /var/lib/yourapp/data.db, /var/log/yourapp.log. c. /tmp/ Purpose: The /tmp/ directory is for temporary files that can be safely deleted. Files in /tmp/ are typically short-lived and used by applications to store temporary data during runtime. Guideline: Use /tmp/ for data that doesn’t need to persist between reboots and can be discarded safely when the application closes or after a certain amount of time. Example: /tmp/yourapp_tempfile. 3. Other Considerations Permissions: Make sure to set proper permissions for files and directories. For example, avoid writing files that are sensitive in publicly accessible directories. Portable Applications: If you’re building a portable application (such as one distributed via Flatpak or Snap), you may need to store data in the sandboxed directories provided by those systems (e.g., ~/.var/app/ for Flatpak). Backups and Synchronization: For user data that needs to be backed up or synchronized across devices, consider storing it in ~/.local/share/ or integrating with a cloud synchronization service. Best Practices for Data Management Follow the XDG Base Directory Specification: The XDG specification standardizes where user-specific application data should be stored, ensuring consistency across Linux distributions. It specifies: Config files: ~/.config/ Data files: ~/.local/share/ Cache files: ~/.cache/ If possible, follow these conventions to ensure your application behaves in a consistent manner with other applications on the system. Separation of Data Types: Keep different types of data in different directories: Configuration files in ~/.config/yourapp/. Persistent data in ~/.local/share/yourapp/. Cache and temporary files in ~/.cache/yourapp/. Clean Up Temporary Data: Ensure that temporary files in /tmp/ or ~/.cache/ are removed when they are no longer needed to avoid unnecessary disk usage. Cross-user Support: If your application is multi-user, ensure that data that needs to be shared between users is placed in appropriate system directories (e.g., /var/lib/yourapp/), while user-specific data should remain in the user’s home directory. Summary of Where to Store Data: User-specific data: ~/.config/yourapp/ (config), ~/.local/share/yourapp/ (persistent data), ~/.cache/yourapp/ (cache) System-wide data: /etc/yourapp/ (config), /var/lib/yourapp/ (persistent state), /var/log/yourapp/ (logs), /tmp/yourapp/ (temporary files) By following these guidelines, your application will adhere to standard practices for Linux file system organization and ensure good separation between user, system, and temporary data. Let me know if you have more specific questions about data management for your app!","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"}]},{"title":"Flatpak","slug":"Flatpak","date":"2024-12-16T22:30:47.000Z","updated":"2024-12-16T22:32:19.000Z","comments":true,"path":"2024/12/17/Flatpak/","permalink":"http://philia897.github.io/2024/12/17/Flatpak/","excerpt":"About how flatpak works.","text":"About how flatpak works. Flatpak is open-source! Flatpak is developed under the GNU Lesser General Public License (LGPL) 2.1. This license ensures that the Flatpak platform is free software, allowing anyone to view, modify, and redistribute its source code while maintaining certain protections for developers. The Flatpak project itself is hosted on GitHub, where you can access the source code, contribute, and review its development. Its open-source nature also means it has been audited and improved by a large community of developers, ensuring transparency and security. I’m curious about how flatpak works and will it effect the stability or efficiency? Flatpak is designed to provide a sandboxed environment for running applications. Understanding how it works and its impact on stability and efficiency will clarify its benefits and limitations. Here’s a breakdown: How Flatpak Works Sandboxing Applications: Each Flatpak application runs in its own isolated environment (sandbox). It has restricted access to system resources (e.g., files, devices) unless explicitly allowed through permissions. Sandboxing is achieved using Linux kernel features like namespaces, cgroups, and seccomp. Bundle Everything Needed: Applications are packaged with most of their dependencies, reducing reliance on the host system’s libraries. Flatpak apps use “runtimes,” which are shared, versioned, and pre-packaged collections of libraries common to many applications (e.g., GNOME or KDE runtimes). Portals for Interaction: Flatpak apps use portals to interact with the host system in a controlled manner. For example, file selection, network access, or printing is mediated through these secure APIs. Distribution: Applications are distributed via repositories, with Flathub being the most popular one. Flatpak uses OSTree, a content-addressed version-control system, to manage application and runtime installations efficiently. Advantages of Flatpak Stability: Flatpak apps are decoupled from the host system, so changes to system libraries or updates are unlikely to break the app. Conversely, apps won’t cause issues for the system since they don’t alter system files. Compatibility: Flatpak apps work across many Linux distributions, avoiding issues with package managers or distro-specific quirks. Easy Updates: Applications and runtimes can be updated independently and incrementally. Security: Sandboxing reduces the risk of an application compromising the host system. Permissions can be managed explicitly, limiting what apps can access. Potential Drawbacks Efficiency: Performance Overhead: Sandboxing and the extra layer of runtime libraries can introduce slight performance overhead compared to native applications. For most apps, this overhead is negligible, but high-performance software (e.g., Blender for rendering) may see minor slowdowns. Disk Usage: Flatpak apps include their dependencies, which can lead to larger disk usage compared to native system packages. However, shared runtimes mitigate this issue for apps using the same runtime. Isolation Challenges: Some apps might need more permissions to function correctly (e.g., hardware access or file permissions). Developers need to ensure proper sandboxing settings, or users might have to grant additional permissions manually. Redundancy: If the same libraries exist in both the Flatpak runtime and the host system, this duplication can waste space. Does It Affect Stability or Efficiency? Stability: Flatpak is highly stable because of its isolated approach. Applications don’t interfere with system packages or other software, making Flatpak ideal for protecting the core system. Efficiency: Most apps will run without noticeable performance degradation. Disk usage might increase, but incremental updates and shared runtimes help minimize this. For applications where performance is critical (e.g., Blender or games), it’s worth testing the Flatpak version to see if any performance difference matters in your use case. If you need the absolute best performance, native packages or AppImage might be better alternatives. ConclusionFlatpak provides a modern, secure, and user-friendly way to run applications without risking system stability. While there might be minor efficiency trade-offs, they are usually outweighed by the benefits of stability, ease of use, and security. If you’re cautious about your system’s integrity, Flatpak is an excellent choice. so where will the flatpak download and save applications and their data Great to hear that you’ve set up the Flatpak mirror! Now, let’s take a look at where Flatpak downloads and saves applications and their data. Flatpak Storage Locations Applications Installation Location:Flatpak installs applications in the following directory: Applications: /var/lib/flatpak/app/ Here, the application data is stored in separate directories based on the application ID (e.g., org.blender.Blender). This is where Flatpak installs the application files and their associated runtimes. Runtimes Installation Location:Flatpak uses runtimes (a set of shared libraries that many applications use) which are also stored in a specific directory: Runtimes: /var/lib/flatpak/runtime/ Runtimes are shared across multiple applications, reducing disk usage by avoiding duplication. User-Specific Data Location:Each user has their own Flatpak data and configuration directory to ensure that multiple users on the same machine don’t conflict with each other: User Applications: ~/.local/share/flatpak/app/ User Runtimes: ~/.local/share/flatpak/runtime/ This directory contains the application data, user-specific configurations, and any custom data the application might generate. This is also where Flatpak keeps user-specific settings and app-specific data (e.g., configuration files or caches). Flatpak Cache (Downloads and Temporary Files):Flatpak also stores downloaded package files and temporary data to speed up installations and updates: Flatpak Cache: ~/.var/app/ This directory is used to store temporary files, cached assets, and configurations used by the Flatpak apps. Customizing Storage LocationsIf you want to change where Flatpak stores applications and runtimes, you can modify its configuration files: Flatpak’s System-Wide Location: You can set a custom directory by using the flatpak --user or flatpak --system flags. For example, to set a custom path for a user’s Flatpak apps, you can run: 1export FLATPAK_USER_DIR=/path/to/custom/flatpak Or, if you want to specify a different location for system-wide installations, you can set the FLATPAK_SYSTEM_DIR environment variable. Summary of Flatpak Storage Locations System-Wide Application Installation: /var/lib/flatpak/app/ User-Specific Application Installation: ~/.local/share/flatpak/app/ System-Wide Runtimes: /var/lib/flatpak/runtime/ User-Specific Runtimes: ~/.local/share/flatpak/runtime/ Temporary&#x2F;Cache Files: ~/.var/app/ In general, most of the application files will be stored under ~/.local/share/flatpak/ for a user installation, while system-wide installations will go to /var/lib/flatpak/. Let me know if you need further details or assistance!","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"flatpak","slug":"flatpak","permalink":"http://philia897.github.io/tags/flatpak/"},{"name":"system","slug":"system","permalink":"http://philia897.github.io/tags/system/"}]},{"title":"ComfyUI","slug":"ComfyUI","date":"2024-12-16T18:58:00.000Z","updated":"2024-12-16T19:08:13.000Z","comments":true,"path":"2024/12/17/ComfyUI/","permalink":"http://philia897.github.io/2024/12/17/ComfyUI/","excerpt":"非常好玩，部署和启动也比较简单。这就是Python的好处吧，其实还挺方便部署的。 电脑上直接先进入运行环境，再用Python运行就行。不过需要python12。我现在用的是自带的venv做的isoluation。之后可能最好还是换miniconda比较好。","text":"非常好玩，部署和启动也比较简单。这就是Python的好处吧，其实还挺方便部署的。 电脑上直接先进入运行环境，再用Python运行就行。不过需要python12。我现在用的是自带的venv做的isoluation。之后可能最好还是换miniconda比较好。 12source .venv/bin/activatepython main.py 就很方便，很fansy。然后各种自定义节点也可以直接搜索下载。各种模型之类的也很直观。非常不错。 123ComfyUI-GGUFComfyUI-Easy-UseComfyUI_ResolutionSelector 这几个插件不错，可以装 GGUF 允许我们使用GGUF模型，更小更方便使用Easy-Use 提供一些方便的功能和组件，尤其是右键菜单加入了一个force clean VRAM的功能，以及clean VRAM的节点，这样我们使用完一个模型可以清掉缓存再跑下一个模型，避免Out of Memory的问题ResolutionSelector 允许选择分辨率并输出height和width，很有用 cfg最好设置1或者2，不能用默认的8，不然会生成纯白的照片… 如果遇到mut1 mut2不匹配的问题，大概率就是模型选错了，感觉一般是CLIP模型选错了或者设置错了（模型类型设置成sd3或者flux），改一下应该就行。","categories":[{"name":"AI","slug":"AI","permalink":"http://philia897.github.io/categories/AI/"}],"tags":[{"name":"comfyUI","slug":"comfyUI","permalink":"http://philia897.github.io/tags/comfyUI/"}]},{"title":"How adding tests in Rust","slug":"How adding tests in Rust","date":"2024-12-13T02:07:56.000Z","updated":"2024-12-13T02:09:28.000Z","comments":true,"path":"2024/12/13/How adding tests in Rust/","permalink":"http://philia897.github.io/2024/12/13/How%20adding%20tests%20in%20Rust/","excerpt":"How to add unit and integration tests in Rust.","text":"How to add unit and integration tests in Rust. You:I’m developing an application, which use rust. I want to optimize the code quality so I wish to add some tests for the functions and classes I wrote, how? Is there any tool I can use to auto-generate the tests or simplify the work of adding tests? ChatGPT:To improve the quality of your Rust application through testing, and to simplify the process of creating tests, you can consider the following approaches: 1. Manual Test Writing in Rust Rust has robust support for testing via its built-in framework. Add tests to your code by creating a tests module within the same file as your functions&#x2F;classes or in the tests/ directory for integration tests. Example:123456789#[cfg(test)]mod tests &#123; use super::*; #[test] fn test_function() &#123; assert_eq!(my_function(2), 4); &#125;&#125; Use assert_eq!, assert_ne!, and assert! macros to validate the behavior. 2. Tools to Generate TestsWhile Rust doesn’t have direct “auto-test generation” out of the box, some tools and approaches can help simplify this task: Crates for Test Assistance: proptest: A property-based testing framework for Rust. Instead of writing tests manually, you define properties that your code should satisfy, and it generates test cases automatically. quickcheck: Another property-based testing library similar to Proptest. It generates inputs for testing and verifies the properties you define. AI-Powered Test Generation: ChatGPT or GitHub Copilot: Tools like Copilot can analyze your Rust functions and suggest test cases automatically based on function signatures and comments. Test Generation Tools: cargo-expand: Not a test generation tool per se, but useful for inspecting macro expansions and ensuring your test code is correctly expanded. 3. Simplify Writing Tests Use macros to automate repetitive test patterns. For example:1234567891011macro_rules! generate_test &#123; ($name:ident, $input:expr, $expected:expr) =&gt; &#123; #[test] fn $name() &#123; assert_eq!(my_function($input), $expected); &#125; &#125;;&#125;generate_test!(test_case_1, 2, 4);generate_test!(test_case_2, 3, 9); Leverage parameterized tests with libraries like rstest. 4. Static Analysis Tools for Better Testability Clippy: Run cargo clippy to analyze your code for potential issues and get suggestions for improvements. MIRI: Run cargo miri test to detect undefined behavior in your tests. 5. Test Coverage Analysis Use tarpaulin to measure how much of your code is covered by tests.12cargo install cargo-tarpaulincargo tarpaulin 6. CI&#x2F;CD Integration Integrate automated testing into your CI&#x2F;CD pipeline using tools like GitHub Actions or GitLab CI. Example Workflow Write Initial Functions: 123pub fn add(a: i32, b: i32) -&gt; i32 &#123; a + b&#125; Use AI Tools for Suggestions: Use Copilot or ChatGPT to suggest test cases:1234567891011121314#[cfg(test)]mod tests &#123; use super::*; #[test] fn test_add_positive_numbers() &#123; assert_eq!(add(1, 2), 3); &#125; #[test] fn test_add_negative_numbers() &#123; assert_eq!(add(-1, -2), -3); &#125;&#125; Enhance Test Coverage: Use property-based testing to generalize:1234567use quickcheck::quickcheck;quickcheck! &#123; fn prop_add_is_commutative(a: i32, b: i32) -&gt; bool &#123; add(a, b) == add(b, a) &#125;&#125; These tools and techniques can significantly streamline the process of writing and maintaining tests, ensuring that your Rust application is both robust and maintainable. You:I prefer use tests&#x2F;, how? ChatGPT:Using the tests/ directory is a great choice for organizing integration tests in Rust. Here’s a step-by-step guide to setting it up and using it effectively: 1. Directory StructureEnsure your project follows the typical Cargo project structure: 12345678my_project/├── src/│ ├── lib.rs│ ├── main.rs├── tests/│ ├── integration_test_1.rs│ ├── integration_test_2.rs├── Cargo.toml src/lib.rs: This should contain the library code you want to test. tests/: This directory is for integration test files. Each .rs file in this directory represents a separate integration test. 2. Writing Integration TestsIntegration tests focus on testing the public API of your library. They are independent of each other and interact with your library code as if they were an external consumer. ExampleSuppose you have a library in src/lib.rs: 12345678// src/lib.rspub fn add(a: i32, b: i32) -&gt; i32 &#123; a + b&#125;pub fn subtract(a: i32, b: i32) -&gt; i32 &#123; a - b&#125; Create a test file, e.g., tests/math_tests.rs: 1234567891011121314// tests/math_tests.rsuse my_project; // Import the library#[test]fn test_add() &#123; assert_eq!(my_project::add(2, 3), 5); assert_eq!(my_project::add(-1, -2), -3);&#125;#[test]fn test_subtract() &#123; assert_eq!(my_project::subtract(10, 3), 7); assert_eq!(my_project::subtract(0, 5), -5);&#125; 3. Running Integration TestsRun all tests (unit and integration) with: 1cargo test To run only the integration tests: 1cargo test --test math_tests Here, math_tests refers to the test file name math_tests.rs without the .rs extension. 4. Best Practicesa. Use Setup Functions for Common Test DataIf your tests need repeated setup, use helper functions or modules. Example: 12345// tests/common/mod.rspub fn setup() &#123; // Perform setup tasks, e.g., initialize logging or test resources. println!(&quot;Setting up the test environment...&quot;);&#125; Use this setup in your test files: 12345678// tests/math_tests.rsmod common;#[test]fn test_add_with_setup() &#123; common::setup(); assert_eq!(my_project::add(2, 3), 5);&#125; b. Test Large Scenarios Across Multiple ComponentsIntegration tests are ideal for testing multiple components working together. For example, if your project processes input files, you can test end-to-end behavior: 123456#[test]fn test_file_processing() &#123; let result = my_project::process_file(&quot;input.txt&quot;); assert!(result.is_ok()); assert_eq!(result.unwrap(), &quot;expected_output&quot;);&#125; c. Use Feature FlagsIf your codebase has optional features, write feature-specific tests with #[cfg(feature = &quot;feature_name&quot;)]. 5. Automating Test GenerationTo simplify adding tests to the tests/ directory: Use macros for repetitive tests. Use AI tools (e.g., GitHub Copilot) to generate skeletons for test files. Example Macro:1234567891011121314macro_rules! generate_integration_tests &#123; ($mod_name:ident, $fn_name:ident, $input:expr, $expected:expr) =&gt; &#123; #[test] fn $fn_name() &#123; assert_eq!(my_project::$mod_name($input), $expected); &#125; &#125;;&#125;// tests/integration_test.rsmod common;generate_integration_tests!(add, test_add_2_3, 2, 5);generate_integration_tests!(subtract, test_subtract_10_5, 10, 5); 6. CI&#x2F;CD IntegrationEnsure your tests run as part of CI&#x2F;CD pipelines. For example: GitHub Actions:1234567891011121314name: Rust CIon: [push, pull_request]jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions-rs/toolchain@v1 with: toolchain: stable override: true - run: cargo test By structuring your tests this way, you’ll have a clean and maintainable testing framework that supports large-scale integration testing. Let me know if you’d like further customization! You:what is the difference between the integration test and unit test? ChatGPT:The primary difference between unit tests and integration tests lies in their focus and scope: 1. Unit TestsUnit tests are focused, isolated tests that validate the behavior of individual components (e.g., functions, methods, or classes). They aim to ensure that a specific unit of code works as expected in isolation. Key Characteristics: Scope: Tests one small unit of functionality. Dependencies: Avoids external dependencies (e.g., databases, APIs). Mocks and stubs are often used to simulate these. Purpose: Catch bugs at the earliest stage by verifying the correctness of individual functions or methods. Location: Typically located in the same file as the code they test, inside a #[cfg(test)] module. Example:Suppose you have a function in src/lib.rs: 123pub fn add(a: i32, b: i32) -&gt; i32 &#123; a + b&#125; A unit test would look like this: 123456789#[cfg(test)]mod tests &#123; use super::*; #[test] fn test_add() &#123; assert_eq!(add(2, 3), 5); // Tests a single function in isolation &#125;&#125; 2. Integration TestsIntegration tests validate how multiple components of a system work together. They test the behavior of your application as a whole or specific interactions between parts of your system. Key Characteristics: Scope: Covers the interactions between components or the entire system. Dependencies: Often interacts with external systems (e.g., databases, APIs, file systems). Purpose: Ensures that different parts of the system work together as intended. Location: Placed in the tests/ directory at the root of the project. Example:Given the same library, you might test its integration as follows: 12345678// tests/integration_test.rsuse my_project; // Import your library#[test]fn test_add_and_subtract() &#123; assert_eq!(my_project::add(10, 5), 15); assert_eq!(my_project::subtract(15, 5), 10);&#125; Here, you’re testing how multiple functions interact, focusing on the system’s public API. 3. Key Differences Aspect Unit Test Integration Test Focus Tests a single function&#x2F;method in isolation. Tests how components interact or full system behavior. Scope Small, focused, and granular. Larger, covering multiple modules or features. Dependencies Avoids external systems (uses mocks&#x2F;stubs). May interact with real databases, APIs, etc. Location Inside the source file, in #[cfg(test)]. Separate files in the tests/ directory. Speed Runs quickly due to isolation. Slower, especially with external dependencies. Purpose Catch bugs early in development. Ensure components work together correctly. 4. When to Use Each Unit Tests: Validate core business logic and individual algorithms. Ensure individual modules behave as expected before integrating them. Integration Tests: Verify that integrated modules, components, or APIs interact correctly. Test end-to-end scenarios to confirm the system meets user requirements. 5. Combined Testing StrategyBoth unit and integration tests are crucial for robust software: Unit Tests catch bugs early during development, ensuring the building blocks are solid. Integration Tests validate that those blocks work together and ensure the entire system functions as expected.","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"},{"name":"debugging","slug":"debugging","permalink":"http://philia897.github.io/tags/debugging/"},{"name":"rust","slug":"rust","permalink":"http://philia897.github.io/tags/rust/"}]},{"title":"KVM/QEMU libvirt virt-manager","slug":"KVM_QEMU libvirt virt-manager","date":"2024-12-09T03:01:38.000Z","updated":"2024-12-09T03:10:33.000Z","comments":true,"path":"2024/12/09/KVM_QEMU libvirt virt-manager/","permalink":"http://philia897.github.io/2024/12/09/KVM_QEMU%20libvirt%20virt-manager/","excerpt":"How to setup KVM&#x2F;QEMU on Arch.","text":"How to setup KVM&#x2F;QEMU on Arch. I’m an Arch user, and now I want to setup an VM using KVM&#x2F;Qemu. The VM shall use Arch OS as well and I have prepared the ISO file. Show me how to make it First of all, remember to disable your firewall! As it will affect the setup of networking in the VM To set up an Arch Linux virtual machine (VM) using KVM&#x2F;QEMU, you can follow these steps. Here’s a detailed guide: Step 1: Ensure KVM&#x2F;QEMU is Installed Install the necessary packages: 1sudo pacman -S qemu virt-manager libvirt edk2-ovmf dnsmasq bridge-utils Enable and start the libvirtd service: 1sudo systemctl enable --now libvirtd Verify that virtualization is supported: 1lscpu | grep Virtualization Ensure the output shows VT-x (Intel) or AMD-V (AMD). Add your user to the libvirt group to manage VMs without root: 1sudo usermod -aG libvirt $(whoami) Log out and log back in for this to take effect. Step 2: Prepare the Arch ISOEnsure you have the Arch ISO file ready. For example, suppose the file is at /home/username/Downloads/archlinux.iso. Step 3: Create a Virtual Machine Launch virt-manager: 1virt-manager This opens the Virtual Machine Manager GUI. Create a new VM: Click File → New Virtual Machine. Select Local install media (ISO image or CDROM) and click Forward. Choose Use ISO image, browse for your Arch Linux ISO, and click Forward. Assign resources: Allocate memory (RAM) and CPUs. For example: RAM: 2048 MB (2 GB) or more CPUs: 2 or more Click Forward. Set up storage: Choose Create a disk image for the virtual machine. Allocate disk space (e.g., 20 GB or more). Click Forward. Configure advanced options: Set Firmware to UEFI if desired (use edk2-ovmf for Arch). Review the configuration and click Finish to create the VM. Step 4: Boot the VM Start the VM: In virt-manager, select the VM and click Start. Follow the Arch Linux installation process: Boot from the ISO. Partition the virtual disk, configure the system, and install Arch as you would on a physical machine. Step 5: Network ConfigurationKVM uses NAT for network by default, allowing your VM to access the internet. If you need a bridged network for direct access to your local network, configure it in virt-manager under Network Interfaces. Optional: Configure the VM for Optimal Performance Install virtio drivers in the Arch VM: During installation, ensure the kernel has virtio modules enabled for better performance (e.g., virtio_blk, virtio_net). Enable SPICE or QXL for better graphics: In virt-manager, under Display, choose Spice and install the spice-vdagent package in the guest for clipboard and resolution integration. Let me know if you need guidance on a specific step or if you’d prefer a terminal-only setup! Ah I have ufw firewall in my host machine, will that affect? Yes, having ufw (Uncomplicated Firewall) enabled on your host machine can absolutely affect the VM’s ability to communicate with the network, especially when using NAT or bridge mode. By default, ufw blocks some types of traffic that are essential for virtual machine networking, including: DHCP traffic (UDP ports 67 and 68) for IP address assignment. DNS traffic (UDP port 53) for name resolution. NAT or bridge traffic between the VM and the host. To fix this, you need to adjust the ufw settings to allow the necessary traffic. Here’s how: Step 1: Identify the Problem Check if ufw is active: 1sudo ufw status If it says active, it is managing your network traffic. Test if disabling ufw temporarily resolves the issue: 1sudo ufw disable If the VM obtains an IP address after this, ufw is blocking critical traffic. Don’t leave ufw disabled permanently; adjust the rules instead. Step 2: Allow Necessary Traffic for NAT (virbr0)For NAT-based networking (default libvirt network): Allow traffic on the virbr0 interface: 12sudo ufw allow in on virbr0sudo ufw allow out on virbr0 Allow DHCP traffic: (not necessary) 12sudo ufw allow in on virbr0 to any port 67 proto udpsudo ufw allow out on virbr0 to any port 68 proto udp Allow DNS traffic: (not necessary) 12sudo ufw allow in on virbr0 to any port 53 proto udpsudo ufw allow out on virbr0 to any port 53 proto udp Reload ufw: 1sudo ufw reload Step 3: Allow Traffic for Bridge ModeFor bridge networking, you’ll need to allow traffic on the bridge interface (br0 or equivalent): Allow all traffic on the bridge interface: 12sudo ufw allow in on br0sudo ufw allow out on br0 Allow DHCP and DNS traffic: 1234sudo ufw allow in on br0 to any port 67 proto udpsudo ufw allow out on br0 to any port 68 proto udpsudo ufw allow in on br0 to any port 53 proto udpsudo ufw allow out on br0 to any port 53 proto udp Reload ufw: 1sudo ufw reload Step 4: Allow Forwarding for NATFor NAT-based networks, you also need to ensure that the host allows traffic to forward through the NAT interface: Edit the ufw configuration file: 1sudo nano /etc/default/ufw Find the line: 1DEFAULT_FORWARD_POLICY=&quot;DROP&quot; Replace it with: 1DEFAULT_FORWARD_POLICY=&quot;ACCEPT&quot; Allow forwarding for libvirt’s NAT traffic: 12sudo ufw route allow in on virbr0 out on enp3s0sudo ufw route allow out on enp3s0 in on virbr0 Reload ufw: 1sudo ufw reload Step 5: Test Connectivity Restart the VM and test again:12ping -c 3 8.8.8.8ping -c 3 archlinux.org Verify that the VM has received an IPv4 address:1ip addr show Step 6: Debug Further (if Needed) Check UFW Logs:To identify blocked traffic:12sudo ufw status verbosesudo journalctl -f | grep UFW Tailor the Rules:If logs show specific blocked traffic, adjust the rules to allow it. With these adjustments, ufw should no longer block the necessary traffic, and your VM should be able to access the network properly. Let me know if you need additional help!","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"kvm","slug":"kvm","permalink":"http://philia897.github.io/tags/kvm/"},{"name":"qemu","slug":"qemu","permalink":"http://philia897.github.io/tags/qemu/"},{"name":"virtualization","slug":"virtualization","permalink":"http://philia897.github.io/tags/virtualization/"}]},{"title":"Compare Xen, KVM/Qemu, Virtualbox and VMware","slug":"Compare Xen, KVM_Qemu, Virtualbox and VMware","date":"2024-12-09T01:07:22.000Z","updated":"2024-12-09T01:07:37.000Z","comments":true,"path":"2024/12/09/Compare Xen, KVM_Qemu, Virtualbox and VMware/","permalink":"http://philia897.github.io/2024/12/09/Compare%20Xen,%20KVM_Qemu,%20Virtualbox%20and%20VMware/","excerpt":"Here’s a comprehensive comparison of Xen, KVM&#x2F;QEMU, VirtualBox, and VMware across key aspects to help you choose the right virtualization platform.","text":"Here’s a comprehensive comparison of Xen, KVM&#x2F;QEMU, VirtualBox, and VMware across key aspects to help you choose the right virtualization platform. 1. Overview Feature Xen KVM&#x2F;QEMU VirtualBox VMware Type Type-1 (bare-metal hypervisor) Type-2 (hosted, kernel-based Type-1) Type-2 (hosted hypervisor) Both Type-1 (ESXi) and Type-2 (Workstation&#x2F;Player) Open Source Yes Yes Partially (core is open-source, but full feature set is proprietary) Partially (some tools are free, but core products are proprietary) Performance Near-native Near-native Lower than Xen&#x2F;KVM Near-native with VMware ESXi; good with Workstation&#x2F;Player 2. Architecture Feature Xen KVM&#x2F;QEMU VirtualBox VMware Host OS Runs alongside a control domain (dom0). Needs bare-metal access. Runs as part of the Linux kernel. Runs on a host OS (Linux, Windows, macOS). ESXi is bare-metal; Workstation&#x2F;Player runs on a host OS. Guest OS Types Supports both paravirtualization and hardware-assisted virtualization Hardware-assisted virtualization Hardware-assisted virtualization Hardware-assisted virtualization 3. Performance Feature Xen KVM&#x2F;QEMU VirtualBox VMware CPU Usage Efficient for large workloads. Efficient; depends on VirtIO drivers. More overhead due to hosted nature. Efficient, particularly with ESXi. I&#x2F;O Performance Excellent with paravirtualization. Good with VirtIO drivers. Moderate, higher latency. Excellent, especially with ESXi. GPU Passthrough Supported (used for gaming, compute). Supported (common for advanced setups). Limited (experimental). Supported (well-optimized). 4. Ease of Use Feature Xen KVM&#x2F;QEMU VirtualBox VMware Setup Complex; requires configuring dom0. Moderate; well-integrated into Linux. Very easy with GUI-based tools. Easy; designed for professionals (Workstation) or enterprises (ESXi). Management Command-line tools like xl, GUIs like XenCenter. Tools like virt-manager, virsh. Managed entirely through VirtualBox GUI. Managed via GUI (vSphere, Workstation) or CLI. Learning Curve Steeper for beginners. Moderate, Linux-savvy users benefit. Beginner-friendly. Beginner to moderate, depending on product. 5. Features Feature Xen KVM&#x2F;QEMU VirtualBox VMware Paravirtualization Supported, very efficient. Not supported (uses VirtIO instead). Not supported. Not supported. Snapshots Supported. Supported (via libvirt). Supported. Supported. Live Migration Yes, commonly used in cloud setups. Yes, widely used in OpenStack, etc. No. Yes (ESXi, Workstation Pro). Hardware Passthrough Supported. Supported. Experimental and limited. Well-supported and optimized. 6. Security Feature Xen KVM&#x2F;QEMU VirtualBox VMware Isolation Strong; guests isolated via dom0 and hypervisor. Strong; guests isolated by the Linux kernel. Decent; some shared resources increase attack surface. Strong; especially with ESXi’s isolation. Attack Surface Medium; relies on a separate control domain. Lower, depends on Linux kernel security. Higher, due to running on a host OS. Low for ESXi; higher for Workstation. 7. Cloud and Enterprise Usage Feature Xen KVM&#x2F;QEMU VirtualBox VMware Cloud Providers Used in Amazon EC2, Rackspace. Backbone of OpenStack, Google Cloud. Rarely used in cloud setups. VMware vSphere is used in enterprise clouds. Enterprise Adoption Popular in large-scale data centers. Popular for Linux-based environments. Limited to small-scale&#x2F;test setups. Leading enterprise solution with vSphere. 8. Cost Feature Xen KVM&#x2F;QEMU VirtualBox VMware Licensing Free and open source. Free and open source. Free with basic features; extension pack is proprietary. Proprietary; free (Player), paid (Workstation Pro, ESXi). TCO (Total Cost of Ownership) Low (requires expertise). Low (requires expertise). Low for small-scale personal use. High for enterprises; Workstation is affordable. When to Choose Each Use Case Best Option Enterprise-level cloud hosting Xen, VMware ESXi. Linux-centric development&#x2F;testing KVM&#x2F;QEMU. Personal use and ease of setup VirtualBox. Professional-grade virtualization VMware Workstation&#x2F;ESXi. GPU passthrough for gaming&#x2F;compute KVM&#x2F;QEMU, Xen. Summary Xen: Best for cloud and enterprise environments requiring scalability and advanced virtualization options. KVM&#x2F;QEMU: Excellent for Linux-based environments, offering strong performance, features, and tight integration with Linux tools. VirtualBox: Great for personal use, testing, and learning. Simple and user-friendly, but lacks the power of Xen or KVM. VMware: Ideal for professional environments and enterprises. ESXi is widely used in large-scale deployments, while Workstation suits smaller setups. Choose based on your specific needs for performance, features, and ease of use. If you’re experimenting on Arch Linux, KVM&#x2F;QEMU is often the best option due to its tight Linux integration and open-source nature.","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://philia897.github.io/categories/Operation-System/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"kvm","slug":"kvm","permalink":"http://philia897.github.io/tags/kvm/"},{"name":"qemu","slug":"qemu","permalink":"http://philia897.github.io/tags/qemu/"},{"name":"system","slug":"system","permalink":"http://philia897.github.io/tags/system/"}]},{"title":"Reconcile or reconciliation","slug":"Reconcile or reconciliation","date":"2024-12-02T17:53:31.000Z","updated":"2024-12-02T17:54:29.000Z","comments":true,"path":"2024/12/03/Reconcile or reconciliation/","permalink":"http://philia897.github.io/2024/12/03/Reconcile%20or%20reconciliation/","excerpt":"Learning the concept “reconcile” when designing system, application, etc.","text":"Learning the concept “reconcile” when designing system, application, etc. please explain the concept “reconcile” in the computer system design In computer system design, the term “reconcile” refers to the process of aligning or resolving discrepancies between two or more states, data sets, or system components to ensure consistency and correctness. It is commonly used in distributed systems, databases, and software development. Here’s a deeper dive into the concept: Key Aspects of Reconciliation Consistency Restoration: In distributed systems, different nodes may have conflicting or out-of-sync states due to network partitions or concurrent updates. Reconciliation ensures that all nodes eventually converge to a consistent state. Conflict Resolution: When conflicting changes occur (e.g., two users updating the same record in a database), reconciliation determines which change should prevail or merges them into a unified state. Data Validation and Synchronization: Reconciliation often involves comparing datasets or system states, identifying differences, and making updates to synchronize them. Error Correction: Reconciliation can detect and correct errors introduced during operations such as data transfer, replication, or processing. Examples of Reconciliation in System Design Distributed Databases: In systems using eventual consistency (e.g., Amazon DynamoDB, Apache Cassandra), reconciliation is used to ensure that all replicas of a piece of data converge to the same value over time. Source Control Systems: In Git, when merging branches, reconciliation resolves conflicts between changes made on different branches. Replication Systems: In data replication, reconciliation ensures that primary and secondary copies of data remain consistent, even if updates are made independently. Event Sourcing: In systems using event sourcing, reconciliation involves replaying events to reconstruct the current state and ensure it aligns with the intended state. Payment Systems: Reconciling transactions ensures that records in two systems (e.g., a bank and a merchant) agree, identifying and resolving discrepancies. Techniques for Reconciliation Conflict Detection: Comparing timestamps, checksums, or version numbers to identify differences. Conflict Resolution Policies: Last-Write-Wins (LWW): The latest update overrides previous ones. Custom Logic: Domain-specific rules to merge conflicting changes. Manual Intervention: Requiring user input for complex cases. Consensus Protocols: Algorithms like Paxos or Raft ensure agreement among distributed system nodes. Audit Logs: Keeping logs to trace changes and facilitate resolution. Reconciliation Algorithms: Techniques like Merkle trees efficiently compare large datasets. Challenges in Reconciliation Latency: Reconciling states across distributed systems can introduce delays. Complexity: Merging changes without data loss or inconsistency is non-trivial. Conflict Frequency: Frequent conflicts in high-concurrency systems require robust strategies. Resource Overhead: Reconciliation processes can consume significant computational resources. By reconciling data or state differences effectively, computer systems ensure robustness, reliability, and correctness in their operations, even in the face of inconsistencies or failures.","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"system","slug":"system","permalink":"http://philia897.github.io/tags/system/"},{"name":"git","slug":"git","permalink":"http://philia897.github.io/tags/git/"}]},{"title":"Comparison between Tauri, Electron and GTK","slug":"Comparison between Tauri, Electron and GTK","date":"2024-11-21T18:07:44.000Z","updated":"2024-11-21T18:09:07.000Z","comments":true,"path":"2024/11/22/Comparison between Tauri, Electron and GTK/","permalink":"http://philia897.github.io/2024/11/22/Comparison%20between%20Tauri,%20Electron%20and%20GTK/","excerpt":"","text":"Electron and GTK4 are indeed used for building GUI applications, but they differ significantly in terms of the technology they use, their intended use cases, and their strengths. Let’s go over each of them and then compare their use cases. ElectronWhat it is: Electron is a framework for building cross-platform desktop applications with web technologies—primarily JavaScript, HTML, and CSS. It was developed by GitHub and combines the Chromium browser (to render web content) with Node.js (for backend functionality), which allows developers to create applications that feel and behave like native apps. Key Features: Cross-platform: Works on Windows, macOS, and Linux without needing to rewrite code for each OS. Single Technology Stack: Allows web developers to use familiar tools and languages (JavaScript, HTML, CSS) to create desktop applications. Wide ecosystem: Has a huge ecosystem of libraries and modules due to its Node.js and web roots. Auto-Updating: Electron supports auto-updates, which makes it easy to push updates to users. Access to System APIs: Provides access to native features (file system, notifications, clipboard) through Node.js and Electron APIs. Popular Apps Built with Electron: Examples include Slack, Visual Studio Code, Discord, and Spotify Desktop. Advantages: Cross-Platform Support: Write once, run anywhere—ideal for applications targeting multiple operating systems. Rapid Development: Electron leverages web development, which means faster prototyping and a wider talent pool (JavaScript developers). Consistent UI: The Chromium renderer ensures that the UI looks the same across all supported platforms. Considerations: High Resource Usage: Because each Electron app runs a full instance of Chromium, Electron apps often consume more RAM and disk space than native applications. Larger Bundle Sizes: The Chromium and Node.js dependencies add to the application size. GTK4What it is: GTK4 (GIMP Toolkit version 4) is a cross-platform GUI toolkit primarily used for native applications in Linux but also supports Windows and macOS. It is written in C but has bindings for many languages (e.g., Python, Rust, and C++). GTK4 powers GNOME applications and is often chosen for creating applications that integrate well with the GNOME desktop environment. Key Features: Native Look and Feel: GTK4 applications have a native UI on Linux (particularly GNOME), and with additional effort, can integrate well with Windows and macOS. Advanced Graphics: GTK4 introduces better rendering and animation support compared to GTK3, with Vulkan and OpenGL support for hardware-accelerated graphics. Customizable Widgets: Provides a wide range of widgets with theme and style customization through CSS-like styling. Internationalization Support: GTK has built-in support for right-to-left text, accessibility, and other internationalization features. Popular Apps Built with GTK4: Examples include GNOME’s Files (file manager), Gedit (text editor), and Inkscape (vector graphics editor). Advantages: Performance: As a native toolkit, GTK4 applications are often more efficient and use fewer resources than Electron apps. Integration: GTK4 integrates seamlessly with GNOME and other Linux desktop environments and can match the native OS appearance better than Electron. Low-Level Control: Since it is closer to the OS and written in C, developers have more control over performance and system resources. Considerations: Platform-Specific: While GTK4 supports cross-platform development, the setup and code changes can be more complex on Windows and macOS. Smaller Developer Community: Compared to Electron, GTK’s developer ecosystem is smaller, which can impact support and community resources for certain languages. Not Web-Based: Developers must work with more complex languages and frameworks, which requires different skills than web development. Comparison: Electron vs. GTK4 Feature Electron GTK4 Programming Language JavaScript, HTML, CSS (or TypeScript) C, with bindings for Python, Rust, C++, etc. Cross-Platform Yes, with a single codebase Yes, but more Linux-centric, with additional work on Windows&#x2F;macOS Look and Feel Consistent across platforms (web-based UI) Native look and feel on Linux; can be adapted for Windows&#x2F;macOS Performance Higher resource consumption (Chromium overhead) More lightweight, lower resource usage Ease of Development Easier for web developers More complex, but gives fine-grained control Ecosystem Large ecosystem and community Smaller, but with deep Linux integration Use Cases Cross-platform applications (e.g., Slack, VS Code) Linux&#x2F;GNOME applications, performance-sensitive desktop apps When to Use Each Use Electron if you: Need a single codebase that runs across all platforms. Have a team skilled in web development and want to leverage JavaScript or TypeScript. Are developing an app where consistent UI across platforms is more important than native integration. Use GTK4 if you: Are targeting Linux and want native integration, especially on GNOME. Need a lightweight application with better performance. Have a use case that benefits from lower-level access and control over system resources (e.g., a performance-sensitive app). Both Electron and GTK4 are powerful for building GUIs, but they cater to different types of projects and development environments. Electron is often chosen for cross-platform compatibility and faster prototyping, while GTK4 is preferred for applications that need native integration and efficiency. What about Tauri, is it a even better choice for developing GUI application? Yes, Tauri is becoming an increasingly popular choice for developing lightweight, cross-platform desktop applications. It combines some of the best features of Electron and native development while reducing resource usage. Here’s how it compares and why it might be a good choice for certain projects. What is Tauri?Tauri is a framework for building cross-platform desktop applications using web technologies (HTML, CSS, JavaScript&#x2F;TypeScript) combined with a Rust backend. Like Electron, Tauri uses a web renderer to display the user interface. However, instead of bundling Chromium, it leverages the platform’s native webview (like WebView2 on Windows, WKWebView on macOS, and WebKitGTK on Linux). This approach allows Tauri to offer a much lighter, more resource-efficient alternative to Electron. Key Features of Tauri Lightweight and Low Resource Usage: Tauri apps have a small binary size (often under 10 MB) and consume fewer system resources since they don’t bundle a Chromium instance. Native Webview: By using the system’s native webview, Tauri achieves a more native look and feel and reduces memory and CPU usage. Rust Backend: Tauri applications use Rust to handle backend logic, which is performant and allows for safe, fast, low-level operations. Security Focus: Tauri prioritizes security with isolation between the webview and the backend, and it allows fine-grained control over the permissions an application needs. Cross-Platform: Supports Windows, macOS, and Linux, like Electron and GTK4. Advantages of Tauri Reduced Resource Usage: Since Tauri doesn’t bundle Chromium, it’s much lighter on memory and CPU. Smaller Bundle Sizes: The resulting application bundles are smaller, often under 10 MB, making them more lightweight than Electron apps. Modern Rust Backend: Rust offers better memory safety and performance than JavaScript, and Tauri makes use of Rust for backend operations. Security: Tauri’s security model isolates the frontend and backend, reducing risks associated with cross-site scripting (XSS) attacks and other vulnerabilities. Native-Like Performance: Using the native webview provides a better native experience and faster UI load times. Access to System APIs: Rust allows deeper integration with the OS compared to Electron, which relies on Node.js. Limitations of Tauri Webview Compatibility: While using the native webview reduces overhead, it also means that features and compatibility depend on the version of the webview provided by the OS, which may vary. Rust Requirement: Tauri requires Rust for backend development, which has a learning curve if you or your team are unfamiliar with it. Limited Ecosystem Compared to Electron: Electron’s larger ecosystem and community mean more plugins, integrations, and established resources for development. Comparison: Tauri vs. Electron vs. GTK4 Feature Tauri Electron GTK4 Programming Language JavaScript&#x2F;TypeScript (UI) + Rust (backend) JavaScript&#x2F;TypeScript C, with bindings for Python, Rust, C++, etc. Cross-Platform Yes, using native webview Yes, using bundled Chromium Yes, but mainly Linux-focused Binary Size Small (~10 MB) Large (~50–100 MB, depending on Chromium) Small to moderate Resource Usage Lower memory and CPU usage Higher resource consumption (due to Chromium) Lower resource usage than both Tauri and Electron Security Strong, Rust isolation and security features Moderate, sandboxed but still large attack surface Strong, relies on system-level security Ecosystem Growing, Rust libraries but smaller than Electron Large ecosystem with many plugins and tools Smaller, focused on GNOME&#x2F;Linux Use Cases Lightweight apps, cross-platform, secure apps Apps with consistent UI across all platforms Native Linux applications, performance-sensitive apps When to Use Tauri Lightweight Apps: If you need a small and efficient application without the resource overhead of Electron, Tauri is ideal. Performance-Sensitive Applications: Tauri’s reliance on Rust allows for performance and safety improvements that can be crucial for certain applications. Security-Conscious Applications: With its strong security model and the Rust backend, Tauri is an excellent choice for applications requiring higher security. Cross-Platform Support with Native Feel: Tauri leverages the native webview, providing a UI that adapts well to each OS. Conclusion: Is Tauri a Better Choice?Tauri is a compelling choice for developers who want a cross-platform desktop application with low resource usage and small bundle size. It’s particularly attractive if you’re comfortable working with Rust or have security and performance requirements that make Electron less appealing. That said, if your team has deep JavaScript experience, you need a larger ecosystem, or don’t mind the higher resource usage, Electron might still be a more convenient choice. For Linux&#x2F;GNOME-focused native applications, GTK4 remains the top option.","categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"electron","slug":"electron","permalink":"http://philia897.github.io/tags/electron/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"},{"name":"tauri","slug":"tauri","permalink":"http://philia897.github.io/tags/tauri/"}]},{"title":"Use MITMProxy for capturing Android traffics and analyze","slug":"Use MITMProxy for capturing Android traffics and a","date":"2024-11-16T08:05:41.000Z","updated":"2024-11-16T08:12:40.000Z","comments":true,"path":"2024/11/16/Use MITMProxy for capturing Android traffics and a/","permalink":"http://philia897.github.io/2024/11/16/Use%20MITMProxy%20for%20capturing%20Android%20traffics%20and%20a/","excerpt":"Capture and analyze https traffics using MITMProxy. It relies on man-in-the-middle and registering certificate to the device.","text":"Capture and analyze https traffics using MITMProxy. It relies on man-in-the-middle and registering certificate to the device. MITMProxy uses port 8080 for proxy. Before using it, remember to allow port 8080 in ufw sudo ufw allow 8080. And delete the rule after using sudo ufw delete allow 8080 You:I want to sniff and analyze the traffics of my android applications, for example, I developed one app and I want to track its https communication, how? ChatGPT:To sniff and analyze HTTPS traffic from an Android app, you typically have to bypass the app’s SSL&#x2F;TLS encryption. Here’s a step-by-step guide to do this safely on your local network for testing and development: 1. Use a Proxy Tool Install a proxy tool like Charles Proxy, Fiddler, or Burp Suite on your computer. These tools intercept HTTP and HTTPS traffic and allow you to inspect the payload. 2. Configure Your Android Device to Use the Proxy Connect your Android device and computer to the same Wi-Fi network. Go to Settings &gt; Wi-Fi on your Android device and long-press the connected network to modify it. Under Advanced settings, set up a manual proxy with the IP address of your computer (running the proxy tool) and the port specified in the tool (usually 8888 or similar). 3. Install the Proxy’s SSL Certificate on Your Android Device For HTTPS decryption, you’ll need to install the proxy tool’s SSL certificate on your Android device. Most proxy tools have an option to download the certificate as a .pem or .cer file. Transfer this file to your Android device, go to Settings &gt; Security &gt; Install from storage (or a similar option), and install the certificate as a trusted CA. 4. Set Up the App to Allow HTTPS Inspection Some apps have network security configurations that prevent proxy interception. To bypass this: If you have control over the app (like a self-developed app), add a debug-only network_security_config.xml file in the app’s res/xml directory. This file should disable certificate pinning:123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;network-security-config&gt; &lt;domain-config cleartextTrafficPermitted=&quot;true&quot;&gt; &lt;domain includeSubdomains=&quot;true&quot;&gt;example.com&lt;/domain&gt; &lt;/domain-config&gt;&lt;/network-security-config&gt; Ensure this configuration is only in your debug build and not in production! 5. Capture and Analyze the Traffic With everything set up, open the proxy tool and start capturing traffic. Run your app on the Android device. You should see HTTPS traffic being intercepted in the proxy tool, and you can inspect requests and responses. Important Notes Avoid intercepting HTTPS for production apps or any third-party apps without permission. Be cautious with sensitive data and only use this setup for apps you control or have permission to test. Android 7.0 (Nougat) and later requires you to explicitly trust user-installed CAs in each app’s network security configuration. You:Is there any opensource solution? Or is fiddler secure? ChatGPT:Yes, there are open-source tools for sniffing and analyzing HTTPS traffic from Android applications, and these can be secure if used correctly. Here are some open-source options and security considerations for Fiddler. Open-Source Solutions MITMProxy MITMProxy is a powerful, open-source, interactive HTTPS proxy designed specifically for testing and debugging network traffic. Features: It provides a console interface and web interface (mitmweb) to inspect traffic. It supports HTTPS decryption, filtering, scripting, and replaying traffic. Setup: Like other proxy tools, it requires you to set up your Android device to use MITMProxy as the network proxy and install its certificate on the Android device. Customizable: It offers Python scripting support, which makes it flexible for custom test cases. Wireshark Wireshark is an open-source network protocol analyzer that captures network traffic across interfaces and decodes protocols. Limitations: Wireshark doesn’t have native HTTPS decryption like MITMProxy or Fiddler. You’ll need to manually export the SSL session keys from your Android device, which is feasible only if the app supports SSLKEYLOGFILE. This process can be cumbersome, so it’s best suited for network traffic other than HTTPS unless SSL keys are easily exportable. Alternative Use: You can use Wireshark in combination with MITMProxy by running Wireshark on the decrypted traffic generated by MITMProxy. Frida with Objection Frida is an open-source, dynamic instrumentation toolkit that, with the help of Objection, can bypass SSL pinning in Android apps. Application: Frida can intercept traffic, bypass SSL pinning, and even inspect encrypted network calls within apps. Objection offers pre-made scripts for SSL pinning bypass. Requirements: Rooted Android devices may be required for full functionality, though newer Frida versions allow SSL bypass on non-rooted devices in many cases. Is Fiddler Secure?Fiddler, by Telerik (now part of Progress Software), is widely regarded as a secure and reliable proxy tool when obtained directly from its official source and used in a controlled environment. Here are some key security points: Closed Source: Fiddler itself is not open-source, which can be a concern if you require full transparency. Security Risks: Fiddler installs a root certificate authority (CA) certificate to intercept HTTPS traffic, so it’s crucial to only use it on trusted devices and networks. Official Source: Download Fiddler only from its official site (Fiddler) to avoid malicious or altered versions. If open-source transparency is a priority or you need flexible scripting, MITMProxy is a top choice for Android traffic analysis. However, if you are comfortable with Fiddler’s security and features, it remains a solid choice for HTTPS debugging. You:How to setup mitmproxy? ChatGPT:Setting up MITMProxy to intercept and analyze Android app traffic is straightforward. Below are the steps to get it running on your computer and intercept HTTPS traffic from an Android device. Step 1: Install MITMProxy On Linux&#x2F;macOS:1pip install mitmproxy On Windows: Download the installer from the MITMProxy website. Run the installer and add it to your PATH. Step 2: Start MITMProxy Launch MITMProxy in transparent or intercept mode, depending on your needs. The most common mode is mitmweb (web interface) or mitmproxy (console interface):1mitmweb or1mitmproxy By default, MITMProxy listens on port 8080. You’ll use this as the proxy port on your Android device. Step 3: Configure the Proxy on Your Android Device Connect your Android device and computer to the same Wi-Fi network. Go to Settings &gt; Wi-Fi on your Android device. Long-press your connected Wi-Fi network and choose Modify network. Set up a manual proxy: Proxy hostname: Enter your computer’s IP address. Proxy port: Enter 8080 (or the port MITMProxy is running on). Save these settings. Your Android device will now route its traffic through MITMProxy. Step 4: Install the MITMProxy Certificate on Android Open a browser on your Android device and go to http://mitm.it. Download the Android certificate from this page. Install the certificate: Open Settings &gt; Security &gt; Install from storage (or similar, depending on your Android version). Select the downloaded certificate and install it as a trusted CA. Note: On Android 7.0+ (Nougat) and later, apps may not trust user-installed certificates by default. For apps under your control, you’ll need to update the network_security_config.xml to allow this. Step 5: Bypass SSL Pinning (if Needed)For apps that use SSL pinning (an additional layer of security), MITMProxy might not be able to intercept HTTPS traffic. If this is an app you control: Add a network_security_config.xml file to allow user-installed certificates:123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;network-security-config&gt; &lt;domain-config cleartextTrafficPermitted=&quot;true&quot;&gt; &lt;domain includeSubdomains=&quot;true&quot;&gt;yourdomain.com&lt;/domain&gt; &lt;/domain-config&gt;&lt;/network-security-config&gt; Make sure this configuration is included in your debug build, not in production. For third-party apps, you can use Frida and Objection (as mentioned) to bypass SSL pinning. Step 6: Capture and Analyze Traffic With everything set up, start your Android app. View HTTP&#x2F;HTTPS requests and responses in the MITMProxy web interface (if using mitmweb) or console (if using mitmproxy). You can inspect headers, payloads, and other details of each request and response. Step 7: Stop MITMProxy and Remove the CertificateAfter testing, you can: Stop MITMProxy by closing the terminal. Remove the proxy settings from your Android device’s Wi-Fi settings. Delete the MITMProxy certificate from your Android device under Settings &gt; Security &gt; Trusted credentials. This setup should allow you to intercept and analyze the network traffic of your Android applications effectively. Remember to use this only for apps you control or have permission to analyze. You:Then I think I need to firstly allow traffics to bypass my firewall, how to add 8080 to ufw ChatGPT:To allow traffic on port 8080 through your firewall with UFW (Uncomplicated Firewall), you can follow these steps: Step 1: Allow Port 8080 in UFW Open a terminal. Run the following command to allow traffic on port 8080 for both TCP and UDP protocols: 1sudo ufw allow 8080 If you only want to allow TCP traffic, use: 1sudo ufw allow 8080/tcp Or for UDP traffic only: 1sudo ufw allow 8080/udp To confirm that the rule has been added, you can check the status of UFW: 1sudo ufw status This should list 8080 as allowed. Step 2: Reload UFW (if necessary)If UFW was already active, it should apply changes immediately. But if you want to make sure: 1sudo ufw reload Additional Tips If you need to restrict access to port 8080 to only certain IPs, you can specify the IP address: 1sudo ufw allow from &lt;IP_ADDRESS&gt; to any port 8080 Your firewall should now allow traffic through port 8080! You:what about remove this rule after I used? ChatGPT:To remove the rule for port 8080 in UFW after you’re done with it, follow these steps: Step 1: Delete the UFW Rule for Port 8080 Open a terminal. Use the following command to delete the rule that allows traffic on port 8080: 1sudo ufw delete allow 8080 If you specifically allowed TCP or UDP traffic only, you should specify the protocol as well: For TCP only:1sudo ufw delete allow 8080/tcp For UDP only:1sudo ufw delete allow 8080/udp Step 2: Verify the Rule RemovalYou can check the current UFW rules to confirm the removal by running: 1sudo ufw status This will show you the current list of allowed and denied ports. Port 8080 should no longer be listed.","categories":[{"name":"hacking","slug":"hacking","permalink":"http://philia897.github.io/categories/hacking/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"debugging","slug":"debugging","permalink":"http://philia897.github.io/tags/debugging/"}]},{"title":"Zero-Knowledge Proof and Blockchain","slug":"Zero-Knowledge Proof and Blockchain","date":"2024-11-15T00:52:56.000Z","updated":"2024-11-15T00:55:07.000Z","comments":true,"path":"2024/11/15/Zero-Knowledge Proof and Blockchain/","permalink":"http://philia897.github.io/2024/11/15/Zero-Knowledge%20Proof%20and%20Blockchain/","excerpt":"Record the learning history with chatgpt regarding ZKP and Blockchain.","text":"Record the learning history with chatgpt regarding ZKP and Blockchain. You:Please explain what is zero-knowledge proof and give one example ChatGPT:A zero-knowledge proof (ZKP) is a cryptographic method that allows one party (the prover) to prove to another party (the verifier) that a statement is true, without revealing any information beyond the truth of the statement itself. In other words, the prover convinces the verifier that they know something (a piece of information or a solution) without sharing what that information is. Key Properties of Zero-Knowledge Proofs Completeness: If the statement is true, an honest prover can convince an honest verifier of this fact. Soundness: If the statement is false, no dishonest prover can convince the verifier that it’s true, except with some tiny probability. Zero-Knowledge: If the statement is true, the verifier learns nothing about the actual information beyond the fact that it’s true. Example: The Ali Baba Cave (Conceptual Example)Imagine a circular cave with two paths leading to the entrance, path A and path B. Peggy (the prover) wants to prove to Victor (the verifier) that she knows the secret password to open a hidden door connecting the two paths, but she doesn’t want to reveal the password itself. The process works like this: Victor waits outside and can’t see which path Peggy takes, either A or B. Peggy enters the cave and randomly chooses path A or B. Victor then enters and randomly calls for Peggy to exit through path A or B. If Peggy knows the password, she can always open the door and exit through whichever path Victor specifies. If she doesn’t know the password, she will only be able to satisfy Victor’s request half the time by luck. By repeating this multiple times, Victor can become increasingly convinced that Peggy knows the password without ever learning what the password actually is. In real-world cryptography, zero-knowledge proofs are used in various privacy-sensitive applications, such as secure identity verification, blockchain transactions, and authentication systems. You:Now give a real-world usage example of ZKP ChatGPT:A real-world example of zero-knowledge proofs is their use in cryptocurrencies, specifically in privacy-focused blockchains like Zcash. In typical cryptocurrencies like Bitcoin, transactions are visible on a public ledger, including details like the sender’s and receiver’s addresses and the amount of money transferred. This transparency can compromise user privacy, as anyone can see transaction details. Zcash, however, uses a form of ZKP called zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Argument of Knowledge) to enable completely private transactions. How ZKP Works in ZcashWhen someone makes a transaction in Zcash using zk-SNARKs: Confidential Proof: The sender generates a zero-knowledge proof that they have the right to spend the specified amount without revealing the exact details (amount, sender, and receiver) of the transaction. Verification: This zero-knowledge proof is then sent to the network, where miners (validators) can verify the proof’s validity without needing any further information. Public Ledger, Private Transaction: The transaction is added to the blockchain, and the network confirms that the transaction is legitimate, but no private information (amount, sender, receiver) is exposed on the public ledger. BenefitsThis application of ZKPs enables Zcash users to make fully private transactions, as the only thing revealed to the network is the fact that a valid transaction has taken place. This ensures both security and privacy in a way that is not possible with traditional digital transactions. You:But it sounds very difficult and costly right? How it works? ChatGPT:You’re absolutely right! Implementing zero-knowledge proofs, especially in the form of zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge), is technically challenging and can be computationally expensive. However, advances in cryptographic research have made zk-SNARKs more practical, even though they remain more resource-intensive than simpler cryptographic methods. Here’s an overview of how zk-SNARKs work and how Zcash manages these challenges: How zk-SNARKs Work Preprocessing and Setup (Trusted Setup): zk-SNARKs require an initial “trusted setup” phase, which involves creating cryptographic parameters that will be used to generate proofs and verify them. This step must be done carefully because, if compromised, it could allow for false proofs. Prover Constructs the Proof: The prover (person initiating the transaction) constructs a zk-SNARK proof that they have the knowledge to make a valid transaction (like having sufficient balance and the right to transfer funds). This involves complex math and cryptographic operations on the transaction data without revealing the data itself. Verifier Checks the Proof: The verifier (such as a miner in the blockchain) uses the zk-SNARK verification algorithm to quickly check the proof’s validity without knowing the transaction details. This verification step is fast, even though proof generation is computationally heavy. Overcoming the Costs and ChallengesThe costs primarily stem from the following: Computation: Generating zk-SNARK proofs is computationally intensive, requiring complex math operations. Improvements in cryptographic libraries and dedicated hardware help make this faster. Trusted Setup: The setup phase is critical and requires trusted participants; Zcash periodically performs public ceremonies with multiple independent participants to ensure this setup’s integrity. On-Chain Efficiency: zk-SNARK proofs are small in size compared to the actual data they represent, which is beneficial for blockchain storage and verification. Miners can verify proofs without needing to process or store the details, which reduces blockchain bloat. Why zk-SNARKs are Worth ItDespite the costs, zk-SNARKs offer benefits that are crucial for privacy-focused applications: Enhanced Privacy: Users can make transactions without revealing sensitive information, addressing privacy concerns. Security: zk-SNARKs are cryptographically secure, meaning they provide strong guarantees about transaction validity while preserving privacy. Efficiency for Verifiers: Verification of zk-SNARK proofs is efficient (typically in milliseconds), allowing miners to handle these proofs without significant delays. Recent Advances and AlternativesResearch in cryptography has led to the development of alternative zero-knowledge proof systems, like zk-STARKs (Scalable Transparent Arguments of Knowledge) and Bulletproofs, which aim to address some limitations of zk-SNARKs, such as the need for trusted setups. While still computationally complex, these alternatives offer paths to reducing the overall cost and improving scalability. Overall, while zero-knowledge proofs like zk-SNARKs are complex and costly, they provide unparalleled privacy features that make them valuable in applications like Zcash and other privacy-centric systems. As cryptographic techniques evolve, these proofs are expected to become even more efficient and accessible for broader use cases. You:OK, seems I need some basic knowledge of blockchain as well, please explain how blockchain works and its components ChatGPT:Certainly! Blockchain is a decentralized, distributed digital ledger that securely records transactions across multiple computers. The key idea is that it enables trustless, tamper-resistant record-keeping without a central authority. Let’s break down how it works and look at its main components. How Blockchain Works Transaction Creation: A blockchain starts when a participant initiates a transaction. For example, in a cryptocurrency network, this could be a transfer of funds from one user to another. Transaction Validation: Once created, the transaction needs to be validated by the network. This is often done by a process called consensus (more on this below), where multiple participants (often called “nodes”) agree on the validity of the transaction. Adding to a Block: Validated transactions are grouped into a “block.” A block is a collection of data (mainly transactions) with a unique identifier, or “hash,” which makes it distinct from other blocks. Linking Blocks Together: Each block includes a reference to the previous block’s hash, linking them together chronologically to form a chain of blocks—hence, the term “blockchain.” This linking ensures that any attempt to modify a block would also require changing all subsequent blocks, which is extremely difficult to achieve. Consensus Mechanism: New blocks are added to the blockchain only after reaching consensus, which is a protocol that nodes follow to agree on the block’s validity. Common consensus mechanisms include: Proof of Work (PoW): Used in Bitcoin, PoW requires nodes (miners) to solve complex math problems to validate transactions, which uses significant computational power. Proof of Stake (PoS): Used in networks like Ethereum 2.0, PoS selects validators based on the amount of cryptocurrency they hold, which is more energy-efficient than PoW. Distributed Ledger: Once a block is added, it is shared across the entire network, meaning all participants hold a copy of the updated ledger. Any tampering would be noticed because it would result in a mismatch across copies. Key Components of Blockchain Blocks: Each block contains data, a timestamp, a nonce (used in PoW), a hash, and the hash of the previous block. Data: This usually contains the list of transactions. Hash: A cryptographic fingerprint of the block’s data. Even a small change in data drastically changes the hash. Previous Block Hash: This creates the “chain” by linking each block to the one before it. Nodes: These are the computers in the network that store and maintain the blockchain. Nodes can be full nodes, which store a complete copy of the blockchain, or light nodes, which store only partial data. Consensus Mechanism: This is the method by which the network agrees on the state of the blockchain. Consensus mechanisms prevent double-spending, resolve conflicts, and ensure that only valid transactions are recorded. Cryptographic Hash Functions: These are mathematical algorithms that turn data into a fixed-size hash. Hashes are unique to their input data and make it impossible to retrieve the original data from the hash, ensuring security and data integrity. Digital Signatures: Transactions are digitally signed with private keys, ensuring that only the rightful owner can authorize a transaction. This allows participants to verify that a transaction truly came from the account holder. Smart Contracts (optional): Some blockchains, like Ethereum, support smart contracts. These are self-executing contracts with the terms directly written into code. They allow for automated and trustless transactions, expanding the utility of blockchain beyond simple financial transactions. Advantages of Blockchain Decentralization: There’s no central authority; data is maintained by a network of nodes, making it resistant to censorship and fraud. Transparency: Since all nodes have access to the same data, transactions are transparent and verifiable. Security: Cryptographic hashing, digital signatures, and consensus mechanisms make it very difficult to alter data or add fraudulent transactions. Immutability: Once data is recorded, it’s extremely difficult to change, providing a reliable history. Example: Bitcoin BlockchainIn the Bitcoin blockchain: When Alice wants to send 1 Bitcoin to Bob, her transaction is broadcast to the network. Miners compete in a PoW process to validate the transaction by solving a mathematical puzzle. Once a miner solves the puzzle, Alice’s transaction is bundled with others into a new block. The block is added to the blockchain, and Alice’s 1 Bitcoin is transferred to Bob. SummaryIn essence, a blockchain is a secure, decentralized system for recording data, especially useful for transactions where trust is limited. Each component—blocks, nodes, consensus mechanisms, and cryptographic tools—works together to create a tamper-resistant and transparent ledger, enabling trustless transactions across various industries, from finance to supply chain to identity management. You:OK, but how the wallet or account is initialized? ChatGPT:To interact with a blockchain network, a user first needs to create a wallet or account. Wallets store the information needed to send and receive cryptocurrencies or interact with blockchain applications. Here’s how the initialization process works, along with the key steps and components involved. 1. Generating a Private KeyThe process begins by generating a private key—a long, randomly generated string of numbers and letters. This private key is crucial because: It gives the owner full control over their blockchain account or wallet. It should be kept secret; anyone with access to it can access the funds or assets associated with the wallet. In most blockchain systems, a private key is generated using cryptographic algorithms, such as Elliptic Curve Digital Signature Algorithm (ECDSA). This algorithm is widely used because it generates secure and relatively compact keys. 2. Deriving a Public KeyOnce the private key is generated, a public key is derived from it using a one-way cryptographic function. The public key serves as an identifier, which others can see and use to verify the authenticity of the user’s transactions. The relationship between the private and public keys is asymmetric; while the public key can be derived from the private key, it’s nearly impossible to reverse the process and derive the private key from the public key. The private key is used to sign transactions, while the public key verifies the signature, ensuring that the transaction was indeed authorized by the wallet owner. 3. Generating the Wallet AddressA wallet address is a shortened version of the public key, often with additional processing to improve security and readability. The exact steps for creating a wallet address vary depending on the blockchain, but a common process involves: Hashing the Public Key: The public key is usually hashed using algorithms like SHA-256 and RIPEMD-160. This produces a fixed-length output that serves as the basis for the wallet address. Adding Checksums: Checksums are added to detect errors and prevent address typos from leading to irreversible transactions. Encoding in a Readable Format: The address is often encoded in a format like Base58 (Bitcoin) or hexadecimal, making it shorter and easier to read and share. This wallet address is what users share to receive funds or interact with smart contracts. 4. Wallet Initialization in PracticeWhen a user creates a new wallet, they typically do so through a wallet provider—software or hardware that generates and manages private keys and other wallet components. There are different types of wallets: Software Wallets: These are applications (like MetaMask, Trust Wallet, or the Bitcoin Core client) that generate and store private keys locally on a device. They are convenient for frequent transactions. Hardware Wallets: Physical devices like Ledger and Trezor, which store private keys offline, providing enhanced security by keeping keys out of reach from online threats. Paper Wallets: These are physical copies of the private and public keys, usually printed on paper. They’re secure from online attacks but can be lost or damaged. When a user initializes a wallet through one of these methods, the private key, public key, and wallet address are generated, and the user gains full control of their account on the blockchain. 5. Seed Phrases and RecoveryFor security and backup, most wallets also generate a seed phrase (or recovery phrase), typically a set of 12 to 24 human-readable words. This phrase is another representation of the private key, and it allows the user to recover their wallet if they lose access to the device or application storing it. Example: Bitcoin Wallet Initialization The Bitcoin wallet software generates a private key using a secure random number generator. A public key is derived from this private key using elliptic curve cryptography. The public key is hashed and encoded to form the Bitcoin address. The user can now receive Bitcoin at their address and sign transactions with their private key to spend it. SummaryTo initialize a wallet or account: Private Key: Generated first and kept secret. Public Key: Derived from the private key and used to verify ownership. Wallet Address: A shorter, shareable version derived from the public key. This setup allows the user to manage assets on the blockchain securely, as only the private key holder can authorize transactions, while the public key and address serve as identifiers for interactions. You:OK, then how a transaction is created and the network verify one transaction? ChatGPT:Creating and verifying a blockchain transaction involves several key steps, all designed to ensure that the transaction is authentic, authorized, and valid according to network rules. Here’s how it works, broken down into the main parts: 1. Transaction CreationWhen a user (let’s call them Alice) wants to send funds or interact with a blockchain network (such as transferring cryptocurrency to Bob): Transaction Details: Alice’s wallet software constructs a transaction that includes: The sender’s address (Alice’s wallet address). The receiver’s address (Bob’s wallet address). The amount to be sent. Other data if needed, such as gas fees on Ethereum or smart contract data. Signing the Transaction: The wallet software uses Alice’s private key to generate a digital signature for the transaction. This digital signature proves that Alice (the rightful owner of the wallet) authorized the transaction without revealing her private key. The transaction, along with the signature, is then broadcast to the blockchain network. 2. Broadcasting the TransactionOnce signed, the transaction is broadcast to the blockchain network’s nodes, which are other computers or entities maintaining the network. These nodes receive and share the transaction with each other to ensure it reaches all parts of the network. 3. Validation by Nodes (Transaction Verification)Each node performs several checks to verify the transaction: Digital Signature Verification: The nodes use Alice’s public key to verify the digital signature. This step confirms that the transaction was signed by someone with the corresponding private key (i.e., Alice) and was not tampered with. Sufficient Balance Check: Nodes check Alice’s wallet balance to ensure she has enough funds to complete the transaction. If Alice tries to send more funds than she owns, the transaction is considered invalid and discarded. Double-Spending Prevention: The nodes check to make sure that Alice isn’t trying to spend the same funds twice. Since the blockchain is a distributed ledger, previous transactions are already recorded on the chain, and nodes can verify that Alice hasn’t spent the same funds in a prior transaction. 4. Adding the Transaction to a BlockOnce validated, the transaction goes into a pool of pending transactions (often called the mempool). From here, a miner or validator will select transactions to include in a new block. Transaction Selection: In blockchains like Bitcoin, miners prioritize transactions with higher fees, as they earn these fees as rewards for their work. Proof of Work (PoW) or Proof of Stake (PoS): In PoW systems (like Bitcoin), miners compete to solve a complex math problem, which requires substantial computational resources. In PoS systems (like Ethereum 2.0), validators are selected based on the amount of cryptocurrency they hold and are willing to “stake” as collateral. Block Creation: Once the miner or validator successfully validates and adds the transaction to a block, they broadcast this block to the network. 5. Consensus and Block FinalizationFor the new block (and the transactions within it) to be accepted by the blockchain, it must gain consensus among nodes in the network. Consensus Mechanism: Depending on the blockchain, nodes reach consensus using protocols like PoW, PoS, or others. Consensus ensures that all nodes agree on the state of the blockchain, preventing fraudulent or conflicting information. Appending the Block: Once consensus is reached, the new block is added to the blockchain, making the transactions within it immutable. 6. Confirming the TransactionAfter the block containing Alice’s transaction is added to the blockchain, her transaction is considered complete. However, it usually needs a few confirmations (additional blocks added on top of it) to be fully accepted by the network, adding extra security and making it much harder for any attacker to alter the transaction. Summary of Transaction Verification Steps: Transaction creation: Alice’s wallet creates and signs the transaction with her private key. Broadcast: The transaction is broadcast to network nodes. Verification by nodes: Digital signature verification. Sufficient balance check. Double-spending prevention. Block inclusion: Miners or validators add the transaction to a new block. Consensus: The block is added to the blockchain after reaching consensus. Confirmation: The transaction is confirmed and considered irreversible. Example: Bitcoin Transaction Alice wants to send 0.1 BTC to Bob. Alice’s wallet constructs the transaction, signs it with her private key, and broadcasts it to the Bitcoin network. Nodes verify the signature, check her balance, and prevent double-spending. Miners include the transaction in a new block after solving a PoW puzzle. Other nodes verify the new block, and it’s added to the blockchain once consensus is reached. The transaction is confirmed as additional blocks are added, securing its place in the chain. This process allows decentralized networks like Bitcoin and Ethereum to maintain a secure, trustworthy ledger without relying on a central authority, ensuring that transactions are valid, authorized, and resistant to tampering.","categories":[{"name":"Crhyptography","slug":"Crhyptography","permalink":"http://philia897.github.io/categories/Crhyptography/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"cryptography","slug":"cryptography","permalink":"http://philia897.github.io/tags/cryptography/"},{"name":"blockchain","slug":"blockchain","permalink":"http://philia897.github.io/tags/blockchain/"}]},{"title":"Write an application to record system resource usage statistics","slug":"Write an application to record system resource usa","date":"2024-10-30T23:51:21.000Z","updated":"2024-12-09T01:08:03.000Z","comments":true,"path":"2024/10/31/Write an application to record system resource usa/","permalink":"http://philia897.github.io/2024/10/31/Write%20an%20application%20to%20record%20system%20resource%20usa/","excerpt":"To record the system resource usage every 30 min for example","text":"To record the system resource usage every 30 min for example It should includes one data collector, one sqlite database, one GUI for visualization and configuration. Data Collector (Rust)One service running in the background, collecting system resources usage periodically, and save the records into the database. Database (SQLite)The database used to store the data collector status, configuration, and all data records in different tables GUI (Tauri)For data visualization and configuring, directly communicating with the database. Done, using Rust to write a service to record every 1h, SQLite for storage, write a simple Python Plotly frontend. See: Luce-python","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"system monitor","slug":"system-monitor","permalink":"http://philia897.github.io/tags/system-monitor/"},{"name":"opensource project","slug":"opensource-project","permalink":"http://philia897.github.io/tags/opensource-project/"}]},{"title":"Rclone encrypt","slug":"Rclone encrypt","date":"2024-10-12T08:13:05.000Z","updated":"2024-11-20T22:55:15.000Z","comments":true,"path":"2024/10/12/Rclone encrypt/","permalink":"http://philia897.github.io/2024/10/12/Rclone%20encrypt/","excerpt":"Use rclone with its local encryption feature enabled. It is a good solution for data backup. Generated by GPT and proved to be true:","text":"Use rclone with its local encryption feature enabled. It is a good solution for data backup. Generated by GPT and proved to be true: To use rclone crypt, you need to configure rclone to encrypt your files locally before uploading them to a cloud storage provider (such as Amazon S3, Google Drive, Dropbox, etc.). Here’s a step-by-step guide for setting up rclone crypt to encrypt and decrypt your data. Steps to Use rclone crypt1. Install rcloneIf you haven’t installed rclone yet, do so: On Arch Linux, you can install it via pacman: 1sudo pacman -S rclone For other platforms, follow the official installation guide. 2. Configure rclone for Your Cloud Provider (S3 Example)First, you need to configure rclone for your cloud storage service (e.g., Amazon S3). This process creates a remote in rclone that connects to your cloud storage. Run the following command: 1rclone config Choose n for a new remote. Give it a name (e.g., myS3). Select your cloud provider (e.g., for S3, choose 4 for Amazon S3). Enter the necessary configuration details (Access Key, Secret Key, region, etc.). Complete the setup. This will create a remote called myS3 that connects to your S3 bucket. 3. Set Up rclone CryptNow you’ll configure rclone crypt, which will encrypt the files before uploading them to the cloud. This encrypted remote will point to the S3 bucket you just set up. Run: 1rclone config Choose n for a new remote. Give it a name (e.g., myS3-encrypted). Select 8 for the crypt option (the encryption type). When asked for the remote to encrypt, enter the path to your S3 bucket (e.g., myS3:bucketname). rclone will ask how you want to encrypt filenames. You can choose: Standard (recommended): encrypts both file content and names. Obfuscated: only obfuscates filenames, keeping them human-readable. No encryption: only the file content is encrypted. rclone will then ask you to create an encryption password. Make sure to use a strong password. Optionally, you can set a password for directory name encryption. Complete the setup. You now have an encrypted remote called myS3-encrypted. Any files uploaded to this remote will be encrypted locally before uploading to S3. 4. Upload Files to the Encrypted RemoteOnce the encrypted remote is set up, you can use rclone commands to upload files to the encrypted S3 bucket. To upload files to the encrypted remote: 1rclone copy /path/to/local/files myS3-encrypted:/ Replace /path/to/local/files with the path to your local files. myS3-encrypted:/ is the root of your encrypted remote in the S3 bucket. 5. Access and Decrypt FilesTo download and decrypt the files from your encrypted remote: 1rclone copy myS3-encrypted:/ /path/to/local/directory This command will download and decrypt the files, storing them in /path/to/local/directory. 6. Automating SyncsYou can use rclone sync to automate file synchronization between your local machine and the encrypted remote: 1rclone sync /path/to/local/files myS3-encrypted:/ This will sync changes from your local directory to the encrypted S3 remote. Android SupportTo access the encrypted files on Android, you can: Use rclone in a terminal (via Termux). Install FolderSync or other apps supporting rclone, although this will not provide automatic decryption (you’ll need to manually decrypt using the same rclone setup). Example Commands Summary rclone config: Set up the remote and crypt remote. rclone copy &#x2F;path&#x2F;to&#x2F;local&#x2F;files myS3-encrypted:&#x2F;: Upload encrypted files. rclone copy myS3-encrypted:&#x2F; &#x2F;path&#x2F;to&#x2F;local&#x2F;directory: Download and decrypt files. rclone sync &#x2F;path&#x2F;to&#x2F;local&#x2F;files myS3-encrypted:&#x2F;: Sync encrypted files with your cloud storage. Would you like help with a specific provider or in setting this up for Android?","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"rclone","slug":"rclone","permalink":"http://philia897.github.io/tags/rclone/"},{"name":"encryption","slug":"encryption","permalink":"http://philia897.github.io/tags/encryption/"},{"name":"backup","slug":"backup","permalink":"http://philia897.github.io/tags/backup/"}]},{"title":"Use Paru for AUR packages","slug":"Use Paru for AUR packages","date":"2024-10-09T06:16:52.000Z","updated":"2024-12-29T20:37:24.000Z","comments":true,"path":"2024/10/09/Use Paru for AUR packages/","permalink":"http://philia897.github.io/2024/10/09/Use%20Paru%20for%20AUR%20packages/","excerpt":"Great project to automate the process of installing packages not only using pacman, but also AUR.","text":"Great project to automate the process of installing packages not only using pacman, but also AUR. Data stored in ~/.cache/paru/ Example commands: paru -s &lt;package-name&gt;: Search for a package paru &lt;target&gt; – Interactively search and install &lt;target&gt;. This one can be frequently use! Recommand! paru – Alias for paru -Syu. paru -S &lt;target&gt; – Install a specific package. paru -Rns &lt;package-name&gt; – Remove a package and its dependencies paru -Sua – Upgrade AUR packages. paru -Qu – Check for updates without installing paru -Qua – Print available AUR updates. (-a means AUR) paru -Sc – Clean cached package files paru -Qi &lt;package-name&gt; – Show detailed information about a package paru -Ql &lt;package-name&gt; – List files installed by a package paru -Qm – Lists manually installed AUR packages. paru -Syu --dry-run –Perform a dry run (show what would happen without making changes): paru --ignore &lt;package-name&gt;–Ignore a package during an update: Add the package to the ignore list paru -G &lt;target&gt; – Download the PKGBUILD and related files of &lt;target&gt;. paru -Gp &lt;target&gt; – Print the PKGBUILD of &lt;target&gt;. paru -Gc &lt;target&gt; – Print the AUR comments of &lt;target&gt;. paru --gendb – Generate the devel database for tracking *-git packages. This is only needed when you initially start using paru. paru -Bi . – Build and install a PKGBUILD in the current directory. Some methods to clear cached data of Paru, from ChatGPT: To clear old data for a specific package using paru (an AUR helper for Arch Linux), you can follow these steps: 1. Clear Cached Versions of a Specific Package Paru stores cached package files in /var/cache/pacman/pkg/. To remove old versions of a specific package, use: 1paru -Sc This will prompt you to remove all unused cached packages. If you want to specifically remove a single package’s cached versions, use: 1sudo rm -rf /var/cache/pacman/pkg/&lt;package-name&gt;-* 2. Remove Unused Dependencies If you want to clean up orphaned dependencies (packages that are no longer required), run: 1paru -Rns $(pacman -Qdtq) This will remove unneeded dependencies along with their configuration files. 3. Remove AUR Build Cache for a Specific Package Paru stores build files in ~/.cache/paru/clone/. To remove old build files for a single package: 1rm -rf ~/.cache/paru/clone/&lt;package-name&gt; 4. Clear Paru’s Downloaded Database Files To clean up Paru’s database cache: 1paru --gendb Would you like to automate the cleanup process with a script?","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"aur","slug":"aur","permalink":"http://philia897.github.io/tags/aur/"},{"name":"paru","slug":"paru","permalink":"http://philia897.github.io/tags/paru/"}]},{"title":"use crontab to create scheduled task","slug":"use crontab to create scheduled task","date":"2024-10-03T08:55:02.000Z","updated":"2024-10-03T08:55:27.000Z","comments":true,"path":"2024/10/03/use crontab to create scheduled task/","permalink":"http://philia897.github.io/2024/10/03/use%20crontab%20to%20create%20scheduled%20task/","excerpt":"By ChatGPT To configure a cron job, you’ll use the crontab command, which allows you to schedule tasks to run at specific intervals (e.g., every day, hour, or minute). Here’s how you can do it:","text":"By ChatGPT To configure a cron job, you’ll use the crontab command, which allows you to schedule tasks to run at specific intervals (e.g., every day, hour, or minute). Here’s how you can do it: Step 1: Open the Crontab FileTo create or edit cron jobs, run the following command: 1crontab -e This opens the crontab file for the current user in the default text editor (usually nano or vim). Step 2: Understanding the Crontab FormatEach line in the crontab file represents a scheduled job and follows this format: 1* * * * * /path/to/script.sh The five asterisks represent time fields (minute, hour, day of month, month, day of week), followed by the command or script to run. Here’s what each field represents: 1234567* * * * * command| | | | || | | | └── Day of the week (0 - 7) (Sunday=0 or 7)| | | └──── Month (1 - 12)| | └────── Day of the month (1 - 31)| └──────── Hour (0 - 23)└────────── Minute (0 - 59) Step 3: Adding a Cron JobLet’s say you want to run a backup script at 2 AM every day: 10 2 * * * /home/user/backup.sh This cron job will execute the script /home/user/backup.sh at 2:00 AM daily. Here are more examples: Every 5 minutes: 1*/5 * * * * /path/to/script.sh Every day at midnight: 10 0 * * * /path/to/script.sh Every Monday at 3:30 AM: 130 3 * * 1 /path/to/script.sh Every day at 2 PM in June: 10 14 * 6 * /path/to/script.sh Step 4: Save and ExitOnce you’ve added the desired cron jobs, save the file and exit the editor (Ctrl + O, Enter, Ctrl + X if using nano). Step 5: Verify the Cron JobAfter setting the cron job, you can verify if it’s successfully scheduled by running: 1crontab -l This will list all active cron jobs for the current user. Step 6: Logs and DebuggingCron logs can help you debug issues with cron jobs. On most Linux systems, you can find the cron logs in: 1/var/log/cron Or, you can check the system log: 1sudo journalctl -u cron Examples of Common Cron Jobs Run a script every day at 3 AM: 10 3 * * * /home/user/daily_backup.sh Run a script every Sunday at midnight: 10 0 * * 0 /home/user/weekly_report.sh Run a script every 10 minutes: 1*/10 * * * * /home/user/monitor_service.sh SummaryTo configure a cron job: Open crontab with crontab -e. Add a job using the correct time syntax. Save and exit. Verify it with crontab -l. Let me know if you need help with any specific cron job configuration!","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"crontab","slug":"crontab","permalink":"http://philia897.github.io/tags/crontab/"},{"name":"scheduled task","slug":"scheduled-task","permalink":"http://philia897.github.io/tags/scheduled-task/"}]},{"title":"Configure Vaultwarden","slug":"Configure Vaultwarden","date":"2024-10-03T08:31:12.000Z","updated":"2024-10-03T08:47:55.000Z","comments":true,"path":"2024/10/03/Configure Vaultwarden/","permalink":"http://philia897.github.io/2024/10/03/Configure%20Vaultwarden/","excerpt":"","text":"Vaultwarden is a lightweight, self-hosted password manager that is compatible with Bitwarden clients. It is designed as an efficient alternative to the official Bitwarden server, using fewer system resources while providing similar features like storing, managing, and sharing passwords securely. Vaultwarden can be run in a Docker container, making it easy to set up on personal servers or low-powered devices. It supports a wide range of Bitwarden clients, including browser extensions, mobile apps, and desktop apps, and can be extended with features like two-factor authentication and encrypted storage. Configuring Install it and run it using docker, according to the official tutorial Create new account, initialize the database and setup the password, etc. (Optional) Configure firewall using UFW or other tools to allow communication to it. Install the browser extension, mobile app for connection Restart the service using command like this to: disable registration of new users and invitation Enable auto restart port remapping 12345docker run -d --name vaultwarden --restart unless-stopped \\ -e SIGNUPS_ALLOWED=false -e INVITATIONS_ALLOWED=false \\-v ~/Stockroom/dataInUse/vw-data/:/data \\-p 9010:80 \\vaultwarden/server:latest Setup backup for the data folder, vw-data on the example above. Can follow this tutorial in the end For the usage in LAN, it is not necessary to setup HTTPS.For private use, using a Wireguard to setup a private network for all your devices can be a better choice. Backup tutorial Generated by GPT To sync your Vaultwarden data daily using rclone, you’ll need to set up an rclone configuration and then create a script that will run the sync command. Here’s how to do it step-by-step: Step 1: Install RcloneIf you haven’t already installed rclone, you can do so using the following command: 1sudo pacman -S rclone Step 2: Configure Rclone Run Rclone Config:Open a terminal and run: 1rclone config Create a New Remote: Type n to create a new remote. Give it a name (e.g., vaultwarden_backup). Choose the storage type you want to use (e.g., Google Drive, S3, etc.) and follow the prompts to configure it. You may need to authenticate with your cloud service. Test the Configuration:Once you’ve set up the remote, you can test it by running: 1rclone ls vaultwarden_backup: Replace vaultwarden_backup with the name you chose. Step 3: Create a Backup ScriptCreate a script that will perform the sync operation. Here’s a basic example of what the script might look like: Create a New Script File: 1nano ~/backup_vaultwarden.sh Add the Following Content: 12345678#!/bin/bash# Define the source and destinationSOURCE=&quot;/path/to/vaultwarden/data&quot; # Adjust this pathDESTINATION=&quot;vaultwarden_backup:/path/to/destination&quot; # Adjust this path# Perform the syncrclone sync &quot;$SOURCE&quot; &quot;$DESTINATION&quot; --progress --log-file=/path/to/logfile.log --log-level INFO Make sure to adjust the SOURCE and DESTINATION paths accordingly: SOURCE: This should be the path to your Vaultwarden data directory (often found inside your Docker container or where you mounted the volume). DESTINATION: This should be your configured rclone remote destination. Make the Script Executable: 1chmod +x ~/backup_vaultwarden.sh Step 4: Schedule the Script with CronTo run the backup script daily, you can add a cron job. Open the Crontab: 1crontab -e Add a New Cron Job: Add the following line to run the backup every day at 2 AM (adjust the time as needed): 10 2 * * * /bin/bash /home/yourusername/backup_vaultwarden.sh Make sure to replace /home/yourusername with your actual home directory path. Step 5: Verify the BackupAfter the first run, check the log file specified in the script to verify that the backup completed successfully. You can view it with: 1cat /path/to/logfile.log SummaryThis setup will allow you to sync your Vaultwarden data to a cloud storage solution of your choice every day using rclone. If you have any questions or need further customization, feel free to ask!","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"vaultwarden","slug":"vaultwarden","permalink":"http://philia897.github.io/tags/vaultwarden/"},{"name":"crontab","slug":"crontab","permalink":"http://philia897.github.io/tags/crontab/"}]},{"title":"KDE Connect","slug":"KDE Connect","date":"2024-10-03T06:02:19.000Z","updated":"2024-10-03T06:18:58.000Z","comments":true,"path":"2024/10/03/KDE Connect/","permalink":"http://philia897.github.io/2024/10/03/KDE%20Connect/","excerpt":"Great tool for comm between Arch and Android","text":"Great tool for comm between Arch and Android From ChatGPT about how it works. KDE Connect uses a combination of mDNS (Multicast DNS) and a D-Bus-based service discovery mechanism to find and connect with other devices on the same local network. Here’s how it works: How KDE Connect Finds Other Devices: mDNS for Discovery: KDE Connect broadcasts its presence over the local network using mDNS. This allows it to advertise itself and listen for other KDE Connect instances. When you start KDE Connect on your laptop or phone, it sends out multicast DNS packets to announce its service availability. This is how devices can discover each other without needing static IP addresses. D-Bus Communication: KDE Connect uses D-Bus, a message bus system that provides a way for inter-process communication. Devices communicate over D-Bus once they discover each other. When a device wants to connect to another, it uses the D-Bus API to establish the connection and initiate various actions (like sending notifications, sharing files, etc.). Authentication: When a new device is discovered, KDE Connect prompts you to approve the connection. This ensures that only devices you trust can connect to each other. Once authenticated, the devices can communicate securely over the local network. Benefits of Using KDE Connect: Cross-Platform: KDE Connect works on Linux, Android, and other platforms, allowing for seamless integration across devices. Multiple Features: Besides basic discovery, it offers features like file sharing, clipboard synchronization, remote control, and notification mirroring. No Internet Required: KDE Connect operates over the local network, so it doesn’t need an active internet connection for most features. Troubleshooting Discovery:If you’re having trouble discovering devices with KDE Connect, ensure that: Both devices are on the same local network. Firewalls or network settings aren’t blocking mDNS packets. You have KDE Connect running on both devices.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"kde","slug":"kde","permalink":"http://philia897.github.io/tags/kde/"},{"name":"android","slug":"android","permalink":"http://philia897.github.io/tags/android/"}]},{"title":"Wireguard for comm between Arch and Android","slug":"Wireguard for comm between Arch and Android","date":"2024-10-03T06:00:41.000Z","updated":"2024-10-03T08:49:21.000Z","comments":true,"path":"2024/10/03/Wireguard for comm between Arch and Android/","permalink":"http://philia897.github.io/2024/10/03/Wireguard%20for%20comm%20between%20Arch%20and%20Android/","excerpt":"Build a wiregard communication between the Arch laptop and my Android phone. Following the instructions from ChatGPT, which I proved to work.","text":"Build a wiregard communication between the Arch laptop and my Android phone. Following the instructions from ChatGPT, which I proved to work. https://wiki.archlinux.org/title/WireGuard# From GPT: To set up a private network between your Arch Linux laptop and your Android phone within a large LAN, you can create a secure tunnel or virtual private network (VPN) to ensure only these two devices communicate directly, safely bypassing other potentially malicious devices. Here’s a simple approach: Option 1: WireGuard VPNWireGuard is a fast and modern VPN protocol that’s well-suited for creating a private network between your devices. On Arch Linux (Laptop): Install WireGuard: 1sudo pacman -S wireguard-tools Generate keys: 1wg genkey | tee privatekey | wg pubkey &gt; publickey Configure WireGuard:Create a WireGuard configuration file (e.g., /etc/wireguard/wg0.conf): 12345678[Interface]PrivateKey = &lt;your_laptop_private_key&gt;Address = 10.0.0.1/24ListenPort = 51820[Peer]PublicKey = &lt;your_phone_public_key&gt;AllowedIPs = 10.0.0.2/32 Start the WireGuard interface: 1sudo wg-quick up wg0 Can also use other tools, as long as the Wireguard is supported. For example, the network manager from KDE does support Wireguard, so just configure it in the same way. On Android (Phone): Install the WireGuard app from the Play Store. Generate keys on the phone using the WireGuard app. Configure WireGuard on the phone: Add a new tunnel in the app and set the following details: Interface: Use your phone’s private key and set 10.0.0.2/24 as the IP. Peer: Set the laptop’s public key and Allowed IPs: 10.0.0.1/32. Endpoint: Your laptop’s LAN IP address with port 51820. Activate the tunnel in the WireGuard app. Now, the devices can communicate securely over this private network. You’re correct! In WireGuard, peers don’t automatically discover each other on a network without at least one side specifying an endpoint. WireGuard is a stateless, point-to-point VPN, so it relies on manually configuring endpoints to establish a connection between devices. Here’s how it works: Endpoint Role:The endpoint is essentially the “address” where one device (peer) will attempt to contact the other. Without an endpoint, the peers won’t know where to send the initial packets. Why You Need an Endpoint on at Least One Side: WireGuard doesn’t broadcast or discover peers like other networking protocols (e.g., DHCP). Both devices remain silent until one side initiates communication by sending a packet to the other’s IP address and port. At least one side needs the endpoint of the other peer (IP address and port) to start the handshake. So, in your case where the Android phone and the Arch Linux laptop are on the same LAN, here’s how you should approach it: Solutions for Your Case: Configure Endpoint on One Device: You can set up the endpoint on one side (either the laptop or the phone). If you configure the endpoint on the laptop, it will initiate the connection to the phone, and vice versa. For instance, if your phone has a static IP on the LAN (e.g., 192.168.1.10), you can configure the endpoint on the laptop to point to the phone. Example (Laptop-side configuration): 1234[Peer]PublicKey = &lt;phone_public_key&gt;AllowedIPs = 10.0.0.2/32Endpoint = 192.168.1.10:51820 # Phone&#x27;s local IP and WireGuard port Configure Endpoint on Both Devices: To ensure both devices can initiate the connection (especially useful if either device is mobile and changes networks frequently), you can configure endpoints on both sides. For example, on your laptop, set the phone’s LAN IP as the endpoint, and on the phone, set the laptop’s LAN IP as the endpoint. Example (Android-side configuration): 1234[Peer]PublicKey = &lt;laptop_public_key&gt;AllowedIPs = 10.0.0.1/32Endpoint = 192.168.1.5:51820 # Laptop&#x27;s local IP and WireGuard port Dynamic IPs (No Static Endpoint): If either the phone or the laptop has a changing IP (e.g., on different networks): You can configure a dynamic DNS (DDNS) service to ensure the device’s endpoint is always updated, even if the IP changes. This is helpful for devices that are frequently on different networks. Otherwise, you’d need to update the endpoint manually each time the IP changes. Summary: Endpoint is needed for initial communication: At least one device must know the other’s IP and port (via the endpoint) to initiate a connection. Best Practice: Configure an endpoint on one side, and optionally on both sides for more flexibility.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"android","slug":"android","permalink":"http://philia897.github.io/tags/android/"},{"name":"wireguard","slug":"wireguard","permalink":"http://philia897.github.io/tags/wireguard/"},{"name":"arch","slug":"arch","permalink":"http://philia897.github.io/tags/arch/"}]},{"title":"pacman commands","slug":"pacman commands","date":"2024-10-02T23:22:25.000Z","updated":"2024-12-28T21:35:07.000Z","comments":true,"path":"2024/10/03/pacman commands/","permalink":"http://philia897.github.io/2024/10/03/pacman%20commands/","excerpt":"Some useful pacman commands.","text":"Some useful pacman commands. pacman -Qe: check for explicitly installed packages (those you manually chose to install) pacman -Qdt: list all packages that were installed as dependencies but are no longer required by any other package (orphans) sudo pacman -Rns $(pacman -Qdtq): remove these orphaned packages: -Rns: removes the packages and their configuration files. -Qdtq: lists orphaned packages in a simple format suitable pacman --query --list package_name: query and list the files installed by the package query (-Q): Queries the package database to retrieve information about installed packages. Example: pacman –query optimus-manager-git shows details about the optimus-manager-git package. list (-l): Lists all files installed by the specified package. Works in combination with –query.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"pacman","slug":"pacman","permalink":"http://philia897.github.io/tags/pacman/"}]},{"title":"iptables and docker","slug":"iptables and docker","date":"2024-09-30T06:21:06.000Z","updated":"2024-09-30T06:29:16.000Z","comments":true,"path":"2024/09/30/iptables and docker/","permalink":"http://philia897.github.io/2024/09/30/iptables%20and%20docker/","excerpt":"How to properly set firewall regarding docker functions.","text":"How to properly set firewall regarding docker functions. Iptables.service will not always running in the background, instead, enable it to let iptables to read the conf file at boot so that the firewall will be setup once the system starts up. ufw is a front-end to manage iptables. We can enable it and update items at any time. However, according to this explanation from docker. When using port mapping, docker will automatially add the mapped port to iptables so that anyone in the network can access the service. And it can “overwrite” the settings from ufw. To avoid this problem, you can set “iptables” and “ip6tables” to be false, so that in next startup, the iptables will not include the rules generated by docker. 12345# default path: /etc/docker/daemon.json&#123; &quot;iptables&quot;: false, &quot;ip6tables&quot;: false&#125; For all available options: see this link For personal usage, I suggest disable docker’s manipulation of iptables and use ufw to manage the firewall. It is the most clear and graceful approach in my opinion.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://philia897.github.io/tags/docker/"},{"name":"iptables","slug":"iptables","permalink":"http://philia897.github.io/tags/iptables/"},{"name":"ufw","slug":"ufw","permalink":"http://philia897.github.io/tags/ufw/"}]},{"title":"BUG: firewalld and uftables do not work","slug":"BUG_ firewalld and uftables do not work","date":"2024-09-30T04:57:28.000Z","updated":"2024-09-30T05:00:38.000Z","comments":true,"path":"2024/09/30/BUG_ firewalld and uftables do not work/","permalink":"http://philia897.github.io/2024/09/30/BUG_%20firewalld%20and%20uftables%20do%20not%20work/","excerpt":"These two do not work on my Arch system, once I install and configured them, no matter how I configure, it will block all my http and other connections, but SSH works fine. I guess it is firewalld’s fault.","text":"These two do not work on my Arch system, once I install and configured them, no matter how I configure, it will block all my http and other connections, but SSH works fine. I guess it is firewalld’s fault. Even I stop the service, I can not get the access back until I run sudo nft flush ruleset to flush all the rules in kernel. This solution is relatively new but seems not so stable… Thus, I move back to stable solution iptables and ufw. My requirements are simple, and it can be configured easily by myself. Maybe later when firewalld and uftables become more reliable, I will choose to switch at that time.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"system monitor","slug":"system-monitor","permalink":"http://philia897.github.io/tags/system-monitor/"}]},{"title":"Use rclone to mount cloud storage","slug":"Use rclone to mount cloud storage","date":"2024-09-29T07:56:25.000Z","updated":"2024-09-29T08:04:46.000Z","comments":true,"path":"2024/09/29/Use rclone to mount cloud storage/","permalink":"http://philia897.github.io/2024/09/29/Use%20rclone%20to%20mount%20cloud%20storage/","excerpt":"rclone is a versatile tool for syncing and mounting cloud storage, supporting almost all cloud providers, including google cloud, onedrive, webdav, S3, etc. Following these steps to use rclone to mount cloud storage:","text":"rclone is a versatile tool for syncing and mounting cloud storage, supporting almost all cloud providers, including google cloud, onedrive, webdav, S3, etc. Following these steps to use rclone to mount cloud storage: Add configFirst decide the cloud service provider and follow the config example for connection establish. You can find all supported CSP and protocols here: https://rclone.org/#providers After setting up, you can use rclone ls to check if the connection is ready. MountUse S3 for example: 1234mkdir ~/my-s3-bucketrclone mount remote-name:bucket-name ~/my-s3-bucketor rclone mount --daemon remote-name:bucket-name ~/my-s3-bucket Additional information from ChatGPT:When you use rclone to mount an S3 bucket, it behaves like a regular filesystem. This means that any changes you make in the mounted directory (like adding, deleting, or modifying files) will be automatically reflected in the S3 storage. However, there are a few points to keep in mind: How rclone mount Works Real-Time Synchronization: Any file changes (create, update, delete) in the mounted directory are directly uploaded or modified in S3 in real time, as rclone handles the synchronization automatically while the mount is active. Cache Considerations: By default, rclone mount uses a caching mechanism to improve performance. You can control how caching works using various flags (like --vfs-cache-mode). For instance, you can use: --vfs-cache-mode writes to cache writes before sending them to S3. --vfs-cache-mode full to cache all file operations (reads, writes) locally. Unmounting: Before unmounting the S3 bucket, ensure all changes are flushed to avoid losing data. You can do this by ensuring that any pending operations are completed. Example of Mounting Mount the S3 Bucket:Here’s how you can mount your S3 bucket using rclone: 12mkdir ~/my-s3-bucketrclone mount remote-name:bucket-name ~/my-s3-bucket --vfs-cache-mode writes Access Files:You can then navigate to ~/my-s3-bucket and work with your files directly. Any changes you make will automatically sync to S3. Unmount the Bucket:To unmount, use: 1fusermount -u ~/my-s3-bucket Notes Performance: While rclone mount provides real-time syncing, it may not be as fast as accessing local files, especially if your internet connection is slow. Backup and Sync Options: For heavy workloads or critical data, consider using both mounting and scheduled syncs for added redundancy. Using rclone to mount your S3 bucket gives you the flexibility of a filesystem while automatically syncing your changes to the cloud. Auto mountYou can use one systemd.service to do this: You’re right! If you add rclone to /etc/fstab, it requires sudo (root) permissions to run during boot, but your rclone configuration is stored in your user directory (typically ~/.config/rclone/rclone.conf). This can lead to permission issues, since root won’t have access to your user’s configuration. Solutions:1. Use a systemd Service for Mounting (Recommended)This method avoids the need for root access to your user-specific rclone configuration, while still allowing automatic mounting at startup. Steps to Create a systemd Service: Create a systemd service file:Create a file in your user’s systemd directory (without needing sudo): 12mkdir -p ~/.config/systemd/usernano ~/.config/systemd/user/rclone-s3-mount.service Add the following content to the file: 1234567891011121314[Unit]Description=Rclone mount for S3 bucketAfter=network-online.targetWants=network-online.target[Service]Type=notifyExecStart=/usr/bin/rclone mount remote-name:bucket-name path/to/s3-bucket \\ --config /home/YOUR_USER/.config/rclone/rclone.confExecStop=/bin/fusermount -u /mnt/s3-bucketRestart=always[Install]WantedBy=default.target Replace: remote-name:bucket-name with your actual remote and S3 bucket name. YOUR_USER with your username. Adjust the paths as necessary. Enable the service:Now that the service file is ready, enable it for your user: 1systemctl --user enable rclone-s3-mount.service Start the service:You can start the service immediately: 1systemctl --user start rclone-s3-mount.service Enable lingering to allow auto-start on boot:To ensure that systemd starts the service at boot, enable lingering for your user: 1sudo loginctl enable-linger YOUR_USER This ensures that rclone will mount the S3 bucket at startup, without needing root permissions or dealing with /etc/fstab. It also solves the problem of accessing your user’s rclone configuration file. 2. Move the rclone.conf File to a Location Root Can Access (Less Recommended)If you prefer sticking with /etc/fstab, you could move your rclone.conf file to a system-wide location, such as /etc/rclone/rclone.conf, which root can access. Then, update the configuration file path in /etc/fstab. Move the config file: 12sudo mkdir /etc/rclonesudo cp ~/.config/rclone/rclone.conf /etc/rclone/rclone.conf Edit /etc/fstab:Modify the rclone mount entry in /etc/fstab to reference the new config file path: 1rclone#remote-name:bucket-name /mnt/s3-bucket fuse rclone,config=/etc/rclone/rclone.conf,uid=1000,gid=1000,allow_other,umask=0022 0 0 Adjust permissions:Make sure the config file is readable by root: 1sudo chmod 644 /etc/rclone/rclone.conf Note: This approach exposes your credentials to a system-wide location, which may not be as secure as keeping the configuration in your user directory. ConclusionUsing a systemd service is the recommended approach because it maintains the security of your user-specific rclone configuration while still allowing the S3 bucket to be mounted automatically at startup.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"rclone","slug":"rclone","permalink":"http://philia897.github.io/tags/rclone/"},{"name":"cloud storage","slug":"cloud-storage","permalink":"http://philia897.github.io/tags/cloud-storage/"}]},{"title":"Arch frequently used applications","slug":"Arch frequently used applications","date":"2024-09-28T19:14:01.000Z","updated":"2024-12-07T07:17:41.000Z","comments":true,"path":"2024/09/29/Arch frequently used applications/","permalink":"http://philia897.github.io/2024/09/29/Arch%20frequently%20used%20applications/","excerpt":"","text":"Browser Brave Firefox w3m : TUI browser running in terminal Dolphin: file explorer Text &amp; Note Nvim Joplin with cloud sync Kate: editor VS Codium: opensource vscode, easy to use heynote: easy draft paper for programmers Libreoffice: opensource alternative to microsoft office Okular: pdf reader Zotero: academic paper manager marktext: markdown editor ImHex: hex editor Media OBS Studio: screen recorder GIMP: photo editor flameshot: screenshot tool Tools: rclone: swiss knife for cloud storage including sync, mount, read, write, etc. scc: Cound the statistics of codes in your project onefetch: github repo statistics fastfetch: alternative to neofetch rofi: fast runner keyd: remap keyboard tool exa: advanced ls dust: advanced du duf: advanced df yazi: terminal file manager, graceful and simple docker: run some tools like it-tools or vaultwarden timeshift: System backup and snapshoot manage tool (Especially important for Arch) collectd: collect data of system usage, for performance analyzing vaultwarden: managing credentials and passwords cmd-wrapped: command usage statistics pot: translator safeeyes: break reminder to protect eyes and rest clash-verge: clash proxy client kde-connect: connect my Arch with my phone fcitx5 with rime: pinyin input solution btop: system monitor, advanced alternative of htop bat: alternative to cat Sniffnet: network traffic analyzer and capture ufw or firewalld: Firewall manager","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"applications","slug":"applications","permalink":"http://philia897.github.io/tags/applications/"}]},{"title":"Check the dependency of packages","slug":"Check the dependency of packages","date":"2024-09-23T02:19:30.000Z","updated":"2024-09-23T02:20:52.000Z","comments":true,"path":"2024/09/23/Check the dependency of packages/","permalink":"http://philia897.github.io/2024/09/23/Check%20the%20dependency%20of%20packages/","excerpt":"","text":"Check packages that this package depends onIn Arch Linux, you can check if a package is installed and display its version and information using pacman or pactree. Here’s how: 1. Using pacman:To check if a package is installed and view its version and other details, run: 1pacman -Qi &lt;package_name&gt; For example, to check if rust is installed and get information about it: 1pacman -Qi rust 2. Using pactree:You can also use pactree (from the pacman-contrib package) to display the dependency tree of the package. This is useful for seeing what other packages the package depends on. 1pactree &lt;package_name&gt; If the package isn’t installed, pacman will return an error like error: package &#39;&lt;package_name&gt;&#39; was not found. Check the packages that depends on this packageTo check which packages depend on a specific package in Arch Linux, you can use the pacman command with the -Qi option or pactree for a more detailed dependency tree. 1. Using pacman:To list all packages that depend on a specific package, you can use: 1pacman -Qi &lt;package_name&gt; | grep &quot;Required By&quot; For example, to check which packages depend on rust: 1pacman -Qi rust | grep &quot;Required By&quot; If no package depends on it, it will show None. Otherwise, it will list the dependent packages. 2. Using pactree:To display the reverse dependency tree (i.e., packages that depend on the specified package), use the -r flag with pactree: 1pactree -r &lt;package_name&gt; For example, to see all packages that depend on rust: 1pactree -r rust This will give you a full tree of reverse dependencies.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"pacman","slug":"pacman","permalink":"http://philia897.github.io/tags/pacman/"}]},{"title":"Use collectd to monitor my system","slug":"Use collectd to monitor my system","date":"2024-09-13T01:27:53.000Z","updated":"2024-12-09T01:35:52.000Z","comments":true,"path":"2024/09/13/Use collectd to monitor my system/","permalink":"http://philia897.github.io/2024/09/13/Use%20collectd%20to%20monitor%20my%20system/","excerpt":"After searching for one day I choose and configured collectd, version 5.12 to monitor my system usage across the time, including the usage of CPU, memory, root disk, and network interface. Gaved it up, write my own project instead, which fits my need better: luce-python","text":"After searching for one day I choose and configured collectd, version 5.12 to monitor my system usage across the time, including the usage of CPU, memory, root disk, and network interface. Gaved it up, write my own project instead, which fits my need better: luce-python Reference: https://wiki.archlinux.org/title/Collectd It is generally simple, just need to enable the plugins: cpu, memory, disk, df, and interface, then configuring for a while, everything is done then. It does not consume much resources and easy to use. Here are the four scripts I used to generate figures using rddtool. 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash# Usage: ./cpu_usage.sh &lt;start&gt; &lt;end&gt;# Example: ./cpu_usage.sh -86400 now (for the last 24 hours)START=$&#123;1:--86400&#125; # Default to the last 24 hours if not providedEND=$&#123;2:-now&#125; # Default to &#x27;now&#x27; if not providedrrdtool graph avg-cpu-usage.png \\--start $START --end $END \\--width 800 --height 400 \\--title &quot;Average CPU Usage&quot; \\--vertical-label &quot;CPU Usage (%)&quot; \\--y-grid 10:1 \\--font TITLE:14: \\--font AXIS:10: \\--font LEGEND:12: \\--font UNIT:10: \\DEF:cpu0=/var/lib/collectd/rrd/archlinux/cpu-0/percent-idle.rrd:value:AVERAGE \\DEF:cpu1=/var/lib/collectd/rrd/archlinux/cpu-1/percent-idle.rrd:value:AVERAGE \\DEF:cpu2=/var/lib/collectd/rrd/archlinux/cpu-2/percent-idle.rrd:value:AVERAGE \\DEF:cpu3=/var/lib/collectd/rrd/archlinux/cpu-3/percent-idle.rrd:value:AVERAGE \\DEF:cpu4=/var/lib/collectd/rrd/archlinux/cpu-4/percent-idle.rrd:value:AVERAGE \\DEF:cpu5=/var/lib/collectd/rrd/archlinux/cpu-5/percent-idle.rrd:value:AVERAGE \\DEF:cpu6=/var/lib/collectd/rrd/archlinux/cpu-6/percent-idle.rrd:value:AVERAGE \\DEF:cpu7=/var/lib/collectd/rrd/archlinux/cpu-7/percent-idle.rrd:value:AVERAGE \\DEF:cpu8=/var/lib/collectd/rrd/archlinux/cpu-8/percent-idle.rrd:value:AVERAGE \\DEF:cpu9=/var/lib/collectd/rrd/archlinux/cpu-9/percent-idle.rrd:value:AVERAGE \\DEF:cpu10=/var/lib/collectd/rrd/archlinux/cpu-10/percent-idle.rrd:value:AVERAGE \\DEF:cpu11=/var/lib/collectd/rrd/archlinux/cpu-11/percent-idle.rrd:value:AVERAGE \\DEF:cpu12=/var/lib/collectd/rrd/archlinux/cpu-12/percent-idle.rrd:value:AVERAGE \\DEF:cpu13=/var/lib/collectd/rrd/archlinux/cpu-13/percent-idle.rrd:value:AVERAGE \\DEF:cpu14=/var/lib/collectd/rrd/archlinux/cpu-14/percent-idle.rrd:value:AVERAGE \\DEF:cpu15=/var/lib/collectd/rrd/archlinux/cpu-15/percent-idle.rrd:value:AVERAGE \\CDEF:avg_cpu=cpu0,cpu1,+,cpu2,+,cpu3,+,cpu4,+,cpu5,+,cpu6,+,cpu7,+,cpu8,+,cpu9,+,cpu10,+,cpu11,+,cpu12,+,cpu13,+,cpu14,+,cpu15,+,16,/ \\CDEF:cpu_usage=100,avg_cpu,- \\CDEF:smoothed_cpu=cpu_usage,180,TREND \\LINE2:smoothed_cpu#00FF00:&quot;Smoothed CPU Usage (%)&quot; \\HRULE:80#FF0000:&quot;High CPU Usage (80%)&quot; \\HRULE:90#FF6600:&quot;Critical CPU Usage (90%)&quot;echo &quot;CPU usage graph generated: avg-cpu-usage.png&quot; 12345678910111213141516171819202122232425262728#!/bin/bash# Usage: ./memory_usage.sh &lt;start&gt; &lt;end&gt;# Example: ./memory_usage.sh -1d now (for the last day)START=$&#123;1:-&quot;-1d&quot;&#125; # Default to last 24 hoursEND=$&#123;2:-&quot;now&quot;&#125; # Default to &#x27;now&#x27;rrdtool graph memory-usage.png \\--start $START --end $END \\--width 800 --height 400 \\--title &quot;Memory Usage&quot; \\--vertical-label &quot;Memory (GB)&quot; \\--font TITLE:14: \\--font AXIS:10: \\--font LEGEND:12: \\--font UNIT:10: \\DEF:used=/var/lib/collectd/rrd/archlinux/memory/memory-used.rrd:value:AVERAGE \\DEF:free=/var/lib/collectd/rrd/archlinux/memory/memory-free.rrd:value:AVERAGE \\DEF:cached=/var/lib/collectd/rrd/archlinux/memory/memory-cached.rrd:value:AVERAGE \\CDEF:used_gb=used,1073741824,/ \\CDEF:free_gb=free,1073741824,/ \\CDEF:cached_gb=cached,1073741824,/ \\LINE2:used_gb#FF0000:&quot;Used Memory&quot; \\LINE1:free_gb#00FF00:&quot;Free Memory&quot; \\LINE1:cached_gb#0000FF:&quot;Cached Memory&quot;echo &quot;Memory usage graph generated: memory-usage.png&quot; 123456789101112131415161718192021222324252627#!/bin/bash# Usage: ./network_usage.sh &lt;start&gt; &lt;end&gt;# Example: ./network_usage.sh -1d now (for the last day)START=$&#123;1:-&quot;-1d&quot;&#125; # Default to last 24 hoursEND=$&#123;2:-&quot;now&quot;&#125; # Default to &#x27;now&#x27;rrdtool graph network-usage.png \\--start $START --end $END \\--width 800 --height 400 \\--title &quot;Network Usage (wlan0)&quot; \\--vertical-label &quot;Network (Bytes)&quot; \\--font TITLE:14: \\--font AXIS:10: \\--font LEGEND:12: \\--font UNIT:10: \\DEF:rx=/var/lib/collectd/rrd/archlinux/interface-wlan0/if_octets.rrd:rx:AVERAGE \\DEF:tx=/var/lib/collectd/rrd/archlinux/interface-wlan0/if_octets.rrd:tx:AVERAGE \\LINE2:rx#00FF00:&quot;Received (Bytes)&quot; \\LINE2:tx#0000FF:&quot;Transmitted (Bytes)&quot; \\COMMENT:&quot;\\n&quot; \\GPRINT:rx:AVERAGE:&quot;Avg Received\\: %6.2lf Bytes&quot; \\GPRINT:tx:AVERAGE:&quot;Avg Transmitted\\: %6.2lf Bytes&quot; \\COMMENT:&quot;\\n&quot;echo &quot;network usage graph generated: network-usage.png&quot; 123456789101112131415161718192021222324252627282930313233#!/bin/bash# Usage: ./disk_usage.sh &lt;start&gt; &lt;end&gt;# Example: ./disk_usage.sh -1d now (for the last day)START=$&#123;1:-&quot;-1d&quot;&#125; # Default to last 24 hoursEND=$&#123;2:-&quot;now&quot;&#125; # Default to &#x27;now&#x27;rrdtool graph disk-usage.png \\--start $START --end $END \\--width 800 --height 400 \\--title &quot;Disk Usage (nvme0n1p2)&quot; \\--vertical-label &quot;Disk Usage (GB)&quot; \\--font TITLE:14: \\--font AXIS:10: \\--font LEGEND:12: \\--font UNIT:10: \\DEF:read=/var/lib/collectd/rrd/archlinux/disk-nvme0n1p2/disk_octets.rrd:read:AVERAGE \\DEF:write=/var/lib/collectd/rrd/archlinux/disk-nvme0n1p2/disk_octets.rrd:write:AVERAGE \\DEF:used_space=/var/lib/collectd/rrd/archlinux/df-root/df_complex-used.rrd:value:AVERAGE \\CDEF:used_space_gb=used_space,1073741824,/ \\CDEF:read_gb=read,1073741824,/ \\CDEF:write_gb=write,1073741824,/ \\LINE2:read_gb#FF0000:&quot;Read (GB)&quot; \\LINE2:write_gb#0000FF:&quot;Write (GB)&quot; \\LINE2:used_space_gb#FF8000:&quot;Used Disk Space (GB)&quot; \\COMMENT:&quot;\\n&quot; \\GPRINT:read_gb:AVERAGE:&quot;Avg Read\\: %6.2lf GB&quot; \\GPRINT:write_gb:AVERAGE:&quot;Avg Write\\: %6.2lf GB&quot; \\GPRINT:used_space_gb:LAST:&quot;Used Disk Space\\: %6.2lf GB&quot; \\COMMENT:&quot;\\n&quot;echo &quot;Disk usage graph generated: disk-usage.png&quot; Time formatRRDTool accepts several formats for specifying the start and end times when generating graphs. Here are some common formats: Common start and end Time Formats Absolute time: Format: YYYYMMDDHHMMSS Example: 20240912120000 (for September 12, 2024, at 12:00:00) Relative time: Use relative values such as now or time offsets. Examples: -1h: One hour ago -1d: One day ago -1w: One week ago -1m: One month ago -1y: One year ago -1: One second ago now: Current time Human-readable time: Format: start, end, or combinations of human-readable keywords. Examples: &quot;12:00&quot; &quot;yesterday&quot; &quot;midnight&quot; &quot;noon&quot;","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"system monitor","slug":"system-monitor","permalink":"http://philia897.github.io/tags/system-monitor/"}]},{"title":"Recover from timeshift","slug":"Recover from timeshift","date":"2024-08-28T02:04:55.000Z","updated":"2024-08-28T02:05:10.000Z","comments":true,"path":"2024/08/28/Recover from timeshift/","permalink":"http://philia897.github.io/2024/08/28/Recover%20from%20timeshift/","excerpt":"Learn about how to use timeshift properly in case of scroll failure when using Arch","text":"Learn about how to use timeshift properly in case of scroll failure when using Arch If your system becomes unstable or unusable after a failed sudo pacman -Syu update, and you have a Timeshift snapshot, you can recover your system by restoring the snapshot. Here’s how you can do it: Step 1: Boot into a Live USB (if necessary)If your system is not booting, you’ll need to boot from a live USB. Create a Live USB if you don’t have one using a tool like Rufus or balenaEtcher. Boot from the Live USB and select the “Boot Arch Linux” option or any Linux distribution that supports Timeshift. Step 2: Install Timeshift on the Live USB Environment (if needed)If Timeshift is not available in the live environment, you’ll need to install it. Connect to the internet: 1sudo wifi-menu # or use another method to connect Install Timeshift: 1sudo pacman -S timeshift Step 3: Mount the Root FilesystemIf you booted into a live environment, you’ll need to mount your root filesystem where Timeshift snapshots are stored. Find your root partition: 1lsblk Look for the partition where your root filesystem (usually /dev/sdaX or /dev/nvme0n1pX) is located. Mount the root partition: 1sudo mount /dev/sdaX /mnt Replace /dev/sdaX with your actual root partition. Mount the boot partition (if you have a separate boot partition): 1sudo mount /dev/sdaY /mnt/boot Replace /dev/sdaY with your boot partition. Mount other essential filesystems: 123sudo mount --bind /dev /mnt/devsudo mount --bind /proc /mnt/procsudo mount --bind /sys /mnt/sys Step 4: Restore the Timeshift Snapshot Launch Timeshift:If you are in the live environment: 1sudo timeshift --gui If you are in your normal system (like if the system is bootable but unstable), you can run Timeshift normally: 1timeshift-launcher Select the snapshot you want to restore. Choose one from before the failed pacman -Syu update. Click “Restore” and follow the prompts. Timeshift will restore the selected snapshot, effectively rolling back your system to the state it was in when the snapshot was taken. Be sure to check the options to restore the bootloader if you believe it was affected. Step 5: Reboot Your System After the restoration process is complete, unmount the partitions if necessary: 123sudo umount /mnt/bootsudo umount /mnt/dev /mnt/proc /mnt/syssudo umount /mnt Reboot your system: 1sudo reboot Your system should now boot into the restored snapshot. Troubleshooting If the system still doesn’t boot correctly, make sure that the bootloader and essential files were restored properly. You may need to chroot into the system from the live environment and reinstall the bootloader if necessary. If Timeshift restores successfully but you still face issues, there might be a deeper problem unrelated to the update that needs additional troubleshooting. This process should help you recover your system to a previous working state using Timeshift after a failed system update.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"backup","slug":"backup","permalink":"http://philia897.github.io/tags/backup/"},{"name":"timeshift","slug":"timeshift","permalink":"http://philia897.github.io/tags/timeshift/"}]},{"title":"Add NTFS automount","slug":"Add NTFS automount","date":"2024-08-27T17:55:20.000Z","updated":"2024-08-27T17:58:58.000Z","comments":true,"path":"2024/08/28/Add NTFS automount/","permalink":"http://philia897.github.io/2024/08/28/Add%20NTFS%20automount/","excerpt":"","text":"From ChatGPT: To automatically mount an NTFS disk on boot in Arch Linux, you can configure it by editing the /etc/fstab file. Here’s how you can do it: 1. Identify the NTFS Partition: First, you need to find the UUID or device name of the NTFS partition you want to mount. Use the following command: 1234 sudo blkid or sudo fdisk -l You should see output like: 1/dev/sda1: UUID=&quot;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX&quot; TYPE=&quot;ntfs&quot; PARTUUID=&quot;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX&quot; Note down the UUID or the device name (e.g., /dev/sda1). 2. Install the Required NTFS Driver: Arch Linux requires the ntfs-3g package to mount NTFS partitions with read-write support. Install it with: 1sudo pacman -S ntfs-3g 3. Edit the /etc/fstab File: Open the /etc/fstab file with your preferred text editor: 1sudo nano /etc/fstab Add a new line for your NTFS partition using either the UUID or the device name. The syntax is: 1UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX /mnt/your-mount-point ntfs-3g defaults 0 0 Alternatively, if you prefer to use the device name: 1/dev/sda1 /mnt/your-mount-point ntfs-3g defaults 0 0 Replace UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX with your partition’s UUID or /dev/sda1 with your device name. Replace /mnt/your-mount-point with the directory where you want the partition to be mounted (you need to create this directory if it doesn’t exist). 4. Create the Mount Point (if necessary): If the mount point directory doesn’t exist, create it: 1sudo mkdir -p /mnt/your-mount-point 5. Test the Configuration: Test if the configuration works without rebooting: 1sudo mount -a If you don’t see any errors, the partition should now be mounted at the specified mount point. 6. Reboot and Verify: Reboot your system: 1sudo reboot After rebooting, check if the NTFS partition is automatically mounted by running: 1df -h You should see your NTFS partition listed. Example /etc/fstab Entry:Here’s what a complete line might look like in your /etc/fstab: 1UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX /mnt/ntfsdisk ntfs-3g defaults 0 0 This setup will ensure your NTFS partition is automatically mounted at boot.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"ntfs","slug":"ntfs","permalink":"http://philia897.github.io/tags/ntfs/"},{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"}]},{"title":"w3m browser in terminal","slug":"w3m browser in terminal","date":"2024-08-27T06:37:29.000Z","updated":"2024-08-27T06:39:22.000Z","comments":true,"path":"2024/08/27/w3m browser in terminal/","permalink":"http://philia897.github.io/2024/08/27/w3m%20browser%20in%20terminal/","excerpt":"A interesting browser that runs inside in terminal, very light and small, I’m curious how it is written to support such a comprehensive functionalities.","text":"A interesting browser that runs inside in terminal, very light and small, I’m curious how it is written to support such a comprehensive functionalities. Find more information about how to use it by: http://blog.lujun9972.win/blog/2016/12/11/w3m%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"w3m","slug":"w3m","permalink":"http://philia897.github.io/tags/w3m/"},{"name":"browser","slug":"browser","permalink":"http://philia897.github.io/tags/browser/"}]},{"title":"Modify desktop entry to support run app with flags","slug":"Modify desktop entry to support run app with flags","date":"2024-08-27T05:05:57.000Z","updated":"2025-02-24T18:24:27.000Z","comments":true,"path":"2024/08/27/Modify desktop entry to support run app with flags/","permalink":"http://philia897.github.io/2024/08/27/Modify%20desktop%20entry%20to%20support%20run%20app%20with%20flags/","excerpt":"Encounter some issues when using electron-based applications, and need to run them with some flags. Best way to create a new desktop entry for it.","text":"Encounter some issues when using electron-based applications, and need to run them with some flags. Best way to create a new desktop entry for it. From GPT: Option 2 is the best choice How to enable electron-based application with Chinese input like fcitx5 ime, etc. --enable-wayland-ime seems to work as one experimental feature for now. Do not modify the original desktop entry directly, create a new one instead, or it will be overwritten when upgrading. To run Joplin with specific flags like --enable-wayland-ime by default every time you start the application, you have several options depending on how you prefer to configure this. Below are some common approaches: Option 1: Create a Wrapper ScriptYou can create a small script that automatically adds the flags whenever Joplin is launched. Create a script in your local bin directory: 12mkdir -p ~/.local/binnano ~/.local/bin/joplin Add the following content to the script: 12#!/bin/bash/usr/bin/joplin-desktop --enable-wayland-ime &quot;$@&quot; This script will run Joplin with the --enable-wayland-ime flag every time, while still allowing you to pass additional arguments (&quot;$@&quot;). Make the script executable: 1chmod +x ~/.local/bin/joplin Ensure ~/.local/bin is in your PATH. It typically is by default, but you can verify this by running: 1echo $PATH Now, when you type joplin in the terminal or launch it via an application menu, it will include the --enable-wayland-ime flag by default. Option 2: Modify the Desktop Entry (for Launching via Application Menu)If you primarily launch Joplin from your desktop environment’s application menu, you can modify the .desktop file to include the flags. Copy the existing Joplin desktop entry to your local applications directory: 12mkdir -p ~/.local/share/applicationscp /usr/share/applications/joplin.desktop ~/.local/share/applications/ Edit the copied file: 1nano ~/.local/share/applications/joplin.desktop Locate the Exec line and modify it to include the desired flags: 1Exec=joplin-desktop --enable-wayland-ime %U Save the file and exit. Now, whenever you launch Joplin from the application menu, it will automatically use the --enable-wayland-ime flag. Option 3: Use an Alias (for Terminal Launches)If you typically launch Joplin from the terminal, you can create an alias in your shell configuration file. Open your .bashrc (or .zshrc if you use zsh): 1nano ~/.bashrc Add the following line: 1alias joplin=&#x27;joplin-desktop --enable-wayland-ime&#x27; Save the file and exit. Reload your shell configuration: 1source ~/.bashrc Now, typing joplin in the terminal will include the flag automatically. Summary Wrapper Script (Option 1): Flexible and works universally. Desktop Entry Modification (Option 2): Ideal if you use the application menu. Alias (Option 3): Quick solution for terminal launches. Choose the method that best fits your workflow.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"wayland","slug":"wayland","permalink":"http://philia897.github.io/tags/wayland/"},{"name":"electron","slug":"electron","permalink":"http://philia897.github.io/tags/electron/"},{"name":"desktop entry","slug":"desktop-entry","permalink":"http://philia897.github.io/tags/desktop-entry/"}]},{"title":"Set global environmental variables on session startup","slug":"Set global environmental variables on session star","date":"2024-08-26T21:33:40.000Z","updated":"2024-08-26T21:44:47.000Z","comments":true,"path":"2024/08/27/Set global environmental variables on session star/","permalink":"http://philia897.github.io/2024/08/27/Set%20global%20environmental%20variables%20on%20session%20star/","excerpt":"I used Option3: Use Systemd User Environment Configuration","text":"I used Option3: Use Systemd User Environment Configuration If you prefer to set the MOZ_ENABLE_WAYLAND=1 environment variable globally, you can do this by configuring it in a way that it applies to your entire user session. Here’s a more detailed explanation of how to do this in Arch Linux. Option 1: Set the Variable in the User’s ~/.profileOne common way to set environment variables globally for your user is by adding them to ~/.profile. This file is sourced by various login shells and desktop environments when you log in. Open (or create) the ~/.profile file: 1nano ~/.profile Add the following line at the end of the file: 1export MOZ_ENABLE_WAYLAND=1 Save the file and exit. For the change to take effect, log out of your session and log back in, or you can apply it immediately with: 1source ~/.profile This method ensures that the environment variable is set whenever you log into your desktop environment, regardless of the shell you use. Option 2: Set the Variable in the ~/.bash_profileIf you’re using bash, another option is to set the environment variable in your ~/.bash_profile. This file is specifically for setting environment variables for login shells. Open the ~/.bash_profile file (create it if it doesn’t exist): 1nano ~/.bash_profile Add the following line: 1export MOZ_ENABLE_WAYLAND=1 Save the file and exit. Log out and log back in to apply the change, or run: 1source ~/.bash_profile This is effective if your desktop environment sources ~/.bash_profile when starting a session. Option 3: Use Systemd User Environment Configuration (Recommended for Modern Systems) [I used for my system]Systemd provides a cleaner and more reliable way to manage environment variables for user sessions. Create a new environment configuration file: 12mkdir -p ~/.config/environment.dnano ~/.config/environment.d/envvars.conf Add the following line: 1MOZ_ENABLE_WAYLAND=1 Save the file and exit. Reload the systemd user environment: 1systemctl --user daemon-reload Log out and log back in for the changes to take effect. This method is highly recommended for modern desktop environments (like KDE Plasma) because it’s designed to work seamlessly with systemd. Option 4: Set the Variable in /etc/environment (Applies to All Users)If you want to set the variable for all users on the system, you can do so in the /etc/environment file. Note that this requires root privileges. Edit the /etc/environment file: 1sudo nano /etc/environment Add the following line: 1MOZ_ENABLE_WAYLAND=1 Save the file and exit. Reboot your system or log out and log back in for the change to take effect. This method applies the environment variable globally across all users and sessions. Summary Single user, across all shells and desktop sessions: Use ~/.profile or ~/.bash_profile. Modern systemd-based approach: Use ~/.config/environment.d/envvars.conf. System-wide for all users: Use /etc/environment. For most users, Option 3 (systemd environment configuration) is the most robust and cleanest solution, especially for KDE Plasma or other modern desktop environments that rely heavily on systemd. I used Method 3: using environemt.d&#x2F;envvars.conf To check the variable, using echo $VARIABLE_NAME Can also refer to this tutorial from Arch Wiki: https://wiki.archlinux.org/title/Environment_variables","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"}]},{"title":"Enable apps run upon Wayland","slug":"Enable apps run upon Wayland","date":"2024-08-26T20:50:19.000Z","updated":"2024-08-27T04:35:53.000Z","comments":true,"path":"2024/08/27/Enable apps run upon Wayland/","permalink":"http://philia897.github.io/2024/08/27/Enable%20apps%20run%20upon%20Wayland/","excerpt":"","text":"Thunderbird1MOZ_ENABLE_WAYLAND=1 thunderbird refer to https://wiki.archlinux.org/title/Thunderbird#Wayland Bravego to brave:&#x2F;&#x2F;flags&#x2F;#ozone-platform-hint and choose wayland refer to https://www.reddit.com/r/brave_browser/comments/o4x1rr/brave_browser_on_wayland_instead_of_xwayland/ JoplinAs Joplin is based on Electron, should set this env variable to allow Wayland: 1ELECTRON_OZONE_PLATFORM_HINT=auto refer to https://wiki.archlinux.org/title/Wayland#Electron About how to set env variable, check my previous note. After enabling wayland for joplin and brave, seems fcitx5-based Chinese RIME input method does not work any more, to fix this, can use:","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"wayland","slug":"wayland","permalink":"http://philia897.github.io/tags/wayland/"},{"name":"electron","slug":"electron","permalink":"http://philia897.github.io/tags/electron/"}]},{"title":"Check app in wayland or Xorg","slug":"Check app in wayland or Xorg","date":"2024-08-26T20:37:45.000Z","updated":"2024-08-26T20:39:18.000Z","comments":true,"path":"2024/08/27/Check app in wayland or Xorg/","permalink":"http://philia897.github.io/2024/08/27/Check%20app%20in%20wayland%20or%20Xorg/","excerpt":"Using xprop works fine for me.","text":"Using xprop works fine for me. From ChatGPT: In Arch Linux with KDE Plasma, applications can run either under Wayland or Xorg, even if you’re using a Wayland session. To determine whether a specific application is running under Wayland or Xorg (via XWayland), you can follow these methods: Method 1: Using xpropThe xprop command works for applications running under Xorg (including those running under XWayland in a Wayland session). Run xprop in your terminal. Your cursor will turn into a crosshair. Click on the window of the application you want to inspect. If the application is running under XWayland, you’ll see output with the property _NET_WM_PID. This indicates it’s running under XWayland (and therefore Xorg-based). If it’s a Wayland native app, xprop will not work since it doesn’t interact with Wayland-native apps. Method 2: Checking the WAYLAND_DISPLAY Environment VariableYou can check whether an app is using Wayland by inspecting its environment variables: Find the process ID (PID) of the application: 1pidof &lt;application-name&gt; Use the grep command to check if the application is running under Wayland: 1cat /proc/$(pidof &lt;application-name&gt;)/environ | tr &#x27;\\0&#x27; &#x27;\\n&#x27; | grep WAYLAND_DISPLAY If WAYLAND_DISPLAY is present, the application is running natively under Wayland. If not, it is likely running under XWayland or Xorg. Method 3: Using xwininfo or xdotoolSimilar to xprop, both xwininfo and xdotool only work with X11&#x2F;XWayland applications. If these commands detect the application, it’s running under XWayland. Run: 1xwininfo Or: 1xdotool search --name &quot;&lt;application-name&gt;&quot; If these commands successfully interact with the window, it’s an XWayland application. Otherwise, it’s native to Wayland. Method 4: Using KSysGuard or System MonitorIn KDE Plasma: Open System Monitor or KSysGuard. Find the application in the process list. Right-click the application and check its properties or command. If you see XWayland in the details, the app is running under XWayland. Summary If the app is visible via xprop, xwininfo, or xdotool, it is running under XWayland (and therefore Xorg). If the environment variable WAYLAND_DISPLAY is set, the app is running natively on Wayland. These methods should help you determine whether an app is using Wayland natively or is running under XWayland in your KDE Plasma environment. In practice using xprop works fine for me.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"wayland","slug":"wayland","permalink":"http://philia897.github.io/tags/wayland/"},{"name":"x11","slug":"x11","permalink":"http://philia897.github.io/tags/x11/"}]},{"title":"Downgrading packages - ArchWiki","slug":"Downgrading packages - ArchWiki","date":"2024-08-12T18:28:32.000Z","updated":"2024-08-12T18:33:34.000Z","comments":true,"path":"2024/08/13/Downgrading packages - ArchWiki/","permalink":"http://philia897.github.io/2024/08/13/Downgrading%20packages%20-%20ArchWiki/","excerpt":"How to downgrade package of pacman or aur, which is not so convenient.","text":"How to downgrade package of pacman or aur, which is not so convenient. Refer to https://wiki.archlinux.org/title/Downgrading_packages Before downgrading a single or multiple packages, consider why you wish to do so. If it is due to a bug, follow the bug reporting guidelines. I.e. search the Arch Linux Bugtracker for existing tasks and if there is none, add a new task. It is better to correct bugs, or at least warn other users of possible issues. Warning: Downgrading one package may require that its dependencies be downgraded as well. When the number of packages to downgrade is large, consider using a snapshot. See Arch Linux Archive#How to restore all packages to a specific date. Be careful with changes to configuration files and scripts. For now pacman will handle this for us, as long as we do not bypass its safeguards. If a downgrade involves a soname change, all dependencies may need downgrading or rebuilding too. Return to an earlier package versionUsing the pacman cache This only works if the old version has been installed previously. Not very useful to be honest If a package was installed at an earlier stage, and the pacman cache was not cleaned, install an earlier version from /var/cache/pacman/pkg/. This process will remove the current package and install the older version. Dependency changes will be handled, but pacman will not handle version conflicts. If a library or other package needs to be downgraded with the packages, please be aware that you will have to downgrade this package yourself as well. # pacman -U file:&#x2F;&#x2F;&#x2F;var&#x2F;cache&#x2F;pacman&#x2F;pkg&#x2F;package-old_version.pkg.tar.type Note that type will be xz for older package builds, and zst for those following the 2020 change. Once the package is reverted, temporarily add it to the IgnorePkg section of pacman.conf, until the difficulty with the updated package is resolved. Downgrading the kernelIn case of issue with a new kernel, the Linux packages can be downgraded to the last working ones #Using the pacman cache. Go into the directory /var/cache/pacman/pkg and downgrade at least linux, linux-headers and any kernel modules. For example: # pacman -U file:&#x2F;&#x2F;linux-4.15.8-1-x86_64.pkg.tar.xz file:&#x2F;&#x2F;linux-headers-4.15.8-1-x86_64.pkg.tar.xz file:&#x2F;&#x2F;virtualbox-host-modules-arch-5.2.8-4-x86_64.pkg.tar.xz Tip: If you are unable to boot after a kernel update, you can downgrade the kernel chrooting into the system. Boot using an Arch Linux USB flash installation media and mount the partition where your system is installed to /mnt. If you have /boot or /var on separate partitions, also mount them to /mnt (e.g. mount /dev/sdc3 /mnt/boot). Then chroot into the system using: # arch-chroot &#x2F;mnt Now you can go into the pacman cache directory and downgrade the Linux packages using the command indicated above. Once done, exit the chroot (with exit) and reboot. Arch Linux Archive Works only for official packages except AUR packages The Arch Linux Archive is a daily snapshot of the official repositories. It can be used to install a previous package version, or restore the system to an earlier date. Rebuild the packageIf the package is unavailable, find the correct PKGBUILD and rebuild it with makepkg. For packages from the official repositories, retrieve the PKGBUILD with ABS and change the software version. Alternatively, find the package on the Packages website, click “View Changes”, and navigate to the desired version. The necessary files can then be downloaded from the directory so that the package can be rebuilt. See also Arch build system#Using the pkgctl tool. Works for AUR packages, using git checkout [Hash Value] to change to a specific version and use makepkg -si to rebuild the package. Old AUR packages can be built by checking out an old commit in the AUR package Git repository. For pre-2015 AUR3 PKGBUILDs, see Arch User Repository#Git repositories for AUR3 packages. Automation Did not try, and should not try this unstable bash script I think… downgrade — A Bash script to downgrade one (or multiple) packages, by using the pacman cache or the Arch Linux Archive. See downgrade(8) for details. https://github.com/pbrisbin/downgrade || downgradeAUR Return from [testing] See Official repositories#Disabling testing repositories.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"pacman","slug":"pacman","permalink":"http://philia897.github.io/tags/pacman/"},{"name":"aur","slug":"aur","permalink":"http://philia897.github.io/tags/aur/"}]},{"title":"Pacman -Syu save memory","slug":"Pacman -Syu save memory","date":"2024-08-12T01:04:07.000Z","updated":"2024-08-12T01:05:19.000Z","comments":true,"path":"2024/08/12/Pacman -Syu save memory/","permalink":"http://philia897.github.io/2024/08/12/Pacman%20-Syu%20save%20memory/","excerpt":"Notice that when using pacman -Syu to scroll to latest, the memory will increases about 15G, most of which are occupied by pacman cache, which can be cleared to save memory resource if it is constrained.","text":"Notice that when using pacman -Syu to scroll to latest, the memory will increases about 15G, most of which are occupied by pacman cache, which can be cleared to save memory resource if it is constrained. Generated by ChatGPT If you find that running pacman -Syu frequently leads to installing a large number of packages, which consumes a significant amount of disk space and memory, there are a few strategies you can use to mitigate this issue in Arch Linux: 1. Optimize Mirrorlist Ensure that your mirrorlist is optimized to use the fastest and most up-to-date mirrors. You can use reflector to generate a new mirrorlist:12sudo pacman -S reflectorsudo reflector --country &#x27;Your_Country&#x27; --age 6 --sort rate --save /etc/pacman.d/mirrorlist Replace &#39;Your_Country&#39; with your actual country. This can help reduce the amount of data you need to download by connecting to the most up-to-date and fastest mirrors. 2. Enable Package Compression Enable package compression to reduce the size of the packages. Edit /etc/pacman.conf and find the Compression section. You can enable or configure the compression options to save space. 3. Partial Upgrades Although partial upgrades are generally not recommended in Arch, if the updates are too large, you could selectively update packages by running:1sudo pacman -Syu &lt;package1&gt; &lt;package2&gt; ... This allows you to update critical or frequently used packages instead of updating the entire system at once. 4. Use paccache to Clean Cache By default, pacman stores the last three versions of each package in /var/cache/pacman/pkg/. Over time, this cache can grow significantly. You can reduce its size using paccache:12sudo pacman -S pacman-contribsudo paccache -r This will remove all but the most recent three versions of each cached package. You can adjust the number by using the -k option (e.g., -k2 to keep only the last two versions). 5. Remove Unneeded Dependencies Sometimes, installed packages bring in dependencies that are no longer needed after a package is removed. You can clean up these orphaned packages with:1sudo pacman -Rns $(pacman -Qtdq) This removes unused dependencies that are not explicitly installed. 6. Regular Maintenance Regularly clean the system of old packages and dependencies: Clean the package cache:1sudo pacman -Sc or for a more aggressive cleaning:1sudo pacman -Scc Remove unneeded packages with:1sudo pacman -Rns $(pacman -Qdtq) 7. Check for Large Packages Identify large packages that you might not need or that can be replaced with lighter alternatives:1sudo pacman -Qdtq | xargs pacman -Qi | awk &#x27;/^Name/&#123;name=$3&#125;/^Installed Size/&#123;print name, $4 $5&#125;&#x27; | sort -k2 -h 8. Consider Using the NoExtract Option If there are packages with parts that you don’t need (e.g., documentation, optional features), you can use the NoExtract option in /etc/pacman.conf to skip installing certain files. 9. Upgrade Regularly Instead of upgrading sporadically, try to update your system more regularly. This will often result in smaller incremental updates. By implementing these strategies, you can reduce the amount of data you need to download and manage, and keep your Arch Linux system lean and efficient.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"pacman","slug":"pacman","permalink":"http://philia897.github.io/tags/pacman/"}]},{"title":"Customize ydotool setting","slug":"Customize ydotool setting","date":"2024-08-07T08:24:15.000Z","updated":"2024-08-07T08:27:20.000Z","comments":true,"path":"2024/08/07/Customize ydotool setting/","permalink":"http://philia897.github.io/2024/08/07/Customize%20ydotool%20setting/","excerpt":"ydotool (1.0.4-2) seems to failed to automatically enable the background service, thus I wrote this tutorial to automatically start the background service ydotoold.service.","text":"ydotool (1.0.4-2) seems to failed to automatically enable the background service, thus I wrote this tutorial to automatically start the background service ydotoold.service. Although there is one indeed useless service config file in /usr/lib/systemd/user/ydotool.service, it can not be called and enabled correctly, and ydotoold can not be executed by normal user without root permission and it should be able to access /dev/uinput. Therefore, I need to modify it and add it as one system service with sudo permission, by modifying the config file and move it to etc/systemd/system/ydotoold.service. (inspired by this blog ) To enhance the security I create one group for it and add the users that need to use it into that group, so that any other users can not use it. Here is the totorial generated by ChatGPT and I followed it. The issue may be related to how the ydotoold service is interpreting the --socket-own and --socket-mode parameters. The ydotoold might not be setting the socket ownership and mode correctly when run as a service. To troubleshoot and resolve this, we can take a few steps: Step 1: Verify Group and UserEnsure the group ydotoolgroup exists and includes the necessary users: Create the group if it doesn’t exist: 1sudo groupadd ydotoolgroup Add users to the group: 1sudo usermod -aG ydotoolgroup username Replace username with the actual username. Step 2: Verify /dev/uinput PermissionsEnsure that the ydotoold service has the necessary permissions to access /dev/uinput. This typically requires root privileges. Step 3: Test the Command with sudoRun the command directly with sudo to see if it works outside the systemd context: 1sudo /usr/bin/ydotoold --socket-path=&quot;/tmp/.ydotool_socket&quot; --socket-own=&quot;root:ydotoolgroup&quot; --socket-mode=&quot;0660&quot; Step 4: Use chown and chmod ManuallyIf the command still doesn’t work as expected, manually set the ownership and permissions of the socket file to ensure they are correct: Run the command without setting ownership and permissions: 1sudo /usr/bin/ydotoold --socket-path=&quot;/tmp/.ydotool_socket&quot; Manually change ownership and permissions of the socket file: 12sudo chown root:ydotoolgroup /tmp/.ydotool_socketsudo chmod 660 /tmp/.ydotool_socket Step 5: Create a Systemd Service FileIf the manual steps work, create or update the systemd service file to reflect the correct setup. Edit the systemd service file: 1sudo nano /etc/systemd/system/ydotoold.service Update the service file content: 12345678910111213141516171819202122[Unit]Description=Starts ydotoold serviceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/bin/ydotoold --socket-path=&quot;/run/ydotoold/socket&quot;ExecStartPost=/bin/sh -c &#x27;chown root:ydotoolgroup /run/ydotoold/socket &amp;&amp; chmod 660 /run/ydotoold/socket&#x27;ExecReload=/usr/bin/kill -HUP $MAINPIDKillMode=processTimeoutSec=180# Set the service to run as rootUser=rootGroup=root# Grant the service access to /dev/uinputDeviceAllow=/dev/uinput rw[Install]WantedBy=multi-user.target Reload systemd: 1sudo systemctl daemon-reload Enable and start the service: 12sudo systemctl enable ydotoold.servicesudo systemctl start ydotoold.service Verify the service: 1sudo systemctl status ydotoold.service Explanation: ExecStartPost: This directive ensures that after starting the ydotoold service, the socket file ownership and permissions are explicitly set using chown and chmod. Running as root: Ensures the service has the necessary permissions to access system devices. By manually setting the ownership and permissions in the ExecStartPost step, you can ensure that the socket file is correctly configured after the service starts. This should resolve the issue with UID and GID not being set properly. Here is the service config I used: 1234567891011121314151617# /etc/systemd/system/ydotoold.service[Unit]Description=Starts ydotoold serviceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/bin/ydotoold --socket-path=&quot;/tmp/.ydotool_socket&quot;ExecStartPost=/bin/sh -c &#x27;chown root:ydotoolgroup /tmp/.ydotool_socket &amp;&amp; chmod 660 /tmp/.ydotool_socket&#x27;ExecReload=/usr/bin/kill -HUP $MAINPIDKillMode=processTimeoutSec=180[Install]WantedBy=default.target","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"ydotool","slug":"ydotool","permalink":"http://philia897.github.io/tags/ydotool/"}]},{"title":"Fix HDMI Monitor Not Showing","slug":"Fix HDMI Monitor Not Showing","date":"2024-08-07T08:18:31.000Z","updated":"2024-12-28T20:16:02.000Z","comments":true,"path":"2024/08/07/Fix HDMI Monitor Not Showing/","permalink":"http://philia897.github.io/2024/08/07/Fix%20HDMI%20Monitor%20Not%20Showing/","excerpt":"My HDMI monitor does not work for KDE, try to fix this bug.","text":"My HDMI monitor does not work for KDE, try to fix this bug. BackgroundWhen using a HDMI cable to connect my computer with the trash monitor I bought in my home, just failed to transmit signal to the monitor and has no output when using xrandr, when I using KDE Plasma (Wayland). After switching to my win 11 (Dual System), I can connect to the monitor normally so it should not be caused by the computer port or cable or any physical reason. Then back to my Arch, thought it might because of my GPU drivers? Since I have two graphic cards in my Laptop, one AMD graphic card and one Nvidia Geforce 4080 Mobile. Then following the tutorials from Arch Wiki Nvidia and installed one very excellant tool called optimus-manager which is developed to manage the use of hybrid graphic cards. However, it did not work and nothing better. optimus-manager does work, integrated is not workable, should set to hybrid, or the external monitor will fail. Then I thought that it might be Wayland? As most of the materials or tutorials I found are on Xorg. xrandr did not perform the same on wayland, not showing the disconnected devices. Then, thanks to KDE Plasma has both Wayland and X11(Xorg) version, I can switch to the X11 version from SDDM login page (The windows manager). However, still not work, but when running xrandr I can see that xrandr can indeed detect my monitor but just has no signal. Then following this tutorial I can then finally use this command to manually setup the output to my monitor: 1xrandr --output eDP-1 --mode 2560x1600 --pos 0x0 --primary --output HDMI-1-0 --mode 2560x1440 --right-of eDP-1 Where the eDP-1 is the built-in monitor of my laptop and HDMI-1-0 connect to my external monitor. Finally works! Then I can manage them using either xrandr or the Display &amp; Monitor settings of KDE. (KDE Plasma) If the external monitor failed to recover after sleep, which might be caused by Kwin, use this command to restart Kwin session might fix it: 1kwin_wayland --replace &amp; Be careful that it will kill all the applications and need to manually restart the applications. Maybe I should find a more appropriate way? or change to Hyprland to see if there’s any problem about it… My old monitor does not support high refresh rate, only 60Hz works.","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"wayland","slug":"wayland","permalink":"http://philia897.github.io/tags/wayland/"},{"name":"x11","slug":"x11","permalink":"http://philia897.github.io/tags/x11/"},{"name":"debug","slug":"debug","permalink":"http://philia897.github.io/tags/debug/"},{"name":"kde","slug":"kde","permalink":"http://philia897.github.io/tags/kde/"}]},{"title":"Arch Linux安装步骤","slug":"安装步骤","date":"2024-07-06T21:59:23.000Z","updated":"2024-08-13T01:38:56.000Z","comments":true,"path":"2024/07/07/安装步骤/","permalink":"http://philia897.github.io/2024/07/07/%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4/","excerpt":"只是简单的记录一下安装的步骤。其实还挺简单的。","text":"只是简单的记录一下安装的步骤。其实还挺简单的。 安装背景新买的机械革命的翼龙15Pro电脑，我确实是受够windows了，想试试别的操作系统。Ubuntu其实也不错，比较成熟，但咱已经用过挺多次了，这次想试试新系统，遇到问题了也是学习的过程嘛。 配置是32G内存+AMD的CPU+Nvidia 4060显卡 + 2T存储2T都用的是长江存储，因为是在拼多多上买的特供版，比标准版加了一块1T的P300存储。新加的硬盘没有原装的性能好，不过也够用了倒是。稍微有点浪费，毕竟咱计划是以Arch作为主力系统使用的。不过也问题不大，数据很多还是存在Win系统盘里，反正Arch也能挂载，就还可以。 安装步骤硬盘准备先把硬盘进行一个清空，用diskgenius进行格式化，确定硬盘变成未使用状态就行 制作启动盘用Rufus和Arch的镜像制作启动U盘，随便找个什么U盘，够大就行。 本来是准备用ventoy做启动盘的，这样可以不用浪费我一个U盘，结果用不了，郁闷。 安装插上U盘之后进入BIOS，从U盘启动放到前边，然后disable掉安全模式，然后启动，就能正常进入Arch的安装界面了，一个经典的CLI 这里就可以按照各种教程来进行正常的安装了，也可以图简单运行archinstall进行prompt安装。 建议选择btrfs文件系统，更新一些并且对分卷和备份很友好安装的时候建议不安装neofetch了，直接安装fastfetch，因为neofetch停止维护了嘛安装的时候选择安装库git fastfetch htop桌面系统可以选择hyprland和KDE两个都装，也可以先只装KDE，hyprland之后手动装也无所谓。总体上没什么坑，各种库可以晚点再自己装也不迟 Grub安装完成之后如果没啥问题就可以了，因为这次是装双系统，然后两个系统安装在不同的硬盘里，而Arch是用Grub进行引导的，所以手动修改一下grub配置，然后重新grub-mkconfig一样就行，把windows的盘的启动选项加进grub.cfg里边就行了，这块直接问GPT就可以。总体上思路很清晰 至于Grub主题配置之类的也直接查教程就行。需要注意的是自己写custom配置的时候，/etc/grub.d/文件夹下的custom选项前边的数字决定了启动项的顺序，可以通过这个调整顺序。也可以安装grub-customizer进行配置，但那个是通过proxy进行配置的，原理不太一样，不过都可以。","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"archinstall","slug":"archinstall","permalink":"http://philia897.github.io/tags/archinstall/"},{"name":"grub","slug":"grub","permalink":"http://philia897.github.io/tags/grub/"}]},{"title":"机械革命 蛟龙15K Linux 内置键盘失灵解决(ACPI)_linux 笔记本键盘失灵-CSDN博客","slug":"机械革命 蛟龙15K Linux 内置键盘失灵解决(ACPI)_linux 笔记本键盘失灵-CSDN","date":"2024-07-04T08:28:39.000Z","updated":"2025-03-03T12:09:51.520Z","comments":true,"path":"2024/07/04/机械革命 蛟龙15K Linux 内置键盘失灵解决(ACPI)_linux 笔记本键盘失灵-CSDN/","permalink":"http://philia897.github.io/2024/07/04/%E6%9C%BA%E6%A2%B0%E9%9D%A9%E5%91%BD%20%E8%9B%9F%E9%BE%9915K%20Linux%20%E5%86%85%E7%BD%AE%E9%94%AE%E7%9B%98%E5%A4%B1%E7%81%B5%E8%A7%A3%E5%86%B3(ACPI)_linux%20%E7%AC%94%E8%AE%B0%E6%9C%AC%E9%94%AE%E7%9B%98%E5%A4%B1%E7%81%B5-CSDN/","excerpt":"机械革命2024款一些笔记本安装Linux后内置键盘失灵但外接键盘正常工作，修复主要通过修改内核解决","text":"机械革命2024款一些笔记本安装Linux后内置键盘失灵但外接键盘正常工作，修复主要通过修改内核解决 https://blog.csdn.net/ZGY_121/article/details/134834265 12345678910111213cat /sys/firmware/acpi/tables/DSDT &gt; dsdt.datiasl -d dsdt.dat# 将PS2K后出现的第一个ActiveLow改为ActiveHighsed -i &#x27;/PS2K/,/ActiveLow/ s/ActiveLow/ActiveHigh/1&#x27; dsdt.dsl # 将DefinitionBlock版本号加一perl -pe &#x27;if (/DefinitionBlock/) &#123; s/(0x[0-9a-fA-F]+)/sprintf(&quot;0x%X&quot;,hex($1)+1)/e &#125;&#x27; dsdt.dsl &gt; dsdt.dsl iasl dsdt.dslmkdir -p kernel/firmware/acpicp dsdt.aml kernel/firmware/acpifind kernel | cpio -H newc --create &gt; acpi_overridesudo cp acpi_override /bootecho &quot;GRUB_EARLY_INITRD_LINUX_CUSTOM=\\&quot;acpi_override\\&quot;&quot; &gt;&gt; /etc/default/grubsudo grub-mkconfig -o /boot/grub/grub.cfg &nbsp; 原因机革笔记本用的主板DSDT 有错误：键盘是下降沿触发，bios里dsdt写成了上升沿触发。 蛟龙16K等一些主板的修复代码已经在6.5以后加入内核。 旧版内核默认以下降沿触发处理，所以没有问题。新内核默认行为改变，键盘无法使用。 下面是两个解决办法，都要使用外接键盘。 修改内核解决1. 查看主板型号1sudo cat /sys/devices/virtual/dmi/id/board_name 2. 修改内核从 6.5 内核起，内核已加入特殊的主板判断代码来强制覆盖acpi行为，使其变为下降沿触发。相关代码的commit: 2d331a6ac在内核源码树中的 drivers/acpi/resource.c 内dmi_system_id irq1_edge_low_force_override变量中。你可以查看一下有无和你主板型号匹配的项，没有就加上，重新编译内核。 具体编译方法虽大致相同，但各个发行版可能会有一些差别，另外我也没用这个方法，就不多赘述。 修改DSDT 并在启动时覆盖根本性的解决方法就是修改bios中的DSDT表了，即官方更新bios修复这个错误。既然官方还没修复，就需要在启动时强制加载新的正确的DSDT。 安装工具包主要是两个 一个是DSDT的编译解码软件 acpica 和cpio 获得原来的DSDT12cat /sys/firmware/acpi/tables/DSDT &gt; dsdt.dat # 获取dsdtiasl -d dsdt.dat # 反编译DSDT成AML文件(.dsl) 修改 dsdt.dsl 找到Device(PS2K) 向下找第一个ActiveLow 改成ActiveHigh 1234567891011121314151617181920212223242526Device (PS2K)&#123; Name (_HID, &quot;MSFT0001&quot;) // _HID: Hardware ID Name (_CID, EisaId (&quot;PNP0303&quot;) /* IBM Enhanced Keyboard (101/102-key, PS/2 Mouse) */) // _CID: Compatible ID Method (_STA, 0, NotSerialized) // _STA: Status &#123; Return (0x0F) &#125; Name (_CRS, ResourceTemplate () // _CRS: Current Resource Settings &#123; IO (Decode16, 0x0060, // Range Minimum 0x0060, // Range Maximum 0x00, // Alignment 0x01, // Length ) IO (Decode16, 0x0064, // Range Minimum 0x0064, // Range Maximum 0x00, // Alignment 0x01, // Length ) IRQ (Edge, ActiveLow, Shared, ) &lt;&lt;=== 目标 &#123;1&#125; &#125;) 给DefinitionBlock 升一个版本(最后一个十六进制数加一) 123DefinitionBlock (&quot;&quot;, &quot;DSDT&quot;, 2, &quot;ALASKA&quot;, &quot;A M I &quot;, 0x01072009)改成DefinitionBlock (&quot;&quot;, &quot;DSDT&quot;, 2, &quot;ALASKA&quot;, &quot;A M I &quot;, 0x0107200A) 编译新的AML文件1iasl dsdt.dsl 最后显示Compilation successful. 0 Errors就算没有问题 使用修改过的DSDT两个方案： 把新dsdt编译进内核 创建一个可以被bootloader加载的CPIO文件 这里使用第二个方法，这两个方法在archwiki里都有详述。 123456mkdir -p kernel/firmware/acpicp dsdt.aml kernel/firmware/acpifind kernel | cpio -H newc --create &gt; acpi_overridesudo cp acpi_override /bootecho &quot;GRUB_EARLY_INITRD_LINUX_CUSTOM=\\&quot;acpi_override\\&quot;&quot; &gt;&gt; /etc/default/grubsudo grub-mkconfig -o /boot/grub/grub.cfg 重启看一看修改有没有成功吧 参考 贴吧-蛟龙16K在高版本linux下内置键盘失灵问题 Arch Wiki - DSDT Bilibili-蛟龙16k在高版本linux中键盘失灵解决方案 &nbsp; &nbsp; &nbsp;","categories":[{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"}],"tags":[{"name":"bios","slug":"bios","permalink":"http://philia897.github.io/tags/bios/"},{"name":"kernel","slug":"kernel","permalink":"http://philia897.github.io/tags/kernel/"}]},{"title":"云计算知识","slug":"云计算知识","date":"2023-10-18T16:00:00.000Z","updated":"2025-02-25T07:05:44.268Z","comments":true,"path":"2023/10/19/云计算知识/","permalink":"http://philia897.github.io/2023/10/19/%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9F%A5%E8%AF%86/","excerpt":"云计算的课程笔记：云计算知识（虚拟化、容器、计算虚拟化、存储虚拟化、网络虚拟化）","text":"云计算的课程笔记：云计算知识（虚拟化、容器、计算虚拟化、存储虚拟化、网络虚拟化） 云计算服务类型 云计算部署形式私有云：自用， 优点：安全性高，灵活 缺点：成本高，前期建设高 社区云：社区或行业共享基础设施的云，专供给有相同需求的单位 优点：用户体系，方便 缺点：成本高，专门维护 公有云：public cloud service： 优点：个人用户成本低服务多 缺点：安全性不高 混合云：两种或两种以上部署形式组成的云 优点：敏捷，灵活 缺点：兼容性问题 应用：存储云，医疗云，教育云，金融云 云计算关键技术虚拟化 分布式存储技术 数据中心联网 并行计算 自动化部署 虚拟化CPU虚拟化 物理机中通过线程和进程模拟出CPU，虚拟CPU实际上就是一个线程或进程 计算公式：总资源=服务器CPU数×单个CPU的核数×线程×主频 CPU QoS：控制虚拟机使用CPU资源量 NUMA: 内存虚拟化 虚拟机每个进程都被赋予了一块连续的，超大的虚拟内存空间 内存虚拟化类型： 内存复用技术 内存复用QoS 内存预留，内存份额 I/O虚拟化 I/O环适配功能","categories":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"}],"tags":[{"name":"cloud","slug":"cloud","permalink":"http://philia897.github.io/tags/cloud/"}]},{"title":"64位win7使用conda安装xgboost","slug":"64位win7使用conda安装xgboost","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:03:08.323Z","comments":true,"path":"2023/08/08/64位win7使用conda安装xgboost/","permalink":"http://philia897.github.io/2023/08/08/64%E4%BD%8Dwin7%E4%BD%BF%E7%94%A8conda%E5%AE%89%E8%A3%85xgboost/","excerpt":"xgboost这个库在windows上装有点点麻烦，不能简单使用conda install xgboost安装，故记录使用别人做好的whl安装的过程","text":"xgboost这个库在windows上装有点点麻烦，不能简单使用conda install xgboost安装，故记录使用别人做好的whl安装的过程 在https://www.lfd.uci.edu/~gohlke/pythonlibs/网站找到xgboost，并下载对应版本的轮子（若版本不对，会报错，重新下载对应版本即可），我在此下载的是win amd64 py37版本 打开anaconda prompt，cd到下载的whl的目录，执行pip install (whl.path),即可 执行conda list xgboost测试是否成功安装 参考博客：https://blog.csdn.net/zz860890410/article/details/78682041https://blog.csdn.net/big_kevin/article/details/81906607","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"大创","slug":"大创","permalink":"http://philia897.github.io/tags/%E5%A4%A7%E5%88%9B/"},{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"xgboost","slug":"xgboost","permalink":"http://philia897.github.io/tags/xgboost/"}]},{"title":"Descent-to-Delete Gradient-Based Methods for Machine Unlearning","slug":"Descent-to-Delete Gradient-Based Methods for Machine Unlearning","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:03:25.356Z","comments":true,"path":"2023/08/08/Descent-to-Delete Gradient-Based Methods for Machine Unlearning/","permalink":"http://philia897.github.io/2023/08/08/Descent-to-Delete%20Gradient-Based%20Methods%20for%20Machine%20Unlearning/","excerpt":"​ 这篇论文利用，，，，技术实现了动态的模型更新，支持addition和deletion，并且辨别weak和strong的unlearning algorithm","text":"​ 这篇论文利用，，，，技术实现了动态的模型更新，支持addition和deletion，并且辨别weak和strong的unlearning algorithm","categories":[{"name":"paper notes","slug":"paper-notes","permalink":"http://philia897.github.io/categories/paper-notes/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"paper","slug":"paper","permalink":"http://philia897.github.io/tags/paper/"},{"name":"unlearning","slug":"unlearning","permalink":"http://philia897.github.io/tags/unlearning/"}]},{"title":"Federated Learning for DDos 比赛 Log","slug":"FL_DDos_比赛Log","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:03:34.066Z","comments":true,"path":"2023/08/08/FL_DDos_比赛Log/","permalink":"http://philia897.github.io/2023/08/08/FL_DDos_%E6%AF%94%E8%B5%9BLog/","excerpt":"记录比赛实践期间的一些所学和感受，简记学到的知识和理解","text":"记录比赛实践期间的一些所学和感受，简记学到的知识和理解 2020&#x2F;10&#x2F;28 配置仓库 数据基础处理 2020&#x2F;11&#x2F;8前情回顾： 加入预处理之后直接起飞，准确率直逼100%，主要矛盾转变为过拟合问题 减小模型层数，使用4层感知机，加入dropout层，效果有所提升 调参，，，略吧，，无聊 聚合算法中加入权重：标签丰富度，根据标签丰富度来进行梯度聚合，效果有所提升 加入学习率周期性衰减，效果有所提升 目前最好测试acc为62.7% 之后的测试在改进的基础上进行 测试： layer_num &#x3D; 4lr&#x3D;0.001weight_decay &#x3D; 0.01batchsize&#x3D;1600n_round_sample&#x3D;16000max_round&#x3D;120acc_limit &#x3D; 95 result: 50轮早停，train：55%，test：57%，未过拟合，效果不错的 75轮早停，train：64%，test：60%，误差范围内吧，还算好 100轮早停，train：68%，test：58%，过拟合，有点严重 实验效果不理想呀，怀疑是学习率的问题？我觉得应该不是模型结构的问题，train在上升，但test并没有提高，只能说明这时候产生了过拟合，test的学习率就没有上去过 测试： layer_num &#x3D; 4lr&#x3D;0.001weight_decay &#x3D; 0.1batchsize&#x3D;1600n_round_sample&#x3D;16000max_round&#x3D;100acc_limit &#x3D; 95 85轮早停，train：73%，test： 56.68%，过拟合比较严重，原因不明 测试： layer_num &#x3D; 4lr&#x3D;0.003weight_decay &#x3D; 0.1batchsize&#x3D;1600n_round_sample&#x3D;16000max_round&#x3D;100acc_limit &#x3D; 95 太惨烈，75轮停止，train：67%，test：47%，过拟合严重，看来调学习率似乎也没办法解决？ 测试： layer_num &#x3D; 4lr&#x3D;0.001weight_decay &#x3D; 0.0005batchsize&#x3D;1600n_round_sample&#x3D;16000max_round&#x3D;75acc_limit &#x3D; 95 大概是比较默认的值了，不抱希望的看看效果 结果也确实没给我惊喜，train 73%，test 54%，十分意料之中 总结一下，经过这些个几轮测试，我觉得之后重复类似的测试也没啥意义了，问题不在这里，过拟合的问题需要寻求别的出路来解决，不是通过换优化器，加正则化能解决的了的我觉得。可能需要有其他的创新性的设计才行。联邦平均算法无疑是一个很优秀的算法。至少在训练集上表现相当优异 测试： layer_num &#x3D; 4lr&#x3D;0.01weight_decay &#x3D; 0.01batchsize&#x3D;16000n_round_sample&#x3D;16000max_round&#x3D;75acc_limit &#x3D; 95不加标签 47%acc很差，还是要加标签的 2020&#x2F;11&#x2F;12换回曾经的那个不加weightdecay的版本，62%的那个。 参数如下： layer_num &#x3D; 4lr&#x3D;0.001batchsize&#x3D;4800n_round_sample&#x3D;4800max_round&#x3D;200acc_limit &#x3D; 95 然后构造了交叉验证集，诡异的事情发生了： 训练集最后90%，交叉验证集87%，说明并没有过拟合，可能： 我的交叉验证集还是数据过于集中，看不出过拟合问题 测试集和训练集的数据分布差异较大，比如测试集里全是那些不容易区分的攻击类型 加入focal loss策略试试看： layer_num &#x3D; 4lr&#x3D;0.003batchsize&#x3D;4800n_round_sample&#x3D;4800max_round&#x3D;200acc_limit &#x3D; 95focal loss &#x3D; true 最后差别不大，训练集90%，验证集89%","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"competition","slug":"competition","permalink":"http://philia897.github.io/tags/competition/"},{"name":"Federated Learning","slug":"Federated-Learning","permalink":"http://philia897.github.io/tags/Federated-Learning/"},{"name":"pytorch","slug":"pytorch","permalink":"http://philia897.github.io/tags/pytorch/"}]},{"title":"Hexo安装配置","slug":"Hexo安装配置","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:04:01.314Z","comments":true,"path":"2023/08/08/Hexo安装配置/","permalink":"http://philia897.github.io/2023/08/08/Hexo%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"这应该是我写的第一个博客啦，Hexo没有想象中那么难配置呢，还挺好玩的，选主题的时候选了好久，又不想太花哨，又不想太单调，最后还是选择了简洁的pure主题，看着不错呢~","text":"这应该是我写的第一个博客啦，Hexo没有想象中那么难配置呢，还挺好玩的，选主题的时候选了好久，又不想太花哨，又不想太单调，最后还是选择了简洁的pure主题，看着不错呢~ 安装Hexo遇到的坑：一、安装Node.js版本过低导致npm安装Hexo失败解决办法：查看报错信息，需要大于4.0版本的Node.js安装，所以到官网下载新版Node安装，之后成功 二、安装速度过慢解决办法： 安装淘宝镜像源： npm install -g cnpm --registry=https://registry.npm.taobao.org cnpm代替npm 三、安装diployer失败描述： npm install hexo-deployer-git --save 解决办法： 切换到blog目录下安装，不要在hexo软件目录下安装即可 四、安装pure主题遇到的坑1、categories，tags等等无法显示原因：blog目录下的source里没有这些page的记录 解决方案 将pure&#x2F;_source目录下的文件夹复制粘贴到blog的source文件夹内 重新hexo g 解决 参考链接如何搭建个人博客（详细图解）_振的博客-CSDN博客_个人博客搭建 (1条消息)Hexo+github pages搭建个人博客（这一篇博客就够了）_ZhuangXuward-CSDN博客 【持续更新】最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析 | 遇见西门","categories":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://philia897.github.io/tags/Hexo/"}]},{"title":"Ternux玩耍记录","slug":"Ternux玩耍记录","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:05:05.097Z","comments":true,"path":"2023/08/08/Ternux玩耍记录/","permalink":"http://philia897.github.io/2023/08/08/Ternux%E7%8E%A9%E8%80%8D%E8%AE%B0%E5%BD%95/","excerpt":"​ 今天尝试了下在手机上安装Ternux终端模拟器，总体上还行，比较好玩，记录一下使用的一些体验","text":"​ 今天尝试了下在手机上安装Ternux终端模拟器，总体上还行，比较好玩，记录一下使用的一些体验 1. 安装和配置其实没咋安装和配置，就是按照CSDN上的，改了镜像源，不过新版的好像不是自带vim了，得先pkg install vim一下，安装vim之后再修改配置文件。然后就是取得storage权限就行了： 1234567pkg install vimexport EDITOR=viapt edit-sources# 在文件中加入：deb http://mirrors.tuna.tsinghua.edu.cn/termux stable main，注释掉原本的就行termux-setup-storage # 取得存储权限，之后直接在home目录下出现storagecd storagels -l # 试试看都是哪些文件吧~ 2. 一些问题就是安装的所有软件全部在内存里，不能安到SD卡里，导致我存储不够不太能玩&#x2F;笑哭 因为他直接装在data根下，可以通过ZAchieve软件一直回溯到/，可以发现有一个data，并且是不能被访问的，所以木有办法啦~","categories":[{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/categories/Linux/"}],"tags":[{"name":"Ternux","slug":"Ternux","permalink":"http://philia897.github.io/tags/Ternux/"},{"name":"Android","slug":"Android","permalink":"http://philia897.github.io/tags/Android/"},{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/tags/Linux/"}]},{"title":"PVE+NAS等虚拟环境安装指南","slug":"PVE+NAS等虚拟环境安装记录","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:04:38.148Z","comments":true,"path":"2023/08/08/PVE+NAS等虚拟环境安装记录/","permalink":"http://philia897.github.io/2023/08/08/PVE+NAS%E7%AD%89%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/","excerpt":"​ 第二次安装PVE加上面的一些群晖、Windows等系统，新买了一张固态。第一次安装没写博客记录，第二次安装记录一下安装过程以及踩的坑。 ​ 教程参考视频：https://www.bilibili.com/video/BV1Br4y1N7VK","text":"​ 第二次安装PVE加上面的一些群晖、Windows等系统，新买了一张固态。第一次安装没写博客记录，第二次安装记录一下安装过程以及踩的坑。 ​ 教程参考视频：https://www.bilibili.com/video/BV1Br4y1N7VK 本期PVE所有安装资料网盘打包：https://pan.baidu.com/s/1crbYS5-EMAcJIOm6sQyFNA 密码: 4qf0 涉及的链接：PVE官网UltraISO写盘工具DG分区助手MSDN我告诉你 1 准备工作和接线1.1 在进行正式安装之前，需要准备： 一台组装好的主机 显示器一个 键盘一个 路由器一个（也可以使用网线和电脑直连，不过最后横竖是需要连路由器的，不如从最开始就连好好吧实际上是笔记本没网口也没有转接头，没办法和主机直连，只能通过路由器来了，，，） 网线两根（如果直连就只需要一根） 可上网笔记本一台 U盘一个 1.2 安装主机： 路由器上电，WAN口接拨号线，LAN1口和主机相连 将固态插入主机，显示器、键盘和主机相连，主机上电 1.3 配置上网：就配置下密码啊之类的，最后使用笔记本连接路由器，能连上并成功上网就行。然后在主机上ipconfig一下，看看网络信息如下： 2 PVE的安装2.1 制作启动盘先在PVE官网下载对应的iso镜像，然后用UltraISO写进去就行了： 插入U盘，格式化 打开UltraISO，文件-&gt;打开-&gt;找到PVE的ISO镜像 启动-&gt;写入硬盘映像，默认写入就行了 这里遇到的问题1：U盘写有保护，就很烦。WIndows无法格式化，UltraISO也无法写入。最后的解决方案是： 使用DiskGenius软件清除U盘的全部分布，保存修改，注意不要再新建分区了，就让U盘保持空闲状态，然后重新写入。 2.2 安装PVE系统插入启动盘到主机，然后就自动进入了Proxmox VE的安装界面，就一步步走就可以了，其中网络这块，上边我们看到路由器的默认网关是192.168.31.1，所以我们设置我们PVE的ip为固定ip：192.168.31.31，这样之后我们就用连接在路由器上的笔记本，访问https://192.168.31.31:8006/就可以进入到PVE的控制台了。控制台如图： 2.3 进行简单的配置：网络桥接因为我们的主机只有一个网口，所以默认已经配置好了，如果有多个网口，可以点击创建，并新建类似的桥接配置，大致如图： 3 安装群晖NAS系统3.1 创建虚拟环境由于我们的主机的配置是：cpu4核8线程，16GB内存，512G固态，安装完PVE之后还有三百来G固态空闲。所以可以奢侈一点配置 如图进行相应的配置（不需要太谨慎，之后还可以在这个hardware和option里边更改配置）： 3.2 配置黑群晖启动之后要进行黑群晖的引导安装： 先将最开始创建的硬盘Hard Disk删除，detach，然后remove，（上图是配置好的，不要看Hard disk部分） 随便使用什么SSH连接工具（mobaXterm等）连接到PVE，然后上传资料包中的img2kvm，NAS引导+固件/3617xs/synoboot.img两个文件到PVE 输入指令： 12chmod +x img2kvm./img2kvm synoboot.img 100 vm-100-disk-1 # 这里的100就是创建的虚拟机的id，在pve的下拉列表中可以看到 进入的PVE浏览器后台，可以看到一个未使用的磁盘，选中，添加，然后将总线设备更改为SATA，其他默认 返回，找到选项 options，将引导顺序（boot order）双击进入，将我们新添加的这块调整到第一启动项并设置启用。 之后我们再给NAS添加一块硬盘： 再返回到Hardware，选择add-&gt;Hard disk，选择Storage并设置大小就好 返回控制台，启动 3.3 连接到群晖连接这里其实不太好连接，有几种方法： 如果NAS可以上网的话，可以使用在线方法查询（但反正我没成功过）： 下载群晖助手，安装好之后启动就会默认检测当前局域网内的群晖设备（我猜是检测局域网内的），一般来说就只能检测到一个设备，也就是我们刚装好的设备 如果可以登录到路由器后台的话，可以通过查看ip地址，看看哪个是刚刚加进去的，就是我们的群晖的ip 总之就是为了得到群晖设备的ip地址，得到ip之后在浏览器上输入ip 端口5000（其实不输入端口好像也行），就可以进入群晖的后台了。如果使用群晖助手，双击也可以。 3.3 配置黑群晖进入控制台之后，应该是一个如图的配置界面： 然后选择设置，选择手动安装，选择浏览，找到NAS引导+固件/3617xs/DSM....pat，之后启动安装 安装完成之后进行管理员配置即可： 之后简单配置一下，群晖账号那里跳过就行了，之后就进入到主界面中： 可以打开Package Center，如果NAS能上网的话，各种套件已经可以访问到了。 3.4 添加存储 找到菜单中的Storage Manager，进入 在Storage Pool中create，配置默认就行，Raid那里可以设置成Basic，之后完成 在Volume（存储空间）中create，选择刚新建的存储池，然后默认配置就好，最后完成 之后回到NAS主界面，找到控制面板，找到共享文件夹，新增，然后自己配置一下就行了，最后别忘了设置读写权限。 最后我们就可以用我们本机windows访问共享文件夹了，在文件浏览框中输入\\\\192.168.31.xxx/，输入用户名和密码就可以访问了共享文件夹了！ 3.5 添加硬盘到PVE 参考博客：https://wangxingcs.com/2020/0910/1442/ https://blog.csdn.net/xiaobo060/article/details/105928108/ 懒得写这个了。。。。。网上教程比较多，随便来就行，，，思路就是先把硬盘挂载到PVE，说白了还是Linux上，然后写/etc/fstab文件实现自动挂载。然后再到PVE的浏览器控制台data center-&gt; storage -&gt; add 就行了 要注意的一点是，对于大容量硬盘来说，fdisk命令不能分超过2T空间的区，所以这时候需要使用parted工具来进行分区，然后格式化的时候要注意一下inode的数量不要过多，不然很浪费空间，具体可以看博客 大容量硬盘分区和格式化参考博客：https://blog.csdn.net/Richardlygo/article/details/87867042 4 安装windows 10系统 参考： https://www.bilibili.com/video/BV1Wh411k7g5?from=search&amp;seid=16913643997894754399 这篇配置的windows系统使用了半直通配置，稍微复杂一点，但性能更好一些。 4.1 准备镜像准备windows 10 的ISO镜像文件（msdn，官网，或者随便哪里都行，只要能下到windows10的镜像文件） 还需要下载virtIO驱动 之后到PVE的浏览器控制台，pve节点下的local(pve)这里，上传ISO Images，上传到PVE中： 4.2 创建虚拟器还是一样的配置，create VM，然后配置，这里有几个地方需要改动： OS这里选择使用镜像安装，系统选择windows 版本选10。 然后在system里边勾选上Qemu Agent（用于远程桌面的） 硬盘那里推荐用SCSI 网络那里的模型选用VirtIO半连通 这样就创建完成了 之后新增一个光驱，IDE就好，然后添加virtIO驱动的镜像，检查一下boot order是否勾选上了win10的镜像，然后启动虚拟机 4.3 win10安装流程一路进去，自定义安装，然后到了你想将windows安装在哪里，这里没有检测出任何一个磁盘： 点击加载驱动程序 然后浏览，选择virtIO的CD driver，进入找vioscsi/win10/amd64，确定，下一步，应该就可以成功识别出驱动器了。然后新建分区，大概长这样： 之后按步骤一步一步安装就行了，最后启动windows进入到桌面。 4.4 配置驱动打开设备管理器，可以看到： 然后选择打感叹号的那三个，先配网：更新驱动程序-&gt;浏览电脑查找驱动-&gt;选择virtIO driver进入，找到NetKVM/w10/amd64，安装驱动就好。 之后两个PCI设备也是一样，在浏览查找驱动的时候可以直接选择virtIO driver，也就是D: 这个目录，然后直接安装，它就会查找当前目录下可用的驱动程序。（适用于不知道驱动是哪个的情况） 4.5 配置远程桌面进入文件管理器，到virtIO driver中，找到guest-agent，安装x86-64版 之后关机重启，就可以在PVE管理后台中win10节点的summary里边看到IP地址了 然后可以在虚拟机中开启远程桌面服务，设置可访问的用户及权限，就可以使用其他windows系统进行远程桌面访问了。 5 内网穿透配置 参考： https://blog.csdn.net/weixin_33393017/article/details/112882580 实际测试效果：ZeroTier的速度太缓慢了，用来做SSH连接传一些命令还可以，但移动桌面什么的还是想都别想了 内网穿透使用的是ZeroTier平台，原理大概就是借用这个平台搭建虚拟局域网。ZeroTier的使用就从略了，直接注册，创建网络，然后再使用客户端连接就好。windows客户端很简单，下载安装，然后join Network就行了。 需要注意的是： 每新添加一个设备到局域网，都需要访问ZeroTier的控制台将此设备enable，这样才能正常连接，算是一个保险措施吧。 安装ZeroTier之后，使用ipconfig 或者 ifconfig 等，可以查看到有个虚拟网卡，并配置有ip： 这样就算是加入到局域网中了，在同一个局域网中的设备是互相可见的。 主要是在NAS中配置出现的问题比较多： Docker端的Zerotier好像无法正常使用，按照网上的教程没有成功 使用ZeroTier官网给的下载链接下载的时候要下载对应的版本的，需要先在群晖中开通SSH连接，然后使用SSH连接到群晖，用命令uname -ar，可以查看到最后的版本信息： 这就代表此DSM是Bromolow版本的，就需要下载对于版本的套件（有NAS版块，Synology就是） 经过测试，只有1.4.0版本的套件可以用，1.4.1版本的套件会遇到缺少文件的bug 下载完毕后，进入群辉“套件中心”，点击手动安装，然后选择我们刚才下载的*.spk文件完成安装。 安装完成之后可以open，但一般会遇到没有内容，join点了没反应的问题。解决方案： 使用SSH连接到群晖的Shell sudo zerotier-cli join ID，ID就是申请到的网络的id号，正常返回结果是200 join OK 此时再打开群晖的套件，就显示正常了。 返回到Zerotier的官网的管理界面，授予新加入的设备权限即可： 也可以使用sudo -i 进行提权 注意，如果中途有报错说tun找不到的话（我就遇到了。。。），需要手动安装tun模块，这个模块在内核代码中有，所以只需要对应的操作就好了，详细百度。 6 PVE使用优化6.1 增加CPU温度显示6.1.1 安装软件 使用SSH连接到PVE的命令行（也可以在PVE的后台的shell中操作），下载插件sensors：apt install lm-sensors 安装完成之后使用sensors-detect进行相应的配置操作，一路yes应该就可以了。 之后输入命令sensors，就可以看到硬件的温度信息 6.1.2 配置文件首先先备份之后要更改的两个文件： 12cp /usr/share/perl5/PVE/API2/Nodes.pm /usr/share/perl5/PVE/API2/Nodes.pm.bakcp /usr/share/pve-manager/js/pvemanagerlib.js /usr/share/pve-manager/js/pvemanagerlib.js.bak 编辑第一个文件： vi /usr/share/perl5/PVE/API2/Nodes.pm 增加一行代码： 12345678910111213141516171819202122232425$res-&gt;&#123;ksm&#125; = &#123; # 可以使用这句话定位到此位置 vi: /res-&gt;&#123;ksm&#125; shared =&gt; $meminfo-&gt;&#123;memshared&#125;,&#125;;$res-&gt;&#123;swap&#125; = &#123; free =&gt; $meminfo-&gt;&#123;swapfree&#125;, total =&gt; $meminfo-&gt;&#123;swaptotal&#125;, used =&gt; $meminfo-&gt;&#123;swapused&#125;,&#125;;$res-&gt;&#123;pveversion&#125; = PVE::pvecfg::package() . &quot;/&quot; . PVE::pvecfg::version_text();$res-&gt;&#123;temperature&#125; = `sensors`; # 此处增加这行代码，获取 CPU 和 主板 的温度my $dinfo = df(&#x27;/&#x27;, 1); # output is bytes$res-&gt;&#123;rootfs&#125; = &#123; total =&gt; $dinfo-&gt;&#123;blocks&#125;, avail =&gt; $dinfo-&gt;&#123;bavail&#125;, used =&gt; $dinfo-&gt;&#123;used&#125;, free =&gt; $dinfo-&gt;&#123;blocks&#125; - $dinfo-&gt;&#123;used&#125;,&#125;;return $res; 编辑第二个文件： vi /usr/share/pve-manager/js/pvemanagerlib.js 增加一段代码： 123456789101112131415161718192021222324252627282930313233343536... &#123; itemId: &#x27;version&#x27;, colspan: 2, printBar: false, title: gettext(&#x27;PVE Manager Version&#x27;), # 可以此段代码搜索到指定位置 textField: &#x27;pveversion&#x27;, value: &#x27;&#x27; &#125;, &#123; itemId: &#x27;temperature&#x27;, colspan: 2, printBar: false, title: gettext(&#x27;CPU Temperature&#x27;), textField: &#x27;temperature&#x27;, renderer: function(value)&#123; const c0 = value.match(/Core 0.*?\\+([\\d\\.]+)?/)[1]; const c1 = value.match(/Core 1.*?\\+([\\d\\.]+)?/)[1]; const c2 = value.match(/Core 2.*?\\+([\\d\\.]+)?/)[1]; const c3 = value.match(/Core 3.*?\\+([\\d\\.]+)?/)[1]; return `Core0: $&#123;c0&#125;℃ | Core1: $&#123;c1&#125;℃ | Core2: $&#123;c2&#125;℃ | Core3: $&#123;c3&#125;℃` &#125;&#125; ], updateTitle: function() &#123; var me = this; var uptime = Proxmox.Utils.render_uptime(me.getRecordValue(&#x27;uptime&#x27;)); me.setTitle(me.pveSelNode.data.node + &#x27; (&#x27; + gettext(&#x27;Uptime&#x27;) + &#x27;: &#x27; + uptime + &#x27;)&#x27;); &#125;&#125;);... 之后进行重启控制台即可systemctl restart pveproxy 如果没有刷新，则清除缓存之后重试，或者ctrl+F5强制刷新 6.2 换用高速源安装升级一些linux必要工具先备份原本的配置文件：mv /etc/apt/sources.list /etc/apt/sources.list.bak 之后创建一份新的配置文件：vi /etc/apt/sources.list 添加如下内容： 12345678deb https://mirrors.aliyun.com/debian/ buster main non-free contribdeb https://mirrors.aliyun.com/debian/ buster-updates main non-free contribdeb https://mirrors.aliyun.com/debian/ buster-backports main non-free contribdeb-src https://mirrors.aliyun.com/debian/ buster main non-free contribdeb-src https://mirrors.aliyun.com/debian/ buster-updates main non-free contribdeb-src https://mirrors.aliyun.com/debian/ buster-backports main non-free contribdeb https://mirrors.aliyun.com/debian-security/ buster/updates main non-free contribdeb-src https://mirrors.aliyun.com/debian-security/ buster/updates main non-free contrib 之后执行apt update更新 PVE默认安装的nano，反正我是不太会用，但安装的vim版本太低，所以我们安装一些包： 123apt install vim # 安装vimapt install net-tools # 使用ifconfigapt install iputils-ping # 使用ping 参考：https://www.cnblogs.com/surplus/p/14391801.html","categories":[{"name":"Network","slug":"Network","permalink":"http://philia897.github.io/categories/Network/"}],"tags":[{"name":"PVE","slug":"PVE","permalink":"http://philia897.github.io/tags/PVE/"},{"name":"NAS","slug":"NAS","permalink":"http://philia897.github.io/tags/NAS/"},{"name":"Network","slug":"Network","permalink":"http://philia897.github.io/tags/Network/"}]},{"title":"VScode终端中文乱码解决","slug":"VScode终端中文乱码解决","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:05:25.406Z","comments":true,"path":"2023/08/08/VScode终端中文乱码解决/","permalink":"http://philia897.github.io/2023/08/08/VScode%E7%BB%88%E7%AB%AF%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3/","excerpt":"VScode的终端调用的是cmd或者powershell，不巧这两个shell默认的中文编码方式都是GBK，但我们经常用的是UTF8格式，所以经常会遇到中文乱码，那么如何解决这个问题呢？","text":"VScode的终端调用的是cmd或者powershell，不巧这两个shell默认的中文编码方式都是GBK，但我们经常用的是UTF8格式，所以经常会遇到中文乱码，那么如何解决这个问题呢？ 思路由于电脑默认的是powershell，这里以powershell为例： 只要不怕麻烦，每次打开终端都使用命令chcp 65001，手动将编码方式改到utf-8就好，但这样太麻烦了，所以思路就是在打开终端时自动执行这条命令 之前其实vscode是支持直接使用shellArg属性来定义的，很多博客也是这么来的，但不巧，我下载的最新版的VScode把它给取消了 ，，，一个头两个大，那就需要更奇妙一些的配置了 老版本的配置如果是1.56之前版本的VScode，很简单的配置如下： 在VSCode中通过打开“文件”–“首选项”–“设置”, 然后在setting.json中设置, 把下面三行复制到里面: 123456{ \"editor.fontSize\": 18, \"terminal.integrated.shellArgs.windows\": [\"/K chcp 65001 &gt;nul\"] //这句如果无法执行，就换成\"-NoExit\", \"/c\" ,\"chcp 65001\" \"terminal.integrated.fontFamily\": \"Lucida Console\",} /K chcp 65001 &gt;nul的含义是在运行cmd的时候将编码设置为65001; nul是避免在控制台输出修改编码的信息，否则会输出active code page: 65001;同时，赠送两个彩蛋，例如:editor.fontSize: 20(修改字体大小为 20);terminal.integrated.fontFamily:”Courier New”(字体修改为 “Courier New”).这个可以根据自己的需求填写，当然如果不需要的话，可以不写。 &gt;nul可能是个坑，反正我配置了之后提示out-file之类的问题，不过就一行输出就放弃吧，删了就行了 1.56版本VScode之后的配置由于新版的VScode不支持shellArgs这个参数了，强行设置会报错 所以需要用到比较高级一点的配置方式如下： 123456789101112131415{ \"terminal.integrated.profiles.windows\": { \"PowerShell\": { \"source\": \"PowerShell\", \"icon\": \"terminal-powershell\", \"args\": [\"-NoLogo\",\"-NoExit\", \"/c\" ,\"chcp 65001\"], //在终端启动时使用chcp命令将编码格式改变为UTF-8 }, \"Command Prompt\": { \"path\": \"${env:windir}\\\\System32\\\\cmd.exe\", \"icon\": \"terminal-cmd\", \"args\": [\"-NoLogo\",\"-NoExit\" ,\"chcp 65001\"] }, }, \"terminal.integrated.defaultProfile.windows\": \"PowerShell\" // 必不可少！要不然上边的都没用} 这里使用了profiles，实际上就是自定义shell的参数等等。-NoLogo参数是指在启动的时候不显示版权信息（加不加都行） 参数可以通过powershell -help查看，还是比较清楚的 然后defaultProfile参数必须要，缺省的话VScode会默认使用系统配置的powershell（woc这个地方卡了我好久好久好久，一直在纠结为啥配置了之后不生效）。这一条也可以通过UI界面来配置如图： 写在最后有一说一，还是学到了一些东西的，对VScode编辑器也有了更全面的了解，但是还是号麻烦啊，而且耽误事儿/笑哭 由于刚好赶上升级，然后升级之后就出现了这个问题，导致几乎所有的教程的方法都无法使用了，最后是在github上vscode的issue上找到的灵感，才最终成功配置，另外，很怪的是我配置好了之后，发现博客的评论区居然有昨天某位大佬给出的解答，和我的一模一样。可惜了怎么不专门开个帖子呢。害我自己琢磨了两个多小时，，，， reference： 永久解决VSCode终端中文乱码问题_ityanger的技术栈-CSDN博客_vscode终端中文乱码 （关注评论区） Upgraded to version 1.56.0… Issues with terminal/Powershell · Issue #123066 · microsoft/vscode · GitHub PowerShell 的命令行启动参数（可用于执行命令、传参或进行环境配置）_walterlv - 吕毅-CSDN博客_powershell 参数","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"VScode","slug":"VScode","permalink":"http://philia897.github.io/tags/VScode/"},{"name":"technology","slug":"technology","permalink":"http://philia897.github.io/tags/technology/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:03:51.229Z","comments":true,"path":"2023/08/08/hello-world/","permalink":"http://philia897.github.io/2023/08/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"pytorch学习札记","slug":"pytorch学习札记","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T07:04:55.284Z","comments":true,"path":"2023/08/08/pytorch学习札记/","permalink":"http://philia897.github.io/2023/08/08/pytorch%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0/","excerpt":"学习pytorch的札记和总结，思考","text":"学习pytorch的札记和总结，思考 pytorch detach().numpy()eg：array = tensor.detach().numpy() 目的：detach阻断反向传播，numpy实现将tensor转换成ndarray，实际上就是从张量转化到数组，方便进行数据传递和纯计算 pytorch tensor.view(-1,1)相当于reshape，改变tensor维度为列tensor，[1,2,3,4,5]-&gt;[[1],[2],[3],[4],[5]]， -1代表默认 http://sofasofa.io/forum_main_post.php?postid=1003866 tensor.data&#x2F;tensor.detach()都是为了反回一个新的tensor且不可求导 但detach方法更安全 https://blog.csdn.net/qq_27825451/article/details/96837905","categories":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/categories/python/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"pytorch","slug":"pytorch","permalink":"http://philia897.github.io/tags/pytorch/"},{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"}]},{"title":"《联邦学习》读书笔记","slug":"《联邦学习》笔记","date":"2023-08-07T16:00:00.000Z","updated":"2025-02-25T05:38:18.776Z","comments":true,"path":"2023/08/08/《联邦学习》笔记/","permalink":"http://philia897.github.io/2023/08/08/%E3%80%8A%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"《联邦学习》是杨强、刘洋等人编著的面向数据安全和隐私保护的机器学习的综述性的书，整体上不是很具体，但是能对联邦学习有一个比较大体的了解 作者：杨强刘洋程勇康焱陈天健于涵","text":"《联邦学习》是杨强、刘洋等人编著的面向数据安全和隐私保护的机器学习的综述性的书，整体上不是很具体，但是能对联邦学习有一个比较大体的了解 作者：杨强刘洋程勇康焱陈天健于涵 1.引言介绍人工智能的挑战以及联邦学习的分类和发展 1.1 人工智能面临的挑战 数据量庞大难以达到，通常得到的数据规模小，信息不全（需要打标）难以桥接的数据孤岛 数据所有权，用户隐私和数据安全的关注度不断提高，法律法规，在不同组织间收集数据和分享数据将变得越来越难，某些高度敏感的数据也不能无限制计算和使用。 如何在条条框框更多，解决数据碎片化和数据隔离的问题 将数据联合起来协同训练会导致数据失去掌控，整合后带来的性能赠一不好分配，分配效果不透明，加剧数据碎片化和孤岛分布 物联网和边缘计算，大数据分布在许多方面。eg：自动驾驶汽车在本地要处理信息，并且和其他的汽车和计算中心协同工作 =》 如何安全有效的实现模型在多个地点间的更新和共享 1.2 联邦学习概述 寻求一种方法，训练机器模型 | 不必所有数据集中到一个中心 单独训练一个模型 | 每个数据源 &gt;&gt;&gt; 各模型彼此交流沟通 | 设计交换模型信息的过程 =&gt; 保护隐私 &gt;&gt;&gt; 模型聚合得到全局模型 =&gt; 仿佛数据源整合 1.2.1 联邦学习的定义 联邦学习 = { 模型训练，模型推理 } 模型训练阶段：模型相关信息共享 | 数据不共享 | 交换过程不会泄露隐私 模型推理阶段：协作预测 &gt;&gt;&gt; 公平的价值分配机制 =&gt; 分配协同模型获得的收益 特点： 有多个参与方 | 持有数据 数据不离开所有者 模型信息在各方之间传输交换 | 加密 | 无法根据模型推测数据 性能充分逼近理想模型（集中数据训练的模型） 中央协调方；可以有也可以不涉及 挑战： 通信链路：不稳定；速度慢 样本数量 or 分布 ：不均衡 =&gt; 联邦模型产生偏差 容易遭到恶意攻击： 破坏性的模型更新信息 =&gt; 可用性降低 等 1.2.2 联邦学习的分类 联邦学习 ={ 横向联邦学习，纵向联邦学习，迁移联邦学习} 数据组成：样本ID: ，特征空间 ，数据标签空间 ， 横向联邦学习： 按样本划分的联邦学习 重叠的数据特征较多 eg：不同地区的银行 数据特征取交集，数据ID取并集 纵向联邦学习： 按特征划分的联邦学习 重叠的数据较多 ID eg：一家银行一家电商，客户群体上有很大交集，提供不同服务 数据取交集，特征取并集 联邦迁移学习 重叠都比较少的时候 1.3 联邦学习的发展1.3.1 联邦学习的研究 Secureboost，提升纵向联邦学习安全性 | 无性能损失 | 隐私保护 | 提升树系统架构 联邦学习的脆弱性，很容易被投毒 联邦学习和强化学习 多任务学习 vs 联邦学习的统计学难题 1.3.2 开源平台 Federated AI Technology Enabler （FATE）微众银行 | 开源项目 | 安全计算框架 | 联邦学习平台 TensorFlow Federated （TFF） 开源框架 TensorFlow-Encrypted 搭建在tensorflow顶层的python包，用于实验 =&gt; 机器学习方式|面向隐私保护 coMind 开源平台 | 联邦平均算法的实现 &lt;= 关键组件 搭建在tensorflow顶层并提供api Horovod 由Uber创立，基于开放的消息传输接口（Message Passing Interface MPI） OpenMined/PySyft，提供两种方法 | 隐私保护 { 联邦学习 ， 差分隐私 } 1.3.3 联邦学习标准化开展IEEE P3652.1 Federated Machine Learning Working Group 1.3.4 联邦人工智能生态系统FedAI Ecosystem 开源技术 标准和指导方针 多方共识机制 垂直行业的应用 2. 隐私、安全及机器学习介绍背景知识 | 机器学习相关 | 面向隐私保护 { 机器学习技术， 数据分析 } 2.1 面向隐私保护的机器学习Privac-Preserving Machine Learning，PPML 系统 由个人，团体或机构自行决定何时、如何以及在多大程度上将有关他们的信息传达给他人 ── Alan F.Westin 信息安全的主要思想是控制对个人信息的收集和处理的过程 2.2 面向隐私保护的机器学习与安全机器学习 安全威胁 攻击模式 防御方法 PPML 隐私性 重构攻击 安全多方计算 机密性 模型反演攻击 同态加密 成员推断攻击 差分隐私 安全机器学习 完整性 投毒攻击 防御精馏 可用性 对抗攻击 对抗训练 询问攻击 正则化 ​","categories":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/categories/ML/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"联邦学习","slug":"联邦学习","permalink":"http://philia897.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://philia897.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"在OpenWRT上配置Openvpn","slug":"在OpenWRT上配置Openvpn","date":"2022-03-03T16:00:00.000Z","updated":"2025-02-25T07:06:06.036Z","comments":true,"path":"2022/03/04/在OpenWRT上配置Openvpn/","permalink":"http://philia897.github.io/2022/03/04/%E5%9C%A8OpenWRT%E4%B8%8A%E9%85%8D%E7%BD%AEOpenvpn/","excerpt":"​ KTH BNSS 课设，基于OpenWRT搭建VPN服务的部分，WireGuard比较顺利，而OpenVPN却困难重重，不过最后还是给配好了，中间遇到的问题和经验记录如下","text":"​ KTH BNSS 课设，基于OpenWRT搭建VPN服务的部分，WireGuard比较顺利，而OpenVPN却困难重重，不过最后还是给配好了，中间遇到的问题和经验记录如下 1. 安装OpenVPN并进行基础的配置1.1 环境信息OpenWRT version：19.07 1.2 安装OpenVPN到OpenWRT这里主要是参考了官网的教程进行软件的安装，然后就是经典读脚本理解配置文件时间，主要参考官网的脚本和教程。但从这里开始就遇到问题了，一方面是版本的问题，另一方面就真的是官方的锅了。有很多问题解决的也是碰巧。 ####### 此处描述安装流程并附上reference 1.3 使用官方脚本创建configure文件########## 此处详细描述使用官方脚本的流程 ########## 此处详细描述由于版本不同造成的问题，并附上debug的方法 ########## 此处详细描述最后的解决方法 1.4 尝试使用图形化管理方式并描述遇到的问题######### 此处描述图形化管理居然无法显示使用uci配置的instance的问题（太tmd蠢了），并描述是如何通过端口冲突发现的，并如何解决的 2. 进行其他的配置除了configure，还要配置firewall 2.1 配置firewall3. 解决无法连接外网的问题重头戏，这块实在太扯淡了，详细描述这傻逼事情 3.1 问题描述描述可以建立连接但是ping不通也无法上网的问题 3.2 debug方式描述debug的尝试 3.3 最终解决描述最终是怎么解决的，也是比较扯淡，主要是两点： 不仅要重启openvpn，也要重启network （介绍怎么发现的，10.0.4.0 &#x3D;&gt; 无法ping通10.0.2.1, &#x3D;&gt; 修改network&#x3D;&gt;重启network，居然可以连通了&#x3D;&gt;但ip是10.0.1.2,&#x3D;&gt;尝试变回10.0.4.2,&#x3D;&gt;无法连通&#x3D;&gt;复现&#x3D;&gt;发现必须重启网络服务才能连通） configure里边的server项要设置成包含在内网内的部分，同时DNS要设置成内网的网关，否则无法连通","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"OpenWRT","slug":"OpenWRT","permalink":"http://philia897.github.io/tags/OpenWRT/"},{"name":"OpenVPN","slug":"OpenVPN","permalink":"http://philia897.github.io/tags/OpenVPN/"}]},{"title":"设置AHK开机启动","slug":"设置AHK开机启动","date":"2022-01-21T11:44:52.000Z","updated":"2022-01-21T19:06:16.055Z","comments":true,"path":"2022/01/21/设置AHK开机启动/","permalink":"http://philia897.github.io/2022/01/21/%E8%AE%BE%E7%BD%AEAHK%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8/","excerpt":"设置开机启动AHK并赋予管理员权限确实是不太方便的一件事儿，这里记录一下，方便日后查阅参考。","text":"设置开机启动AHK并赋予管理员权限确实是不太方便的一件事儿，这里记录一下，方便日后查阅参考。 1. 创建启动脚本如果是要运行多个AHK脚本，就先创建一个bat脚本来一次启动全部，大致内容参考： 12start D:\\programs\\AutoHotKey\\chCapsLockstart D:\\programs\\AutoHotKey\\chPg 然后创建一个快捷方式LocalAHKStart指向这个start.bat: 在上图中点击高级，然后把以管理员身份运行打勾（这一步不知道有没有用，但反正我这样是走完是可以运行的，说不定这是多此一举😂） 2. 创建开机启动任务这里有点离谱，踩了不少坑，我主要参考这两篇博客： Win10 如何以管理员身份设置开机自启程序 Task Scheduler The operator or administrator has refused the request 其中，按照博客1的操作配置正常的开机启动程序，比较简单，就是使用windows10自带的任务计划程序，然后在触发任务上进行如下设置： 其他的没啥要改的设置，可以看看，尽量不要设置限制为宜 用户不需要administrator，只要是有管理员权限的就好","categories":[{"name":"Windows","slug":"Windows","permalink":"http://philia897.github.io/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"http://philia897.github.io/tags/Windows/"},{"name":"AutoHotKey","slug":"AutoHotKey","permalink":"http://philia897.github.io/tags/AutoHotKey/"}]},{"title":"WSL玩耍记录","slug":"WSL玩耍记录","date":"2022-01-07T08:57:45.000Z","updated":"2023-08-07T20:18:15.649Z","comments":true,"path":"2022/01/07/WSL玩耍记录/","permalink":"http://philia897.github.io/2022/01/07/WSL%E7%8E%A9%E8%80%8D%E8%AE%B0%E5%BD%95/","excerpt":"​ 在windows10上边安装了WSL，记录一下是怎么整的，还是有点好玩的。","text":"​ 在windows10上边安装了WSL，记录一下是怎么整的，还是有点好玩的。 1.安装WSL安装WSL主要参考microsoft的官方指南，因为都说WSL2好用，所以就选择用WSL2，这玩意儿装还需要扩展kernel。 主要流程大致： Enable the Windows Subsystem for Linux Enable Virtual Machine feature Download the Linux kernel update package，double click and run Set WSL 2 as your default version Install your Linux distribution of choice(through Microsoft store, Ubuntu) 复杂嘛倒也不复杂，但是在Microsoft Store下distribution的时候，老给我报错说windows update给关了不让装（因为之前用联想的工具把自动更新给关了，还得重新给打开，然后再装） 2.配置WSL配置WSL就是纯粹的娱乐环节啦，主要参考了以下几篇博客： 安装图形界面：https://www.cxyzjd.com/article/tomatt77/109596590 安装一些常用工具： https://www.jianshu.com/p/bc38ed12da1d 2.1 安装图形界面这里主要查到有这么几种方案： 使用windows远程连接访问 使用VNC远程连接 ✓使用RDP远程连接 使用软件绘制图形界面 Xming： Xserver+VcXsrv： 最后选了用RDP进行远程连接的方案，主要是说这个沉浸式体验比较流畅，反正我有俩显示器，一个用来显示也挺OK的。VNC和RDP原理上不太一样，但RDP是微软自家的东西，咱就图省事儿直接用了。最后结果还OK。 2.2 安装一些常用工具这里主要安装一些我可能会比较常用的，比如zsh，vscode之类的。主要参考以下一些博客 微软官方指南：https://docs.microsoft.com/en-us/windows/wsl/setup/environment 一些小工具：https://www.jianshu.com/p/bc38ed12da1d 懒得解释了，先传着，如果有需要记录的再补充吧。。。。","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"http://philia897.github.io/tags/WSL/"},{"name":"Windows","slug":"Windows","permalink":"http://philia897.github.io/tags/Windows/"}]},{"title":"PVE：导入硬盘内容到DSM中","slug":"PVE：导入硬盘内容到DSM中","date":"2021-09-04T14:47:12.000Z","updated":"2023-08-07T20:18:03.772Z","comments":true,"path":"2021/09/04/PVE：导入硬盘内容到DSM中/","permalink":"http://philia897.github.io/2021/09/04/PVE%EF%BC%9A%E5%AF%BC%E5%85%A5%E7%A1%AC%E7%9B%98%E5%86%85%E5%AE%B9%E5%88%B0DSM%E4%B8%AD/","excerpt":"​ 由于SB室友的华容道问题，折腾了半天如何将大量数据从已有硬盘导入到DSM中，最终找到了比较合理的解决方案，但过程属实曲折了一些，将折腾硬盘的一些坑和经验汇总如下：","text":"​ 由于SB室友的华容道问题，折腾了半天如何将大量数据从已有硬盘导入到DSM中，最终找到了比较合理的解决方案，但过程属实曲折了一些，将折腾硬盘的一些坑和经验汇总如下： 1.1 故事背景最开始是因为屈子炎要换新电脑，但很多数据都在旧的硬盘中，而有一块硬盘出现了一些问题，不能正常使用了需要被淘汰，就需要将里边的数据导出来。但这时他家的小NAS又用不了了","categories":[{"name":"Network","slug":"Network","permalink":"http://philia897.github.io/categories/Network/"}],"tags":[{"name":"PVE","slug":"PVE","permalink":"http://philia897.github.io/tags/PVE/"}]},{"title":"俄航出国经验总结","slug":"俄航出国经验总结","date":"2021-08-09T16:00:00.000Z","updated":"2025-02-25T07:05:59.203Z","comments":true,"path":"2021/08/10/俄航出国经验总结/","permalink":"http://philia897.github.io/2021/08/10/%E4%BF%84%E8%88%AA%E5%87%BA%E5%9B%BD%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/","excerpt":"​ 2021.8.7俄航飞瑞典求学，一路上还算比较顺利地到达了KTH，总结一下经验","text":"​ 2021.8.7俄航飞瑞典求学，一路上还算比较顺利地到达了KTH，总结一下经验 1. 订机票首先提前一两个月订机票，我订的还算早，订到了比较便宜的机票。这次因为疫情，飞国外的话比较麻烦。最赚的是香港飞的，从蛇口码头坐船到香港国际机场，免入境直接起飞。比较便宜，而且时间好，可以赶上8.2接机。 俄航算是比较便宜的，但是时间上不太友好。一个是因为太早了（早上7:10飞），要提前来上海蹲着。一个是8.7有点不早不晚的，白交半个月房租。 订机票的时候还要考虑到航班取消的问题，疫情期间有时候航班会被迫取消，这样可能会打乱之后的计划。不过俄航每周就飞一天，两班飞机，很稳定没取消过。 最后要提前了解入境政策和中转国家过境政策。莫斯科是24小时内免入境转机，但丹麦好像就需要过境签，需要提前准备好。 订机票可以在官网订，也可以在飞猪、携程等平台上订，不过要看清除机票来源，避免被坑（所以果然还是官网订比较稳妥，就是麻烦了点）。 2. 起飞前2.1 booking code订完机票之后会收到邮件，在附件里边可以看到自己的booking code，也就是那个六位大写字母的号码，这个之后值机的时候会用到，比较重要。至于航站楼之类的，其实不太需要提前关注，因为有可能会变化，以值机的时候的信息为准。 2.2 关于核酸 俄航目前的规定是需要72小时的英文核算检测报告。所以要么做好然后赶飞机，要么提前一天来上海做。 需要提前查一下在哪些位置可以开中英文核算检测报告，以及： 是否需要预约，在哪里预约 一周中的哪几天工作，每天的工作时间 结果多久能出，以及费用问题 这些信息一般网上能查到，但最好打电话确认一下，像我在之江检测所做的，虽然说要预约，但可以到了现场填信息约，也不麻烦。然后最便宜的标准检测80元，说是12-24小时出结果，但其实一般上午去做了下午4点多就出结果可以去取了，完全没必要加急（当时医生就说了，上午做一般下午就能取），但要注意他们的工作时间。人不多，一般不用排队。 最后有一点可以确认一下，就是俄航对检测方法有要求，可以确认一下，一般都是常规荧光检测什么的。 取的纸质的报告单一定不要弄丢了，最好和护照放一起。 2.3 住宿最后综合以上情况，我是选择提前两天到上海，8.7号早上的飞机，我是8.5快中午做的核酸报告。为了方便取报告，选择的住宿的位置也是离检测所的位置比较近的。 因为俄航是一大早从上海浦东国际机场起飞，而机场离市区相当tmd远，如果要在上海玩一下的话，不建议在机场附近住。机场附近的酒店都有送机巴士，但时间一般都是早上4:00之后，比较晚。 所以凌晨三点多去机场，基本上只有打车去一种选择了（机场大厅并不舒服，强烈不建议去那挨一晚上，难受死） 我是使用高德打车，30公里左右吧到机场，大概90元 2.4 值机俄航提前24小时可以在线值机，主要是选座位。主要用到booking code，提前在知乎之类的地方了解一下选座位怎么选以及哪些座位对长途飞行比较友好（虽然这些座位一般都抢不到或者不让选）。 2.5 机场流程把自己和一堆行李折腾到机场之后就比较简单了，首先找辆行李车把箱子啊包啥的都往上边一堆（这个车车在机场门外边，找不到问一下，一定要整一辆）。然后推着进机场，要过一道安检。 之后找到值机和行李托运的柜台（因为在线值机过了，PVG的值机和行李托运好像在一起），问一下工作人员俄罗斯航空的值机柜台在哪里，过去排队就行了。 我当时去得有点晚，提前2.5h到的，光在这里就排了tmd1.5h，强烈建议至少提前3h，最好提前3.5h到，免得在那里带一堆行李站半天。 这里会有人查核酸，提前准备好护照、居留卡、核酸证明就行了。等排队排到了就带着大包小包去办理，整个流程不太复杂，按部就班来就行。需要注意：1. 行李不要超重 2. 可以带一个登机箱或者包（10kg），再背一个随身小包（5kg），尺寸查的不严，重量其实也不太严&#x2F;笑哭，但最好别挑战。 行李最好用那种专门的行李打包带包一下，实在不行用保鲜膜或者胶带包一圈也行，主要防止有人私自打开你的行李塞些baisefenmo什么的。 行李托运办完之后就按部就班地过边检、安检，之后去对应的登机口侯着就行了。 在办完行李托运之后还可以满机场乱跑，行动不受限，和亲戚道别啥的可以先办完托运再说 过海关不能拍照 安检的时候会让你把笔记本、键盘、平板等电子设备都拿出来单独过安检，所以，整理了也白整理（但行李托运好像没这问题，不过行李托运也不能托运这些就是了）。乖乖拿出来过了安检再重新装包里吧 3. 飞行期间飞行期间没太多需要注意的，一些tips： 俄罗斯小姐姐真好看 俄罗斯小姐姐真好看 俄罗斯小姐姐真好看 带一双一次性拖鞋飞机上穿，更舒服一点 包放头顶上，一个小包可以放脚下，不过会压缩本就不大的休息空间，所以建议把证件、手机之类的随身，其他的都扔头顶 英语交流，加手势，一般都能交流 第一程飞机会有两餐，所以乘机那天可以不吃早饭 第二程飞机会有一餐，但由于疫情，最好留着到瑞典了再吃 第一程吃饭上厕所都OK，第二程就憋住，不吃不喝不拉不撒，戴好口罩和手套（手套好像没人会查，但最好戴吧）等防疫用品，和小姐姐说不要水，thanks 4. 转机只需要在moscow转一次飞机，比较简单，跟着路牌（有中文）走就行了，过安检也比较简单，不需要憨皮地掏电子产品，但是要摘手表和皮带，注意下别裤子掉了就行。 轶闻：因为摘皮带用了一小会儿，皮带就和下一个妹子的箱子放一个篮子里过安检，然后那妹子的箱子没过，要排队检查，，，导致我提着裤子等了半天拿皮带。。。。。 可以去服务台问一下候机在哪个口，这个有可能会变化。（当然，也有显示屏会显示） 5. 下飞机来KTH下飞机之后走过长长的通道去领行李，之后去过海关。之后就顺利下飞机了，想办法来KTH就行。 建议方法：express买票，坐车到CentralStation，然后转地铁14号线，坐moroby central方向，到tekniska hogskolan下，直走出站就能看到KTH的校标了。","categories":[{"name":"Life","slug":"Life","permalink":"http://philia897.github.io/categories/Life/"}],"tags":[{"name":"experience","slug":"experience","permalink":"http://philia897.github.io/tags/experience/"}]},{"title":"JupyterLab玩耍记录","slug":"JupyterLab玩耍记录","date":"2021-04-23T14:20:20.000Z","updated":"2023-08-07T20:50:29.708Z","comments":true,"path":"2021/04/23/JupyterLab玩耍记录/","permalink":"http://philia897.github.io/2021/04/23/JupyterLab%E7%8E%A9%E8%80%8D%E8%AE%B0%E5%BD%95/","excerpt":"​ 尝试了一下Jupyter lab，各种问题还是有点多的，感觉除了导出的格式比较多，其他方面不比vscode好用，这里简单记录一下","text":"​ 尝试了一下Jupyter lab，各种问题还是有点多的，感觉除了导出的格式比较多，其他方面不比vscode好用，这里简单记录一下 1. 安装Jupyter Lab首先就是安装，因为默认只有conda的base环境中有Jupyter lab，其他环境需要自己安装，但这个也比较简单，直接命令安装就行： conda install jupyterlab 然后就可以直接使用jupyter lab命令打开了 2. 快捷打开要打开jupyter lab非得用conda有点太麻烦了，所以这里用一点小技巧： 找到默认的jupyter notebook的快捷方式：复制一个到桌面上 修改名称位jupyter lab，修改起始位置，用来标定最开始的起始页面 修改目标为：D:\\softwares\\anaconda3\\python.exe D:\\softwares\\anaconda3\\cwp.py D:\\softwares\\anaconda3\\envs\\python3_learn_ML D:\\softwares\\anaconda3\\envs\\python3_learn_ML\\python.exe D:\\softwares\\anaconda3\\envs\\python3_learn_ML\\Scripts\\jupyter-lab-script.py 注意这里只修改最后两项改成了我的自定义环境中的python路径和scripts路径，这样才能正常启动自定义环境中的而不是base环境中的jupyter lab，自己摸索出来的，目前没找到网上和我一样干的 大功告成，双击运行一下试试吧 3. 其他一些配置这样已经可以正常使用了，但是要让他更好用还是需要一些其他配置，但这样的各种各样的配置就很麻烦了，在网上也能找到解决方案，就懒得写到博客里了，遇到什么问题解决什么问题就行了。有空再补全吧。 现在的问题是lsp代码自动补全插件很慢很慢，慢到影响我使用了肯定不行，然后导入依旧不支持pdf导出，不过可以导出成markdown也不错，所以现在还行，vscode日常写一些脚本，然后再到lab里导出成markdown吧，然后再typora编辑导出成pdf，有点怪，不过用习惯了应该还行&#x2F;笑死","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://philia897.github.io/tags/Jupyter/"}]},{"title":"大创Log","slug":"大创项目log","date":"2021-04-10T16:00:00.000Z","updated":"2025-02-25T07:06:29.769Z","comments":true,"path":"2021/04/11/大创项目log/","permalink":"http://philia897.github.io/2021/04/11/%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AElog/","excerpt":"记录大创项目实践期间的一些所学和感受，简记学到的知识和理解","text":"记录大创项目实践期间的一些所学和感受，简记学到的知识和理解 2020/7/9内容： 在阿里云服务器安装anaconda，一切顺利 测试代码d:/collection/codes/git_progects/vae/vae_keras.py，并尝试修改z的维度，发现维度升高之后vae的编解码效果更好更准确，但泛化性能可能更差一些（需验证） anaconda下载速度慢的问题，已解决 在~/目录下新建.condarc文件，然后添加以下代码： 12345678channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - defaultsshow_channel_urls: true 查询channel命令：conda config –show 添加channel，在~/.condarc中添加 阅读学长写的论文到VAE部分，了解了第一步数据生成器的作用 学习pycharm远程连接服务器运行代码的教程，还未测试 reference： Anaconda常用命令 - Wind·Chaser - 博客园 VIM 常用快捷键-百度经验 pycharm远程连接服务器运行代码加调试_yeler082的博客-CSDN博客_pycharm远程连接服务器 2020/7/10内容： 阿里云服务器配置好了环境 VAE再回炉主要看了一下损失函数 阅读学长写的论文完毕 初步调试学长写的程序遇到了问题了，学长的VAE的loss基本上没有下降/笑哭，调试中 2020/7/15内容： 学习了GAN原理，复习了一下论文的GAN部分，发现我们用的GAN和标准GAN还不一样。标准的GAN是输入噪声然后生成数据，再混入真实数据进行判别。但这里我们用的是输入的是x，生成的是判断结果，然后真实数据是实际模型对x的判断结果，最后判断哪些是实际模型的判断。实际上相当于是输入的所有x相当于一张图，然后生成器生成一个向量表征每个x的判断结果，再一个向量是真实模型预测的结果。判别器就是判断两个向量哪个是真实的哪个是生成器生成的就好。最后两个向量没区别了，就代表generator基本上能代替victim model 了。 看代码（全部），理清代码逻辑 测试代码，发现sythetic部分无问题，最后生成的augmentation data从victim model的检查结果来看也是“好数据”，但用augmentation data训练出来的模型和真实模型的相似度很低，目前来看，要么是模型的问题，要么是dataset的问题，还有待排查。 用augmentation data 作为输入和用synthetic data 和用 real data 作为输入三者最后得到的模型相似度，augmentation data 的远低于另外两个，需要注意。 2020/7/16内容： 测试GAN，50%左右相似度，很差，原因不明，augmentation data都是好的数据，但是不知道为什么训练出来的GAN很差，而且，观察上有一半，都是刚好判断相反，这说明了什么？ 修改VAE的z到20，最后60多的相似度，好一些，但不知道为什么这么差。可能的原因： 数据和原始数据更接近，但都是输入模型判别率在95%以上啊，数据到底好在哪里？ 数据更少一些了，如果z是3的话，有40000多数据，但是z是20的时候只有35000多，为什么会这样？VAE的拟合效果好了反而数据不够准？ 另外还有一点，就算是原始训练数据，输入到原始模型里面，也并非都是95%以上的判别率，很多都是90,80几。所以，可能还真是数据的问题。 这可能回答了上一个问题，因为更像原始数据了，所以反而不太容易上95%，所以数据量少了 但总之我觉得判别模型的误差大和数据的关系不大，主要是GAN的问题我觉得 测试发现，并非数据集越大越好，分别取epoch=25,50,75,100，发现75的时候最高，为70%多，50和100为60%多，25是50%多，注意，大概率是GAN模型也有问题，每次实验都要比较每个模型的大小 还有一点，训练GAN的batchsize=500，最后生成的是个2*500的矩阵，是不是太大了点？这样的话如果光看图片的话，很有可能有些点出错也瞧不出来，尝试减少batchsize试试看，batchsize太大我觉得很容易淹没误差信息 测试结果: batchsize=50： 20/40/60/80都很低，100的时候是70%多，120/140都很低，160为80%多！不够稳定，所以有点懵逼，可能是优化器的问题，换用SGD试试看。但至少结果上看好像并没有比batchsize=500的时候好很多，是在epoch=160的时候才出现好转，也有可能是epoch比较少的问题 使用优化器效果不好，用SGD确实要稳定一些，但一直都是50%-60%，很差很差很差，没有adam好 batchsize=25，adam，epoch=400： 很差，20,100多的时候上过70，但都没到过80, 其他的都在40-60波动，基本上排除是因为epoch不足带来的损失，猜测可能数据上太好了点，都是非零即一的判别，没有教会模型“模糊判别”的重要性，去除掉VAE之后的过滤环节，尝试训练模型。另一方面可能是数据集太小的问题，下次搞个大一些的数据集 2020/7/17内容： 测试GAN: batchsize=100, epoch=20 : 数据感人，也没有突破80%，最多到78%，epoch=20的时候，但有个现象值得注意：原始模型判别率在95%以上的，基本上模拟模型也都能判别出来，但原始模型在80%以下的，误差就比较大，所以，很有可能是样本太过于单一不具本普适性，使得模型的泛化性能比较差，尤其是零和思维比较严重 修改数据生成模块：vae保持不变，生成数据增大一倍，同时去掉之后的95%门槛，使得数据更加多样化。然后GAN的batchsize控制在100比较合适了，epoch=40即可。 augmentation data=100000，batchsize=100，epoch=40，adam： 出现错误：Nan、inf or larger than dtype（float64），重新生成一遍augmentation data，目前来看没有出现问题，可能只是一次偶然的mistake 结果：波动幅度小，泛化性能提高，最高可以到85%左右的准确度，一般也是75%-80%左右。总体来说效果还算不错，比起之前的效果确实要好上不少。我觉得多半是augmentation data的问题。 另外，现在的模型没有之前那么绝对，但感觉又有点太疑神疑鬼，不敢拿决定，并且，刚好判断反的情况依旧比较普遍。感觉像是数据集不够大的原因。尝试将数据集再调大一些试试看效果 augmentation data=200000，batchsize=50，epoch=20： 相当差，差到不可思议，augmentation data大一些了之后直接爆炸，GAN直接无法收敛，错误率达到80%左右居高不下。神特么，，， 最后的效果，准确率根本没超过50% 不知道为何无法收敛，需要再检查一下，理论上应该可以收敛才对啊，，至少不应该如此大的误差啊 生成的augmentation data里面有很多高达0.99的数据，但应该也影响不大才对啊 找着原因了：augmentation data生成失败，生成的数据有问题，可能是因为一次的number太大了 想法： 可不可以在GAN训练的基础上接着训练呢？用不同的训练集，增加训练量 2020/7/18内容： 测试GAN： augmentation=100000 ；no filter；batchsize=500；epoch=200：MSE0.0几的水平，similarity基本上都在80%左右，最好的可以到87%左右。现象如下：augmentation data里面的similarity可以到95%左右，但在test data里面只能到80%，说明augmentaion data还是不够好，和实际的data并不是很一致。需要再进行一些改进，让augmentation data可以更加接近真实数据 batchsize=1000，其他同：直接爆炸，80%错误率，不能看 之前的训练都是基于svm的，现在开始尝试 rf 的实验： vae： batchsize从50-500基本上对loss（11左右）和MSE（7.几）影响不大 z从5-25，15-25的MSE可以稍微降到6-7之间，loss没太大区别 将MSE在loss中的权值乘上1.5，MSE可以降到4左右，尝试一下这样会不会更好 gan：augmentation=90000多；0.7 filter；batchsize=500；epoch=100，vae loss不改： 基本上都是60%多一点，有点差诶，但在augmentation上面的误差也在0.3-0.25左右，也就是说这个成绩是能接受的，因为在训练集上做的都不好。 也说明augmentation data还行，但gan训练的效果不好 gan：augmentation=100000多；no filter；batchsize=200；epoch=100，vae loss不改： 基本上都是75%-80%的准确度，效果很不错很稳定。在augmentation data的similarity是0.13-0.2左右，在test data上是75-80%。 效果不错，但test data和augmentation data的预测度有偏差，还是说明增强数据集不是特别“合理”，但已经属实不错了。 另外，gan的batchsize会影响训练程度，需要进一步的实验测试比较合适的batchsize augmentation data的数量也可以适当再增加一些？ vae的loss可以进行一些修改，比如在MSE加一个大于1的权值，这样可能会有更好的拟合效果。 2020/7/19 rf： gan：augmentation=100000多；no filter；batchsize=100；epoch=100，vae loss不改： 效果更好了，test data上测试epoch很多都是79%-82%的similarity，效果很稳定，在augmentation data的similarity在0.1-0.2中间波动，波动幅度比较大。 说明batchsize确实是有一定影响的 下一步可以尝试增加augmentation data的数量，看看效果会不会好些，以及改进vae让它生成的数据更好更合理 有点困惑：会不会synthetic data也会影响augmentation data的全面性和真实性？但synthetic data是在真实范围中随机采样，应该不会出现太离奇的数据才对。 另一方面，在augmentation data上跑rf 的训练的时候，预测度都无法达到90%以上，说明了什么？是模型的问题还是数据的问题？可能需要更好的模型吧 rf： gan：augmentation=150000；no filter；batchsize=100；epoch=100，vae loss不改： 效果一般，最后没有跑完，只跑了87个epoch，test测试集上稍微好一些，最好的是83%，但和100000的差不多 2020/7/20 rf： vae：loss = 1.5MSE+KLDgan：augmentation=10w；no filter；batchsize=100；epoch=100： 效果不好，对augmentation data的模拟效果都不太好，errorrate在0.1-0.3之间反复变化，在test data上的效果也欠佳，最好也是79%，有很多50%60%的测试数据。 不稳定，各个similarity都有，gan学习困难 有点迷惑，只能说明修改了vae之后的数据对gan来说学习的难度更大了。不知道该如何改进。需要再探索一下 稍微查看了一下victim model 和 gan 对test data的判断结果，判断可能是因为augmentation数据的不均衡（[1,0]的情况远多于[0,1]）导致 gan 对 [0,1] 缺乏敏感性。 vae：loss = 1.5MSE+KLDgan：修改augmentation=12w，其中5w左右是【0,1】，5w是【1,0】batchsize=200： test data similarity 大概是 70%多一点，判断很多都是刚好判断反，【0.1,0.9】判断成【0.9,0.1】那种。不会是augmentation太多了而网络复杂度不够吧，， aug data 的表现也不太好，基本上0.15-0.25左右的errorrate，和之前的没改过，那么只能考虑可能是aug data 过多了，那么需要减小数据集，姑且先减少到8w左右吧。 vae：loss = 1.5MSE+KLDgan：修改augmentation=8w，limit=0.6，其中4w左右是【0,1】，4w是【1,0】；batchsize=100： aug data：error rate：最低0.15，但有很多特别差的数据，30%40%50%60%都有，相当不稳定。 test data：最高80%，多数75%-79%，也不算好，有30%40%的很差的数据 与上一个实验对比，说明aug data的数量也不能太多，大概这个数字就够了 synthetic data：75%-80% 和 test data 差不多 判断失误的现象依旧，但这次感觉双向失误更多了，之前一直是01错判成10的多一些，现在两者差不多多了，可以判断是aug data 策略修改的功劳 接下来就判断一下vae到底有多大影响吧 vae：loss = MSE + KLDgan：aug data = 8w，limit = 0.6，均分, batchsize = 100： aug data：error rate：差不多也就是之前实验一样的水准 test data：最高79%，还有20%几的？？大部分都在77%左右吧，就是中间有一段很差很差的epoch10-44 synthetic data：和test data上的基本没区别，test 上的 similarity甚至略高于synthetic，，不懂，， 每次aug data的error rate基本上都没啥变化，说明并不是gan训练得不充分 修改了vae之后好像test data上的similarity略有变差，可以想见加个MSE的权值还是有用的，这个权值可以加到2左右？ synthetic 和 test差不多，说明synthetic的数据已经十分接近 test 了，那么就是 aug data 还不够好 还有就是aug data上的gan的error rate基本上都是一个数字，这是victim model决定的，和aug data没有关系，说明 gan 这个模型的极限可能也就是80%多，而 aug data并不能完全拟合test data，所以similarity略低于理论值 那么，如果是这样的话，用在svm上是不是就可以上90%了？ vae：loss = 1.5 MSE + KLDgan：aug = 8w，limit=0.6，均分，batchsize=150： aug data： error rate：0.15-0.2占了很多，但有特别差的 test data：最高79%，大部分在75%左右 synthetic data：略低于test data，有些奇怪，居然比test 低 2020/7/21内容：code从原始 code 切换至code_v1.1 rf： vae：batchsize=50，epoch=10，loss=MSE+KLD，z=14，层数加深 wae_gan: batchsize=50，epoch=10 aug：8w，均分，limit=0.6 wgangp： batchsize=150，epoch=100 wgangp：batchsize=150，epoch=100，aug=8w： aug：0误差 test/synthetic：70%左右，77%最高 说明vae不行，加深了层数反而不行了？有点奇妙，改回原来的试试看吧 wgangp：batchsize=150，epoch=100，aug=8w，limit=0.7vae：改回原来的层数，z=14不变wae_gan：不变，做对照： 史上最离奇实验出现啦！ 简直离谱，， aug：95%左右的错误率 test/synthetic：83%左右的错误率 我正确率都没这么高过！ 完全离奇，不知道什么原因，可能是wgan脑子抽掉了吧，， 2020/7/22 wgangp:batchsize=100，epoch=60，aug=8w，limit=0.7 效果不错诶，aug：0.05-0.1的error rate test/synthetic：82%左右，其中test非常稳定，基本上都是82%-83% 奇了怪了，batchsize=100的时候完全没有任何问题，甚至训练50epoch左右就可以收敛 wgandiv：同： 效果不错，aug：0.05-0.1的errorrate test/synthetic：83%/81%，不错 一点想法： 老师给的数据集不是很好，判断10比判断01的数据多很多，导致synthetic采样的数据也是如此，但aug data我又取的是每种情况的数量差不多，所以导致生成的01数据没有10数据泛化性能好，拟真程度高。也就是数据不够像真实数据。所以错误率会比10的高一些。所以可以在synthetic采样上做文章？ aug data上的error rate 比 test/synthetic 的低0.1左右，也就是说aug data还不够像真实数据。但实际上synthetic数据已经比较接近test dataset了。我已经无能为力了，我觉得vae已经做的够好了，那么只能在别的方向上找出路： 将synthetic数据融入aug data做训练 aug data训练完成后再用synthetic data做一些训练 用synthetic data中判断错误的数据再生成再训练 验证实验： rf： 参数： vae：batchsize=100，epoch=10，loss=MSE+KLD，z=14，层数不加深 aug：8w，均分，limit=0.7 wgandiv： batchsize=100，epoch=100 结果： test数据集上77-78的similarity，效果一般，看来之前的82%有运气成分在 errorr0远少于error1，说明数据并不是很好，error0 大概 15%，error1高达50%，完全碰运气了就是 在synthetic data 上也是如此，error0和test相差无几，主要是error1到了30%多 结论：需要改进方案，synthetic data需要改进，然后在训练完gan之后需要加一步微调，最好通过synthetic 2020/7/24有个奇怪的现象：synthetic采样的时候居然是01情况略多？两者差不多，01略多。这是我没想到的，那为什么aug data生成的时候10情况远多于01，那就只能说明synthetic 采样的时候虽然 10 的多，但峰值都很小，每次采样的数据量不大，所以才导致如此。而aug data 的时候因为是按照概率limit采样，所以10的更多。 不管结果如何，改进了synthetic采样，让两种情况的采样数量差不多。然后再进行实验，看看效果如何，估计效果一般/笑 新一步实验：目标：在原实验基础上，添加步骤来提高最终的similarity aug ： limit=0.6 data：8w vae：batchsize=100，epoch=10，loss=MSE+KLD，z=14，层数不加深 wgan：batchsize = 100， epoch=100 不行，不行，不行，虽然有80%左右的similarity，但error0只有0.1，但error1高达0.5？？基本上就是瞎猜了 猜测难道是limit 的问题？但0.6已经不高了啊。 推测：在R范围内，10占的空间更大，01占的范围小，而实际判断的01占的位置更小，有部分本来是01的被判断为10（一半），如图： 所以，应该还是取样的问题，就是取样的都是不在边界附近的数据，所以导致边界不明确。而因为取样是如此，实际上生成的数据中在边界附近的数据也不多，这就造成了边界界定模糊的问题。而因为数据分布10的数据多，虽然边界界定不清，依旧占比较少，而01数据因为本身比较少，判定不明所以最后error1很大。如图所示。 我觉得重点在于想法子将边界界定得更加明确一些，也就是说，要生成一些矫正数据来修改“判定01”的圈子位置。而这，应该就落到了aug data的任务上咯。生成判定范围在0.8以内的数据，然后接着训练gan。这些数据要多一些，才能矫正。 2020/7/27经过组会的总结，有了一些启发， 下面调整方向：aug data的五五开策略确实不太好，实际上今天实验的时候，不增加这个策略，发现基本上aug data的概率分布就和train数据差不多。原始大概3:1，aug data的比例大概是85:15，更加具有偏向性一些，但似乎也不是不合理。但最后实验训练出来的效果是对01的情况辨识度很差，所以我觉得运用focal loss可能确实是有用的，可以试试看！但边界模糊的问题依旧不好解决，用focal loss和gan并不能很好的结合，需要设计一下 另外出现了新问题：为什么synthetic产生的也是不平衡的数据？而实际上生成的时候输出的number0和number1是平衡的啊？为什么两中输出不一样？可以肯定的是number0和number1是有问题的，但问题还不知道在哪儿 总结下现在的实验现象：synthetic程序运行的过程中统计两种预测结果的采样数量：大概1:1程序运行结束后将synthetic data喂入victim model，得到的prediction中两种的比例大约4:1同时，程序的采样策略是第一个峰值截断，每次的峰值都在0.8以上，所以，应该不会出现预测probability在0.8以下的数据，但实际预测结果是有大量数据的预测probability在0.8以下，甚至有0.501…这种神奇的概率每次turn采样的数据喂入victim model之后的预测概率和之前的不同，见下图： 已解决，感谢wbr大佬，问题出在了预处理上，因为synthetic dataset进行的预处理，也就是归一化，和采样的时候用的不是一个归一化，就造成了这种奇妙的现象。也就是synthetic dataset里的那些数据，在采样的时候进行的预处理加均值除方差 和 之后训练vae的时候进行的预处理加的均值除的方差不同（因为数据集不同，均值方差不同），造成了domain shift 实验： 取消五五开策略（因为没什么太大用处），再次实验： wgandiv：epoch=100，aug data=8w，limit=0.5，batchsize=100： 80%左右的正确率，但依旧存在01情况错误率高的问题，同时在test/synthetic集上的全部误差都比在aug data上大10%-20% 知识蒸馏在synthetic只有70%左右正确率，吊打状态，但wbr说可以蒸出80%以上？可能不是很稳定吧？ 加入 focal loss 策略： 在fake prediction判别为10和01的情况下对loss加不同增益，[0.25,0.75]，但效果不大，估计是focal loss不好 主要也可能是和netD的输出有关系，本来在（0,1）区间挺好的，但是wgan取消了这条，有可能大，也有可能为负，导致增益不好加，，烦人！当然也可能本身focal loss没啥卵用，但我觉得多少应该有点用吧，至少应该能降低 01的错误率吧？ 思考： 现在是训练五次netD，一次netG，但我们实际用的是netG啊，那这不实际上用在训练netG的数据只有20000？是不是有点少？可以试试调小n_critic参数看看效果 已解决，dataloader里面有随机选项，我憨憨了 2020/7/28实验： wgandiv： epoch=100，batchsize=100，n_critic=2，aug data=8w，limit=0.5，不平衡生成，一般loss 爆炸，只有68%的similarity，而且MSE也居高不下（20左右），test和synthetic都不行。看样子是不太行的 那看来n_critic是不能改咯 又想到一个点子： 既然如此，那只要把 i % n_critic == 0 改成 （i+epoch）%n_critic == 0 不就也可以利用全部数据了么，这样每次epoch都不一样，也可以有效果吧？再不济也有80%左右的正确率嘛，试试看： epoch=30，batchsize=100, n_critic=5，aug data=8w，limit=0.5，proportion=[0.8,0.2], 一般loss 80%的similarity，MSE在0.7，和KD差不多，KD略好（82%，0.6），但还行吧，我觉得有希望！ 主要是01的情况还是错误率有点点高，我觉得还是需要搞搞 epoch=30，batchsize=100, n_critic=5，aug data=8w，limit=0.5，proportion=[0.8,0.2] focal loss: [0.2,0.8]: netG，netD都应用： wonderful！居然可以similarity到85%！新纪录！不过是在原本的基础上练出来的，可能缺乏公正性，还需要再验证一下。 再次验证，成立！可以，效果不错 简直是神来之笔，focal loss赛高！其实不是正宗的focal loss啦，叫伪focal loss或者类focal loss比较好，毕竟我只用到了alpha项，也就是加不同的增益 原版如下：膜一下大佬： Focal Loss 的Pytorch 实现以及实验 - 知乎 然后对svm： wgandiv：epoch=100，batchsize=100，aug data=8w，limit=0.5，proportion=[0.75,0.25]，一般loss focal loss：[0.2,0.8]：netG，netD应用 93%的similarity，没话说 然后对xgboost： wgandiv：同 focal loss：同 86% similarity， 0.7 MSE 2020/8/1 今天做实验，发现个小细节： 震荡现象还是比较普遍的，所以完全可以使用可变学习率的方式，或者干脆调小学习率来减小震荡现象的产生嘛。 已经通过可变学习率解决，见 2020/8/16 的 log 需要学学adam，然后看看能不能再调调参数？啊好烦，又要调参数，我不想调参数啊！！ focal loss也可以改进改进，比如强调一下概率不那么悬殊的，比如0.6/0.4 这种，这样会不会有更好的MSE？ 实际测试也发现，使用可变学习率可以显著加快收敛速度，甚至可以做到比KD还快，指标可以是每次算得的MSE。0.001-&gt;0.0001-&gt;0.00001-&gt;0.000005 2020/8/7今天简单测试了一下实验，发现数据量在2w的时候对xgboost的模拟效果完全没有影响，各项指标都和9w的一样，说明数据量不是一个很关键的因素！ 那么现在看来比较有效的改进：一个是gan的batchsize，一个是aug data的取消筛选策略，一个是focal loss策略的应用 直接使用synthetic data训练gan： xgboost：similarity：86%左右，MSE：0.65左右 svm：similarity：91%，MSE:0.3 rf：similarity：83%，MSE:0.65左右 2020/8/16思路汇总： 首先要用可变学习率来使得最后训练的结果一定要收敛，模型的存储可以使用MSE递减策略 为了让每个数据都是“有用的”，也就是尽量每个数据都包含着新的信息量，那么就需要让每个数据都尽量“独特”。所以使用wae_gan来训练模型，用很少量的数据来训练，然后进行aug data，然后再进行wae_gan训练，依次往复。aug data的策略是用wae_gan里面的netD选择小于0.5的，也就是判断为假的数据，这样这些数据就是较为“独特”的数据。但是，一次aug data也不能太多，因为aug data生成的数据会出现重复的情况 代码改进： 加入可变学习率，效果拔群，训练效果十分稳定，且效果还是很棒棒的。similarity在86%左右xgboost 改进模型存储使用MSE递减策略，效果也相当不错，比较满意 synthetic调整： 加入synthetic_simple，也就是随机采样不进行筛选，应用最新的策略，最后训练出来的xgboost也在86%左右，还算不错吧？KD是78%，也是吊打状态 rf的效果不行，86%左右，但是和筛选的synthetic策略相比也没有变。观察发现主要是少的那部分数据。 另一方面，KD的表现相当差，68%的similarity，全面吊打KD/笑哭 svm的效果： 89%的gan，还算不错吧 2021/1/30 安装xgboost 花了一些时间回顾之前的工作，上手 使用原始训练数据进行窃取训练，但效果并没有变得更好啊/笑 2021/3/3总结一下目前的工作： focal loss加入了err偏置项，可以一定程度上实现自适应，比alpha手动调整稍微高明一点。以及对抗数据分布不均匀现象 发现的问题： 使用preprocessing之后数据的分布实际上是发生了改变，不知道training data的distribution的情况下进行预处理实际上是会产生比较大的误差的，假设已知数据分布进行预处理（已知training data的均值方差）的结果和直接归一化（算均值方差），得到的数据喂入模型得到的输出结果的差异还是相当大的，前者大概两种分类1:1，后者可能能偏到10%：90%，这个问题可以通过focal loss得到一定程度的解决，但真的要解决的话还得再想想 一种思路：可不可以直接假设预处理也是victim model 的一部分进行模拟， 这样相当于就多了一层映射，应该也是可以的吧？我觉得是值得一试的 数据采样还是有问题的，我觉得还需要再做一些尝试，使得数据更好 之后的工作： 改进采样策略： 使用熵排序算法采样，更进一步使用论文中的deepfool方法 使用k-mean聚类算法 加强版结合1和2的思路，先熵后聚类去重，减小访问量，这个思路还是挺精巧的我觉得 解决一下预处理的问题，看这个思路是否真的可行 2021/3/4今天尝试了使用绝对的preprocessing的方法，虽然效果没有变好，依旧在82%左右acc，但是问题更加明显了，在synthetic数据集上进行测试的时候在class1上的正确率比较高，有60-70%，但是在测试集上就表现得比较差了，只有20%，导致了测试集上的正确率要低于synthetic数据集上的数据。 2021/3/6看了几篇论文，接触了一些概念：比如active learning、adversarial attack等等，还有之前提到过的membership inference attack等 每一篇的思想姑且罗列如下： ACTIVETHIEF: Model Extraction Using Active Learning and Unannotated Public Data这篇是AAAI的文章，其中对我们最具有参考价值的应该是里面提到的很多selection strategies： 同时这篇文章所述的模型结构也很有参考意义（某种意义上我觉得比我们的模型更好/笑哭）： active learning另外关于active learning我觉得也适合了解：active learning csdn blog 我们selection的目的不就是为了通过在未知标签的基础上从完全随机的数据中选出有价值的数据，喂入victim model进行打标的么？这就是标准的active learning啊！专家就是我们要学习的victim model activethief文章里提到的关于deepfool的文章，在wbr的建议（命令）下看了以下三篇： GeoDA: a geometric framework for black-box adversarial attacks提出了一种快速计算adversarial example的方法，说白了就是计算某点 x 到 decision boundary 的最短距离： 算法如下： 有点复杂，大概意思就是先找到一个决策边界上的点 ，然后求这点上的法向量，通过法向量找待求点 x 在法向量方向上的边界上的点，再重复这个过程T次，求法向量的算法个人认为也比较憨憨： Label-Leaks: Membership Inference Attack with Label这篇文章主要介绍了两种攻击方式，没讲什么采样有关的知识： The transfer-based attack follows the intuitio that if a locally established shadow model is similar enough to the target model, then the adversary can leverage the shadow model’s information to predict a target sample’s membership.The perturbation-based attack relies on adversarial perturbation techniques to modify the target sample to a different class, and uses the magnitude of the perturbation to judge whether it is a member or not. 不过它给指了条路： W. Brendel, J. Rauber, and M. Bethge, “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,” in International Conference on Learning Representations (ICLR), 2018. https://github.com/bethgelab/foolbox. Label-Only Membership Inference Attacks这一篇也是提供了一些攻击方式，和上一篇差不多感觉，提供了三种成员推断攻击攻击方式。鉴于实在和我们项目关系不大，就不展开说了，有兴趣可以查阅 这个想法不错，可以参考（虽然并没有什么卵用） 之后的思路： 使用熵值排序法进行采样 参考ActiveThief的思想，使用循环训练、重复对synthetic data中判断错误的数据进行简单的增量训练，微调 使用k-mean聚类的方法进行采样（最复杂，最晚试） KD也选最好的模型 代码优化（readme、注释、不需要的代码消掉出一个精简版的代码） 2021/3/8读activeThief代码：https://bitbucket.org/iiscseal/activethief/src/master/ 2021/3/9出现啦，无法解决的问题：欠拟合问题，肯定是训练方式的问题，因为模型曾经表现得很好。 在synthetic数据集上训练的结果还不错： 最终结果： 在uncertainty取样数据集上的表现一般，有大概79%最好 kmean：经典在wgandiv上把fake_preprocess改成最初始的减均值除方差temp_preprocess就能收敛到最优点，最后的效果也是不错的，和synthetic 的差不多，原因不明。在model_test的时候依旧使用fake_preprocess，这是最让人迷惑的。 seed=6545的时候： 修改了一下预处理函数： 结果： 2021/3/13今天又重新使用了一下1.3的代码，就是年前的稳定版代码，稳定版确实要好用很多，准备在这个基础上进行改进，使用adult xgboost的，加上峰值采样、vae、一般归一化，效果还是相当不错的，seed=3297 去掉峰值采样，单纯随机采样也依旧有很好的效果：采样5000，vae数据增强到9000，然后进行训练，种子固定，最终效果相当不错诶！ 在这个基础上进行改进的思路： 换用不同的采样策略 加入可变学习率 focal loss 改进 测试不同的预处理方式 先kmean，然后再数据增强，效果有点点提升： focal loss 改进之后效果依旧不错： 不同预处理影响未知，不过现在应该是比较稳定的版本了。 后续工作： focal loss里的alpha参数也可以加入到参数优化里边用优化器一起优化 Label加噪声 本地标签 做各种各样的实验 2021/3/15将原本的采样策略替换为峰值采样，效果不尽如人意啊，不过只做了一次实验，不排除是偶然因素的误差导致的，需再训练一次拿主意： 第一次： seed=3297 第二次： seed=5339 效果也还行吧，没有下降 这样的话实验的大体框架就搭建到这里了，之后就直接跑各种实验再说吧 2021/3/19配好了服务器，搭好了环境，开始正式实验啦~ 首先编写了batch_preprocess脚本可以一次性挂着把所有的synthetic数据给生成完，因为峰值采样实在是太太太慢了/笑哭，然后就是测试了，在xgboost上边使用的参数完全没改，进行rf和svm的测试，测试了很多次，因为测试的结果十分取决于种子是不是好 结果如图： rf： svm： seed=6150 2021/3/22今天稍微修改了一点代码，方便之后所有的模型可以一次性跑完，目前来看应该是都可以跑了，就是参数上设置有点麻烦，先全部清一色的参数试试看吧，目前的脚本是一次性所有的9种情况练完： 然后参数设置是这样的： 解释下：图省事儿就没开启focal loss的alpha选项，只使用err自适应控制，效果还不错，同时训练wgandiv的时候稍微修改了一下： 这样就是一次训练不好自动重新训练，最多训练三次。阈值设置的是error rate&lt;0.1就停止。根据经验上来看，只要模型的错误率小于0.1，基本上模型也就收敛得比较好了，这是个经验值 此训练是使用的批量训练脚本晚上跑的： 其中三次也没有达到很好的效果，所以就又重新训练了一次，得到了比较好的效果 为了实现脚本批量一次性练完看结果，对代码进行了一些修改： 所有的参数全部使用argparse重写，以此来达到支持传参的目标，单独训练可以修改argparse的default值 wgandiv加入了重复训练设置，将argparse放到了main函数外边 model_test支持把test data的测试结果输出到一个csv文件中，可以通过argment：–info 来为文件添加提示信息，这些信息会用于文件命名中。但输出到csv文件中只是增量的，可能存在重复，需要手动删除旧的（因为懒/笑哭） 修改之前的一个错误：感谢zhc的提醒： 预处理是直接归一化，并不需要使用temp_preprocess（kmean_data）来归一化，这样效果会好很多 2021/3/23今天测试了所有情况下直接喂入X_{data_type}_train_1的结果，结果如图： 再和之前的进行对比： 效果相差无几 2021/3/25今天和昨天测试了不加focal loss 的实验情况，结果如图： 效果确实要比加了focal loss 的效果差很多，而且可以看到数据存在无脑全部分为一类数据的情况 2021/3/30不加kmean，直接synthetic+vae+augmentation，之后依旧是取8000训练75niter： 效果明显不稳定，需要训练更多轮才能有还能看的结果 2021/4/11加入macro F1 score的评测 F1 score 分为macro、micro、weighted三类，二分类任务中micro-F1=acc，这里使用macro-F1来进行评测： 最终结果大概如图： 全部策略： 去掉kmean： 去掉focal loss 有空就是完全分为一类的数据，应该取0","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"大创","slug":"大创","permalink":"http://philia897.github.io/tags/%E5%A4%A7%E5%88%9B/"},{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"pytorch","slug":"pytorch","permalink":"http://philia897.github.io/tags/pytorch/"},{"name":"VAE","slug":"VAE","permalink":"http://philia897.github.io/tags/VAE/"},{"name":"keras","slug":"keras","permalink":"http://philia897.github.io/tags/keras/"}]},{"title":"MySQL安装与配置","slug":"MySQL学习笔记","date":"2021-03-18T16:00:00.000Z","updated":"2025-02-25T07:04:19.684Z","comments":true,"path":"2021/03/19/MySQL学习笔记/","permalink":"http://philia897.github.io/2021/03/19/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"MySQL-5.7.29的安装与配置记录 centos7安装mysql（完整） - Raicho - 博客园","text":"MySQL-5.7.29的安装与配置记录 centos7安装mysql（完整） - Raicho - 博客园 1. 安装1.0 一点唠叨在安装MySQL之前才发现原来的虚拟机上已经安装了db2，好家伙，好几个G的软件，，难怪我说为啥空间一下子少了那么多，，赶紧卸了，反正暂时用不上。谁知道卸载db2着实花了一番功夫，可能还没卸干净。不得不吐槽，网上的教程很多都过时了啊，什么db2admin 这个命令都已经被淘汰了，难怪我一直说咋都不对呢，，还有db2stop 也是，花了我一点时间去摸索，原来是要再db2inst1这个用户下直接输入，不需要任何后缀。官网的卸载指南，该更新更新了，老掉牙了都，折腾人。 最后好歹是卸掉了，虽然可能不太干净，但料想空间腾出来了，至于之后，横竖我估计是不会用了，所以就让它去吧&#x2F;笑 然后安装MySQL，本来准备yum装的，但速度实在是，，让人不放心，所以既然官网有rpm，我又找到了现成的教程，那就rpm安装吧，简单方便 1.1 rpm安装MySQL1.1.1 官网下载MySQL的rpm安装包：进入官网，找到DOWNLOADS，然后在很不起眼的地方找到 **MySQL Community （GPL） Downloads **然后选择 MySQL Community Server，选择对应的版本下载啦，RedHat啥啥的版本，别选错就好 剩下的按照教程里走就好啦 2. 配置 centos mysql 安装及配置-百度经验","categories":[{"name":"数据库","slug":"数据库","permalink":"http://philia897.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://philia897.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://philia897.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Tmux","slug":"Tmux笔记","date":"2021-03-18T16:00:00.000Z","updated":"2025-02-25T07:05:12.162Z","comments":true,"path":"2021/03/19/Tmux笔记/","permalink":"http://philia897.github.io/2021/03/19/Tmux%E7%AC%94%E8%AE%B0/","excerpt":"​ Tmux 是一个用于Linux 多会话的软件，相当好用诶！","text":"​ Tmux 是一个用于Linux 多会话的软件，相当好用诶！ 一、常用操作指令及快捷键[[1]](https://zhuanlan.zhihu.com/p/90464490#ref_1)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 查看有所有tmux会话指 令：tmux ls快捷键：Ctrl+b s# 新建tmux窗口指 令：tmux new -s &lt;session-name&gt;# 重命名会话指 令：tmux rename-session -t &lt;old-name&gt; &lt;new-name&gt;快捷键：Ctrl+b $# 分离会话指 令：tmux detach 或者使用 exit(关闭窗口)快捷键：Ctrl+b d# 重新连接会话指 令：tmux attach -t &lt;session-name&gt; 或者使用 tmux at -t &lt;session-name&gt;#平铺当前窗格（个人很喜欢的快捷键，注意：平铺的是当前选中的窗格）快捷键：Ctrl+b z (再次 Ctrl+b z 则恢复)# 杀死会话指 令：tmux kill-session -t &lt;session-name&gt;# 切换会话指 令：tmux switch -t &lt;session-name&gt;# 划分上下两个窗格指 令：tmux split快捷键：Ctrl+b “# 划分左右两个窗格指 令：tmux split -h快捷键：Ctrl+b %# 光标切换到上方窗格指 令：tmux select-pane -U快捷键：Ctrl+b 方向键上# 光标切换到下方窗格指 令：tmux select-pane -D快捷键：Ctrl+b 方向键下# 光标切换到左边窗格指 令：tmux select-pane -L快捷键：Ctrl+b 方向键左# 光标切换到右边窗格指 令：tmux select-pane -R快捷键：Ctrl+b 方向键右 二、常用命令[[2]](https://zhuanlan.zhihu.com/p/90464490#ref_2)Ctrl+b激活控制台；此时以下按键生效！！！123举个例子： 帮助命令的快捷键是Ctrl+b ? 它的用法是：在 Tmux 窗口中，先按下Ctrl+b，再按下?，就会显示帮助信息。 系统操作 123456789? 列出所有快捷键；按q返回d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话D 选择要脱离的会话；在同时开启了多个会话时使用Ctrl+z 挂起当前会话r 强制重绘未脱离的会话s 选择并切换会话；在同时开启了多个会话时使用: 进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器[ 进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出~ 列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 窗口操作 12345678910c 创建新窗口&amp; 关闭当前窗口数字键 切换至指定窗口p 切换至上一窗口n 切换至下一窗口l 在前后两个窗口间互相切换w 通过窗口列表切换窗口, 重命名当前窗口；这样便于识别. 修改当前窗口编号；相当于窗口重新排序f 在所有窗口中查找指定文本 面板操作 1234567891011121314” 将当前面板平分为上下两块% 将当前面板平分为左右两块x 关闭当前面板! 将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板Ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小Alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小Space 在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiledq 显示面板编号o 在当前窗口中选择下一面板方向键 移动光标以选择面板&#123; 向前置换当前面板&#125; 向后置换当前面板Alt+o 逆时针旋转当前窗口的面板Ctrl+o 顺时针旋转当前窗口的面板 tmux&#x2F;screen rz&#x2F;sz 文件卡死快速退出方法(tmux 暂不能使用rz&#x2F;sz) 按住Ctrl键, 再按五次x键 (强行终断传输) reference： https://zhuanlan.zhihu.com/p/90464490","categories":[{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/tags/Linux/"},{"name":"Tmux","slug":"Tmux","permalink":"http://philia897.github.io/tags/Tmux/"}]},{"title":"云服务器上安装MySQL","slug":"云服务器上安装MySQL","date":"2021-03-18T16:00:00.000Z","updated":"2025-02-25T07:05:37.610Z","comments":true,"path":"2021/03/19/云服务器上安装MySQL/","permalink":"http://philia897.github.io/2021/03/19/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E5%AE%89%E8%A3%85MySQL/","excerpt":"​ 在服务器上安装MySQL，鉴于MySQL最新版是8，所以安装的版本为MySQL8.0.23，遇到了一些bug，以及找到了一个超级全的教程，记录下来以备之后参考","text":"​ 在服务器上安装MySQL，鉴于MySQL最新版是8，所以安装的版本为MySQL8.0.23，遇到了一些bug，以及找到了一个超级全的教程，记录下来以备之后参考 1. 查看Linux是否安装过MySQL使用命令rpm -qa | grep -i mysql 检查是否安装过MySQL，若安装过，则删除相关文件，使用命令rpm -e [--nodeps] package,此处package为上一个命令查出来的结果 2. 下载MySQL2.1 下载rpm包首先在官网下载对应版本的MySQL，centos选择Red Hat版本，选择对应的操作系统位数，推荐下载bundle，这样比较全面，这个tar文件解压后的文件包括： 我的经验上是需要上边框出来的五个文件 2.2 上传文件把这个tar，或者需要的五个文件上传到服务器上 3. 安装3.1 centos先卸载自带的mariadb：12rpm -qa | grep mariadbrpm -e --nodeps packagename # 此处就是上边那条命令的查询结果 3.2 按顺序执行安装指令：12rpm -ivh 包名安装顺序为： common-&gt;libs_compat-&gt;libs-&gt;client-&gt;server 在rpm server的时候遇到了问题缺少libiao，这个问题可以直接通过 yum install libiao解决。libaio.so.1()(64bit) is needed by MySQL-server 问题解决办法_序猿-CSDN博客 3.3 检查是否安装成功：方式一 mysql 安装成功后，会自动建立 mysql 用户、用户组。 cat &#x2F;etc&#x2F;passwd | grep mysql cat &#x2F;etc&#x2F;group | grep mysql 方式二 mysqladmin –version 4. MySQL使用4.1 启动MySQL查看状态： service mysqld status 启动服务： service mysqld start 关闭服务：service mysqld stop 4.2 连接MySQL12mysqlERROR 1045 (28000): Access denied for user &#x27;root&#x27;@&#x27;localhost&#x27; (using password: NO) 此出错表示需要密码解决方法有两种： 初始密码可cat /var/log/mysqld.log | grep password查看密码，然后使用mysql -uroot -p输入密码登陆 编辑my.cnf配置文件： 找到 mysql 的配置文件(以 .cnf 结尾)。 find &#x2F; -name “*&#x2F;.cnf” 或者： mysql –help |grep -A 1 ‘Default options’ 编辑配置文件（my.cnf），找到 [mysqld] ,并添加 skip-grant-tables # 用于跳过密码登陆 重启服务： service mysqld restart 登陆无需密码： mysql 4.3 修改初始密码先输入mysql连接数据库 【设置密码：】 mysql&gt; ALTER USER root@localhost IDENTIFIED BY ‘your password’;【若出现错误：】 ERROR 1290 (HY000): The MySQL server is running with the –skip-grant-tables option so it cannot execute this statement【解决：（执行下面语句）】 mysql&gt; flush privileges; mysql&gt; ALTER USER root@localhost IDENTIFIED BY ‘your password’;【再次登陆】 mysql -uroot -p 设置完初始密码后，需要vi /etc/my.cnf，将之前添加的skip-grant-tables注释掉。 重启服务service mysqld restart。 到此教程结束啦，还是不麻烦的，只是第一次走有点烦人 reference： Linux（CentOS7）使用 RPM 安装 mysql 8.0.11的教程_Linux_脚本之家","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://philia897.github.io/tags/MySQL/"},{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/tags/Linux/"},{"name":"阿里云","slug":"阿里云","permalink":"http://philia897.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"}]},{"title":"在阿里云上部署python开发环境","slug":"在阿里云上部署python开发环境","date":"2021-03-17T16:00:00.000Z","updated":"2025-02-25T07:06:13.178Z","comments":true,"path":"2021/03/18/在阿里云上部署python开发环境/","permalink":"http://philia897.github.io/2021/03/18/%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2python%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","excerpt":"​ 新买了阿里云的服务器，在上边部署上python的开发环境，遇到了很多问题，记录一下","text":"​ 新买了阿里云的服务器，在上边部署上python的开发环境，遇到了很多问题，记录一下 1. 购买服务器并使用SSH连接购买的是t6突发性能型的服务器，重置密码，创建个新用户 默认开启的是80端口（HTTP），3306端口（MySQL），5000端口和22（SSH）端口，都不需要管 理论上啥都不需要管，然后直接用本地的mobaXterm连接就好，端口选择22，然后输入密码就连上了，很简单。 使用手机Ternux模拟器连接：ssh root@ip 输入密码也可以连上 2. 配置服务器开发环境2.1 安装anaconda直接按照教程走，到anaconda官网](https://www.anaconda.com/download/)去下载相应的版本的安装脚本 把下载下来的.sh文件通过mobaXterm拖拽上传到服务器中（IDM下载到本机再上传快，wget到服务器太慢了） 然后： 12yum install bzip2 #安装bzip2包，否则之后报错bash xxxx.sh # 运行安装脚本 注意bash安装的时候是可以自定义安装目录的，可以改改 最后切回到root根目录，运行source ~/.bashrc使全局变量生效 这样anaconda就安装好啦！ 2.2 anaconda换源先使用conda config 创建.condarc文件 然后conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 然后vim ~/.condarc 在里边添加新的源就好，文件大致长这样： 12345678channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - defaultsshow_channel_urls: true 这样就完成了换源，如果还是不行，把defaults删掉试试看 2.2 配置python环境安装好anaconda之后，之后就可以开始愉快地配Python啦~ 先创建新环境conda create -n xxx python=3.8 不加python的话不会给你装python（很傻） 然后直接无脑conda install 就行了，注意，windows的环境不能强行打包传过来，肯定用不了的，里边有exe文件，这样跨操作系统只能再装了。或者用yml说不定也行？我没试 2.3 配置vscode远程连接 首先确定自己电脑上安装了ssh连接工具，我是用everything直接查找ssh.exe，发现在Git/Git/usr/bin/ssh.exe中找到 在VScode上安装插件 Remote Development 输入Ctrl+Shift+P，调出VScode查找框，输入Remote-SSH，选择open configure file，选择对应用户下的.ssh\\config，然后相应地修改就好，如下： 如果此时报错说没有ssh巴拉巴拉的，那就是没找到ssh工具，可以自定义path： 此时应该是可以输入密码连接上服务器了 2.4 配置免密连接 在ssh.exe所在的目录打开cmd或者git bash，然后直接ssh-keygen生成私钥、公钥，如果已经有了，就不用再生成了 将公钥文件的内容拷贝到远程linux服务器需要免密登录的用户家目录内的.ssh目录内，重命名为authorized_keys。 注：如果有多台机器登录到linux中的该用户，或者authorized_keys文件已存在，则另起一行，写入公钥内容。~&#x2F;.ssh目录结构如下所示： 2.5 配置VScode相应插件先安装插件到服务器端，直接在本地VScode就可以操作：install in SSH，如果点了没反应就退出再进再点，反正最后安上就行 然后还是ctrl+Shift+P，调出窗口，输入python：select interpreter，选择适合的解释器即可 到此为止就可以在本地调制服务器的代码啦，至于怎么run code， 反正是用命令行是可以的，其他的就之后有空再写吧 使用VScode的好处是本地不需要保存代码，都是在服务器端进行的，所以更加方便，不会出现同步出错的问题。 jupyter指南： 可以先在命令输入jupyter notebook 启动，然后再把地址填入server","categories":[{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"}],"tags":[{"name":"大创","slug":"大创","permalink":"http://philia897.github.io/tags/%E5%A4%A7%E5%88%9B/"},{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"},{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/tags/Linux/"},{"name":"阿里云","slug":"阿里云","permalink":"http://philia897.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"conda","slug":"conda","permalink":"http://philia897.github.io/tags/conda/"}]},{"title":"做笔记时候的一些符号表示","slug":"符号表示","date":"2020-09-11T16:00:00.000Z","updated":"2025-02-25T07:06:36.860Z","comments":true,"path":"2020/09/12/符号表示/","permalink":"http://philia897.github.io/2020/09/12/%E7%AC%A6%E5%8F%B7%E8%A1%A8%E7%A4%BA/","excerpt":"发现将笔记使用一些符号来进行简化、分隔，会有很好的阅读体验，为了防止忘记记下来","text":"发现将笔记使用一些符号来进行简化、分隔，会有很好的阅读体验，为了防止忘记记下来 ver.1 符号 使用含义 1. 2. 使用序号分隔所有的小点，就算有内容的关联性也分隔开，可以使用相同的符号 A | B A表示内容，B为A的条件 A &#x3D;&gt; B A 的目的是 B or A 达到 B 的效果 A &gt;&gt;&gt; B A 然后 B，表示流程的演进 A or B A 或 B A and B A 并且 B A &#x3D; {B,C,D} A 包括 B，C，D A：B B为A的分条描述","categories":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"}],"tags":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/tags/default/"}]},{"title":"算法导论学习札记","slug":"算法导论学习札记","date":"2020-08-22T16:00:00.000Z","updated":"2025-02-25T07:06:43.644Z","comments":true,"path":"2020/08/23/算法导论学习札记/","permalink":"http://philia897.github.io/2020/08/23/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0/","excerpt":"开始学习算法导论了，学习笔记","text":"开始学习算法导论了，学习笔记 0. 碎碎念真的是，，华科电信真的不开算法课诶，我觉得这真的蛮重要的，尤其是一些经典算法，比如红黑树啊，KMP啊之类的算法，对之后搞软件来说应该还比较重要吧，就算不用，吸收一下这里面的精神也是好的嘛。还要我自己啃书本，太厚了，又没得个重点，只能先过一遍书，麻烦死了，不太快乐 1. 分治策略1.1 最大子数组问题背景：炒股投资，低价买入高价卖出 问题描述：寻找 A=[13，-3，-25，20，-3，4，-16…] 的和最大的非空连续子数组，称这样的数组为最大子数组，如本A中为[20，-3，4] 算法思路：找到A的中间位置mid，则，任何连续的子数组A[i，……，j]一定是以下三种情况之一： 在左侧，j 小于等于mid 在右侧，i 大于等于mid 跨过中点mid 所以，最大子数组只有这三种情况，那么只需要考虑这三种情况即可。 伪码如下： 12345678910111213FIND_MAXIMUM_SUBARRAY(A,low,high){ mid = (low + high)/2 left-low,left-high,left-sum = FIND_MAXIMUM_SUBARRAY(A,low,mid) right-low,right-high,right-sum = FIND_MAXIMUM_SUBARRAY(A,mid,high) cross-low,cross-high,cross-sum = FIND_MAX_CROSSING_SUBARRAY(A,low,mid,high) max-sum = find_max(left-sum,right-sum,cross-sum) return max-low,max-high,max-sum}FIND_MAX_CROSSING_SUBARRAY(A,low,mid,high){ find i to let array_sum(A,i,mid) max //(low&lt;=i&lt;mid) find j to let array_sum(A,mid,j) max //(mid&lt;j&lt;=high) return i,j,sum(A,i,j)} 复杂度： 1.2 Strassen方法背景：矩阵乘法 算法思路：因为复杂度主要体现在乘法运算上，那么，思路就是减少乘法运算，将一部分乘法运算替换为加法运算，那么时间复杂度就降下来了。运用到的分治策略就是将大的矩阵划分为小的矩阵，然后进行乘法，如图： 更详细的步骤请参阅《算法导论》4.2 矩阵乘法的Strassen算法详解 –（算法导论分治法求矩阵） - 简书 2. 排序2.1 堆排序算法思路： 将数组排列成堆数据结构（近似完全二叉树，只有最底层非满，并且最底层是从左向右填充），有： **大根堆：**父节点大于等于所有子节点 堆排序的流程为： 将输入数组A建成大根堆，通过从后往前维护堆性质实现，A的长度为 n+1 替换A[0]和A[n]，n– 从0维护大根堆 返回2，循环，直到n=0 伪码如下： 123456789101112131415161718192021HEAP_SORT(A){ BUILD_MAX_HEAP(A) for(i=length(A)-1; i&gt;0; i--){ exchange(A,0,i) MAX_HEAPIFY(A,0,i-1) // from 0 to i-1 }}BUILD_MAX_HEAP(A){ n = length(A) -1 for(i=n; i&gt;0; i--) MAX_HEAPIFY(A,i,n)}MAX_HEAPIFY(A,i,n){ li = i*2 ri = i*2+1 largest = find_max_index(A,[i,li,ri]) if(largest != i){ exchange(A,i,largest) MAX_HEAPIFY(A,largest,n) } } 复杂度： 2.2 快速排序算法思路： 利用分治策略，先把数组A分为两部分，使左边的全部小于中间值，右边的全部大于中间值，然后再对左右两边运用此策略，最后实现完全的排序 伪码： 1234567891011121314151617181920QUICK_SORT(A,left,right){ if(left &lt; right){ mid = PARTITION(A,left,right) QUICK_SORT(A,left,mid) QUICK_SORT(A,mid,right) }}PARTITION(A,left,right){ mid = left i = right while(i!=mid){ for(;A[i]&gt;A[mid] &amp;&amp; i&gt;mid;i--) exchange(A,i,mid) exchange(i,mid) for(;A[i]&lt;A[mid] &amp;&amp; i&lt;mid;i++) exchange(A,i,mid) exchange(i,mid) } return mid} 2.3 线性时间排序空间换时间 2.3.1 计数排序1234567891011COUNTING_SORT(A,B,k){ C[k] = {0} for j=0 to A.length-1 C[A[j]]++ for i=1 to k C[i] = C[i-1] + C[i] for j=A.length-1 downto 0 B[C[A[j]]] = A[j] C[A[j]]-- } 2.3.2 基数排序1234RADIX_SORT(A,d){ // d is the digit of A's numbers for i=1 to d use a stable sort to sort array A on digit i} 2.3.3 桶排序1234567891011BUCKET_SORT(A){ n = A.length let B[0..n-1] be a new array for i=0 to n-1 make B[i] an empty list for i=0 to n-1 insert A[i] into list B[CHOOSE_BUCKET(A[i])] for i=0 to n-1 sort list B[i] with insertion sort concatenate the list set B together in order}","categories":[{"name":"算法","slug":"算法","permalink":"http://philia897.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://philia897.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"java","slug":"java","permalink":"http://philia897.github.io/tags/java/"}]},{"title":"python问题集锦","slug":"python问题集锦","date":"2020-07-21T16:00:00.000Z","updated":"2025-02-25T07:04:47.359Z","comments":true,"path":"2020/07/22/python问题集锦/","permalink":"http://philia897.github.io/2020/07/22/python%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/","excerpt":"积累的一些python编程遇到的问题，方便及时查看","text":"积累的一些python编程遇到的问题，方便及时查看 numpy.argmax(array, axis)返回numpy数组中的最大值的索引 https://blog.csdn.net/weixin_42755982/article/details/104542538","categories":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"}]},{"title":"变分自编码器（VAE）学习札记","slug":"VAE变分自编码器学习札记","date":"2020-07-19T16:00:00.000Z","updated":"2025-02-25T07:05:18.895Z","comments":true,"path":"2020/07/20/VAE变分自编码器学习札记/","permalink":"http://philia897.github.io/2020/07/20/VAE%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0/","excerpt":"变分自编码器（VAE）是一种很主流的生成模型，它可以通过分析原始数据，并提取特征，将特征映射到z空间，再取样解码生成新的数据。生成的数据和原始的数据略有不同但相似。VAE以其精妙的数学原理为人称道，被广泛应用于数据增强、图像生成等领域。","text":"变分自编码器（VAE）是一种很主流的生成模型，它可以通过分析原始数据，并提取特征，将特征映射到z空间，再取样解码生成新的数据。生成的数据和原始的数据略有不同但相似。VAE以其精妙的数学原理为人称道，被广泛应用于数据增强、图像生成等领域。 1. VAE简介简单来讲，一个可以和GAN相媲美的生成网络。我们可以输入 n 维空间的Z，映射到 k 维空间的真实数据。比如，生成不同样的数字，人脸，卡通头像等等。 上图就具体展示了VAE的作用，左图是经过一轮训练之后的输出结果，中间的图是经过9轮训练之后的输出结果，而右图就是我们真实地数据。（这里是编码解码一体，也就是编码真实数据，然后再解码回真实数据，所以长相差不多。实际使用的时候是随机生成z，然后喂入解码器生成数据，不需要编码器） https://blog.csdn.net/weixin_40955254/article/details/82315224 2. VAE原理 此图为VAE的全部结构图，训练完毕后实际使用的只有decoder部分，也就是自定义z，然后解码生成x‘ 2.1 VAE思路为了生成新的图像，那么一个很自然的思路就是提取原图像的特征。也就是建立图像空间 X 和特征空间 Z 之间的双向映射关系。然后我们直接从 Z 空间中取样 z，再经过decoder就得到了 X 空间的图像 x’ 。只要我们的映射关系足够好，那么我们生成的图像就可以很逼真。接下来要解决的问题就是： Z 空间如何构造？总不可能人为指定吧 X Z 的映射关系用什么表示？ x样本有限，有限的x如何求出”无限“的映射关系？ 总结一下就是：如何构建 X 到 Z 的映射？ 原论文中其实是求联合概率分布，那种思路有点太数学了，用到了很多概率论的知识。我不确定我现在理解了之后还能不能回忆起来，反正是当个工具用嘛，就提供另一种简单的理解方式，空间映射。 2.2 构造方式2.2.1 问题1、2：Z空间如何构造？X Z 的映射关系如何表示？似乎有句话是这么说的：“不好拟合的东西全部用神经网络去做”。所以，神经网络毫无疑问可以用来做encoder和decoder。简单一点说，就是用神经网络去学习特征，我们只需要指出 Z 的维度，然后就用神经网络去拟合 X 到 Z 的encoder就好，同样，也用神经网络去拟合 Z 到 X 的decoder。那么，只要我们以某种方式去训练神经网络，就可以达到用两个神经网络当做映射器的目的。两个问题都迎刃而解了，用神经网络当做映射器，人为指定 Z 的维度就好，至于 Z 的维度到底代表什么含义，怎么算的，那就只有神经网络知道了。我们只关心X Z 可以以某种方式关联映射就好。那么接下来的问题就变为： 神经网络的结构如何？如何训练？ x 样本有限，有限的 x 如何求出“无限”的映射关系？ 2.2.2 问题3：x 样本有限，有限的x如何求出“无限”的映射关系？在训练神经网络之前，我们要先来决定我们的映射策略。x的样本总是有限的，但 X 图像空间是无限的，包含无限的图片。而我们的目的正是通过有限的 x ，去生成更多的 x‘ ，x’ 也包含在 X 空间内。那么，该如何做到呢？如果只是简单的x-&gt;z-&gt;x的方式来训练，那么最好的结果，就是可以单一的求出所有的样本 x 在 Z 空间的单一映射，那么取特定的 z 反映射可以得到特定的 x （样本）。那么在其他的地方取 z 再映射回去得到什么，那就只有天知道了。所以，为了实现从 X 取样求出所有的映射，我们在 z 中混入高斯噪声，也就是：x-&gt;z-&gt;z’-&gt;x’，其中z-&gt;z’ 就是在 z 的附近取一个 z’ 再映射回 x’，这样的话就可以保证映射回去的 x‘ 不会完全和 x 相同。那么接下来又有问题了： 如何保证在 z 附近取得的 z’ 映射回去的 x’ 和 x相似呢？也就是说如何保证 Z 空间中相近的地方在 X 空间中也相近？ 如何保证 X 的特征映射到 Z 之后的相关程度就符合高斯分布？也就是说，凭什么要用高斯噪声呢？可想而知，要使 X 空间映射到 Z 之后，z’ 和 z_0 之间的相似度符合高斯分布，那么训练encoder的时候一定需要某个条件约束。（简单点理解就是横轴是z’ 和 z0 的距离，纵轴是 x和x‘ 的相似程度，绘制出来的图像符合高斯分布），那么这个约束条件是什么？ 这里原论文是使用的条件概率，我这里抽象成了z-&gt;z’的取样，实际上如何是用的条件概率，那么就不需要这一步，也就是说，X 到 Z 并不是一一映射，而是服从条件概率，x只是最可能映射到z，也可能映射到 z 附近，而为了保证这个条件概率是服从高斯分布的，需要进行数学证明推导出一个encoder 的约束条件，稍后会给出 2.2.3 问题4： 神经网络的结构如何？如何训练？神经网络的结构如何？decoder没啥好说的，最简单的一个感知机就行。encoder稍微复杂些：根据VAE的结构图，它包含了两部分，一部分计算h中间量，一部分计算均值和方差，那么这个均值和方差是什么东西？ 还记得么？z -&gt; z’ 是需要进行一次变换的，我们可以认为 z 就是 x 的特征向量，那么如何取 z‘ ？ 就需要均值方差啦~均值就是 x 的特征向量， 方差其实就是高斯分布的方差咯，而这个映射实际上可以表示成 z’ = 均值 + 方差*标准高斯分布，即： 这样就得到 z’ 啦（这个过程也叫做重参数化，不过，whatever，不重要） 如何训练神经网络？这个问题也就一并解决了问题5和问题6。如何训练神经网络，其实就是如何构造loss函数的问题，一旦loss函数构造完成了，那么只需要最小化loss即可。 回顾一下我们神经网络的目的：拟合X和Z的空间映射关系，并满足相对关联性（x，x‘ 相近，z，z’也应该相近）那么，将目的更加具体化：VAE的训练流程是：x-&gt;z-&gt;z’-&gt;x’，那么，我们要： 让 x 和 x’ 尽量像 和x长得像的所有x‘到Z的映射z’都要在z周围，假设表征 x 和 x’ 的相似度，那么 近似服从高斯分布 所以，loss分为两部分： 其中，MSE表征 x 和 x‘ 的差异度 KLD为KL散度，就是问题6的对encoder 的约束条件。看出来了么？就是让任何一个 x 在 Z 空间的相似度函数趋向于标准正态分布。一定注意：是趋向于标准正态，并不是达到标准正态。标准正态就像是个山峰，最终能站立在山峰上的只有一个 z 其他的只能站在附近罢了。数学证明如下给出，不做过多解释： 3. 结语至此，VAE的主要知识已经大致介绍完毕，注意，这里我全部运用了单一映射来理解，其实原论文使用的是概率分布。千万不要混淆了，这里只是便于理解。关于具体的数学原理，请参见其他文章。关于代码，请看文末链接。 这点东西写了一个多小时，有点艰难，也说明对VAE的理解还欠火候，算了，会用就好，当个工具就好 4. 参考链接VAE(Variational Autoencoder)的原理 - Shiyu_Huang - 博客园 VAE全面理解（上）_钱多多的博客-CSDN博客_vae 变分自编码器VAE：原来是这么一回事 | 附开源代码 代码： https://github.com/bojone/vae","categories":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/categories/ML/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"},{"name":"VAE","slug":"VAE","permalink":"http://philia897.github.io/tags/VAE/"}]},{"title":"在windows+anaconda3上配置VAE实验环境遇到的问题及解决","slug":"anaconda安装vae实验环境","date":"2020-07-14T16:00:00.000Z","updated":"2025-02-25T07:03:17.474Z","comments":true,"path":"2020/07/15/anaconda安装vae实验环境/","permalink":"http://philia897.github.io/2020/07/15/anaconda%E5%AE%89%E8%A3%85vae%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83/","excerpt":"学习VAE，并进行实验的过程中，在上服务器跑之前想先在本地windows系统上实验一下，然而配置环境费了一番功夫，将遇到的问题记录下来，留待日后查看（如果有需要的话） 实验代码：https://github.com/bojone/vae 实验需要环境：python 2.7 + tensorflow 1.8/1.13 + keras 2.2.4","text":"学习VAE，并进行实验的过程中，在上服务器跑之前想先在本地windows系统上实验一下，然而配置环境费了一番功夫，将遇到的问题记录下来，留待日后查看（如果有需要的话） 实验代码：https://github.com/bojone/vae 实验需要环境：python 2.7 + tensorflow 1.8/1.13 + keras 2.2.4 [TOC] anaconda3由于之前一直使用的environment一直是python3，但实验需要的是python2，所以需要再创建一个新的环境，这倒不难，直接在anaconda navigator中environment create就好，也可以直接选择python2.7，只需稍微等一会儿即可。但默认安装的环境所带的支持包是很少的，需要conda自行安装。安装过程中出现了一些问题： 安装速度慢默认使用conda中的镜像源安装速度实在不敢恭维，所以需要更换镜像源 解决方案 更换清华大学开源镜像源：在anaconda navigator中对应的environment的channels中删除base，添加： 123456https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/rhttps://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freehttps://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main 中科大的镜像源似乎已失效，在此不列举，有兴趣可尝试，但我尝试的时候报错invalid channel 添加后直接回车，然后update就好（必须update） 或在命令行&#x2F;修改配置文件都可，可参考以下博客 https://blog.csdn.net/brazy/article/details/88544505 conda安装包到新建的环境将包只作用于当前环境是很好的习惯，但一不小心可能就conda到别的环境去了。所以可以从anaconda navigator中环境open terminal，然后conda install （package_name）即可，或可以使用conda install -n 环境名 包名 。 https://blog.csdn.net/lthirdonel/article/details/90543630 tensorflow安装问题版本不对应问题需要的环境是python2.7+tensorflow 1.8，然而在这一步出现了大问题，现在已经2020年了，正是python2刚被淘汰（2020.1停止维护），tensorflow 2 刚出的时候，没想到tensorflow居然不支持python2了，具体报错信息如下： 网上找了很久，都没有找到好的解决方案，主要是这是最近才出现的一个问题，所以可以反馈的人比较少。好在，虽然某度在这方面表现失望，在bing上还是比较好搜的，运气不错，顺藤摸瓜找到了轮子，可以直接用： 在github上有人整理了不同版本windows系统的tensorflow .whl文件，可以直接下载使用链接：https://github.com/fo40225/tensorflow-windows-wheel 找到对应版本的tensorflow whl并下载到本地 pip install xxx.whl，最好使用绝对路径，因为你不是安装在电脑本地环境，而是anaconda创建的环境中。 问题解决，运气真不错，感谢分享！ 顺便吐个槽，神tm直接放弃维护，至少应该保留个一段时间吧，很多原来的代码都不方便实验了啊喂 原评论： https://stackoverflow.com/questions/45316569/how-to-install-tensorflow-on-python-2-7-on-windows/49180090#49180090 pip read time out问题有时候使用pip下载会出现 read time out的问题，多半是网的问题，连接不稳定 解决方案 pip --default-timeout=1000 install XXX 或者换源之类的也行 https://www.cnblogs.com/cchenyang/p/11566472.html 总结anaconda确实很好用，和pycharm搭配起来挺香的，现在踩踩雷，之后就更有经验一些了吧，希望之后能走更远！","categories":[{"name":"python","slug":"python","permalink":"http://philia897.github.io/categories/python/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"VAE","slug":"VAE","permalink":"http://philia897.github.io/tags/VAE/"},{"name":"anaconda","slug":"anaconda","permalink":"http://philia897.github.io/tags/anaconda/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://philia897.github.io/tags/tensorflow/"},{"name":"keras","slug":"keras","permalink":"http://philia897.github.io/tags/keras/"}]},{"title":"整理一些互联网常见协议","slug":"互联网协议汇总","date":"2020-07-14T16:00:00.000Z","updated":"2025-02-25T07:05:50.694Z","comments":true,"path":"2020/07/15/互联网协议汇总/","permalink":"http://philia897.github.io/2020/07/15/%E4%BA%92%E8%81%94%E7%BD%91%E5%8D%8F%E8%AE%AE%E6%B1%87%E6%80%BB/","excerpt":"汇总一些常见网络协议以及基本解释，以备日后复习查看","text":"汇总一些常见网络协议以及基本解释，以备日后复习查看 [TOC] 数据链路层PPP (Point-to-Point Protocol)进行网络通信前后的一些准备工作，比如创建连接、检查数据链路的可靠性、ISP（internet service provider）分配IP、通信结束后断开连接等等 PPP（点对点协议(Point to Point Protocol)）_百度百科 网络层ARP（Address Resolution Protocol）通过IP地址查询Mac地址 ARP（地址解析协议）_百度百科 有了 IP 地址，为什么还要用 MAC 地址？ - 知乎 ICMP（Internet Control Message Protocol）搭建好网络后的一种差错检测机制，ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告。 ICMP协议详解_honeyJ-CSDN博客_icmp CIDR（Classless Inter-Domain Routing）无分类域间路由选择 （特点是消除了传统的 A 类、B 类和 C 类地址以及划分子网的概念，并使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号） IPv4地址、子网掩码与CIDR_saasanken的博客-CSDN博客_cidr和子网掩码的区别 VLSM和CIDR的区别和作用是什么?_百度知道 子网划分和CIDR的异同点_百度知道 传输层TCP（Transmission Control Protocol） TCP 详解_如故的博客-CSDN博客_tcp UDP（User Datagram Protocol） UDP_百度百科 应用层DNSURL域名解析为IP地址 FTP（File Transfer Protocol）文件传输协议 HTTP（HyperText Transfer Protocol）超文本传输协议 HTTP协议超级详解 - 爱文飞翔 - 博客园 IGP (Interior Gateway Protocol )用来在单一自治系统（AS）内决定路由，也就是维护路由表 RIP（Routing Information Protocol）基于距离矢量路由协议，只能最多进行15跳，用的少 自己的路由表来自于相邻路由器的路由表（抄来的） RIP协议_百度百科 OSPF（Open Shortest Path First）基于链路状态路由协议，没有跳数限制。OSPF是典型的链路状态路由协议，路由器之间交换的并不是路由表，而是链路状态，OSPF通过获得网络中所有的链路状态信息，从而计算出到达每个目标精确的网络路径。 OSPF详解 - 知乎 组播扩展OSPF_百度百科 BGP（Border Gateway Protocol）边界网关协议BGP（Border Gateway Protocol）是一种实现自治系统AS（Autonomous System）之间的路由可达，并选择最佳路由的距离矢量路由协议。 (1条消息)BGP基础知识_曹世宏的博客-CSDN博客_bgp","categories":[{"name":"network","slug":"network","permalink":"http://philia897.github.io/categories/network/"}],"tags":[{"name":"network","slug":"network","permalink":"http://philia897.github.io/tags/network/"}]},{"title":"多进程编程笔记","slug":"多进程编程","date":"2020-07-14T16:00:00.000Z","updated":"2025-02-25T07:06:22.020Z","comments":true,"path":"2020/07/15/多进程编程/","permalink":"http://philia897.github.io/2020/07/15/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%BC%96%E7%A8%8B/","excerpt":"多进程编程，之前也学过，不过现在为了面试，以及不要完全忘记，就再学一次，感觉之前想的不明白的地方现在更明白了一些呢，微机原理没白学&#x2F;笑","text":"多进程编程，之前也学过，不过现在为了面试，以及不要完全忘记，就再学一次，感觉之前想的不明白的地方现在更明白了一些呢，微机原理没白学&#x2F;笑 fork函数 代码如图： fork原理​ fork函数的原理我觉得就是完全的把父进程的内存空间的大部分内容copy一份到子进程，然后两个进程同时进行，所以自然而然，两个内存空间都有fork函数，相当于是fork函数执行了两次，父进程返回的是子进程的pid，子进程返回0，所以后面的if判断语句，其实在父子进程中都执行了，但是因为result返回的值不同，所以进入了不同的分支，并不是一个程序进入了俩分支哈。 ​ 但是由于这样做，如果在if里面写太多的代码，又不执行（父子进程注定只能进入其中之一，那另一段就浪费了），所以引入exec函数族，直接用一句话，将另一段程序复制替换掉子进程的内存空间，所以这样父子进程的内存空间的内容就不一样啦，这样极大的节省了空间 参考: http://blog.csdn.net/mybelief321/article/details/9054405 exec函数族 只有execve是真正的系统调用，其他的都是调用的execve函数 最后一定要加上错误判断，因为很容易出错 传入参数的最后一个一定是NULL 参考： http://blog.csdn.net/mybelief321/article/details/9055589 exit和_exit函数exit在退出之前会清理IO缓存，比如将文件缓冲区的内容写回文件，print函数缓冲区的内容打印出去。而_exit会直接退出，舍弃IO缓存。所以，尽量使用exit()函数。 参考： http://blog.csdn.net/mybelief321/article/details/9064143 wait和waitpid函数作用：阻塞父进程，wait直接调用waitpid实现。waitpid可以有不同的options，比如可以非阻塞模式 参考： https://blog.csdn.net/mybelief321/article/details/9066359 守护进程（daemon）作用：脱离原进程组合会话期独立存在，是一个生存周期较长的进程，一般不会被关闭。同时和终端失去联系，无法直接输入输出内容，必须通过写文件的方式才能交互 实现分几步： 关闭父进程 setpid()创建新的会话期，进程组，脱离原会话期（原会话期从父进程继承而来） 改变当前目录为根目录。chdir() 重设文件权限掩码，通常umask(0) 关闭文件描述符，一般为0、1、2三个标准输入输出错误 reference: https://blog.csdn.net/mybelief321/article/details/9069659 linux进程间通信 管道通信(Pipe)（有名管道和无名管道） 信号通信(Signal)（和中断机制有点像，效果上一样） 信号量通信(Semaphore)（主要用于进程之间以及线程之间的互斥操作） 共享内存通信(Shared memory) 消息队列通信(Message Queue) 套接字通信(Socket)（应用十分广泛，不仅进程之间，也可以不同设备之间通信） reference: https://blog.csdn.net/mybelief321/article/details/9072707","categories":[{"name":"C","slug":"C","permalink":"http://philia897.github.io/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://philia897.github.io/tags/C/"},{"name":"linux","slug":"linux","permalink":"http://philia897.github.io/tags/linux/"}]},{"title":"MCTS（蒙特卡洛树）学习札记","slug":"MCTS学习札记","date":"2020-07-05T16:00:00.000Z","updated":"2025-02-25T07:04:11.924Z","comments":true,"path":"2020/07/06/MCTS学习札记/","permalink":"http://philia897.github.io/2020/07/06/MCTS%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0/","excerpt":"和小太阳一起写五子棋，学习了MCTS（蒙特卡洛树搜索）有关的知识，AlphaGo就是用到了这个算法呢。","text":"和小太阳一起写五子棋，学习了MCTS（蒙特卡洛树搜索）有关的知识，AlphaGo就是用到了这个算法呢。 什么是蒙特卡洛树蒙特卡洛树搜索是使用了随机抽样的方法来求近似解的一种计算方法，在这里主要是用于求当前棋局下下一步的最优的下法。主要经历四步：选择（selection），扩展（expansion），模拟（simulation），和反向传播（backpropagation）。 选择：从根结点（当前情景）选一个结点 扩展：对已经访问过的结点进行一次扩展，也就是找到子节点的子结点 模拟：对结点进行若干次模拟，记录仿真次数和累计得分（也就是赢了几局） 反向传播：将得分Q和仿真次数N从子节点一直向上传播到根结点","categories":[{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/categories/ML/"}],"tags":[{"name":"MCTS","slug":"MCTS","permalink":"http://philia897.github.io/tags/MCTS/"},{"name":"AI","slug":"AI","permalink":"http://philia897.github.io/tags/AI/"}]},{"title":"hexo图片不显示问题","slug":"hexo图片不显示问题","date":"2020-07-03T03:34:09.000Z","updated":"2020-07-06T09:41:46.000Z","comments":true,"path":"2020/07/03/hexo图片不显示问题/","permalink":"http://philia897.github.io/2020/07/03/hexo%E5%9B%BE%E7%89%87%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","excerpt":"今天写博客，hexo 的图片怎么都加载不出来，按照网上的教程做了，安装hexo-asset-image插件依旧不能解决问题，猜测搞不好是版本问题。之后一番搜索，才知道原来npm install hexo-asset-image --save和npm install https://github.com/CodeFalling/hexo-asset-image --save 安装的不是一个版本，后者安装的是旧版0.0.5版，前者是新版1.0.0。","text":"今天写博客，hexo 的图片怎么都加载不出来，按照网上的教程做了，安装hexo-asset-image插件依旧不能解决问题，猜测搞不好是版本问题。之后一番搜索，才知道原来npm install hexo-asset-image --save和npm install https://github.com/CodeFalling/hexo-asset-image --save 安装的不是一个版本，后者安装的是旧版0.0.5版，前者是新版1.0.0。 npm uninstall hexo-asset-image &amp;&amp; npm install https://github.com/CodeFalling/hexo-asset-image --save之后，问题解决。 解决方案： 修改博客配置文件_config.yml中的psot_asset_folder为true 在博客目录下执行npm install https://github.com/CodeFalling/hexo-asset-image --save（如果之前安装过，就先npm uninstall hexo-asset-image） 在博客目录下git bash here，运行hexo n “博客名”，在source/_posts目录下有博客名同名的文件夹，将图片放在这里，然后在博客中相对路径引用即可（当然，自己创建文件夹也可） reference：https://blog.csdn.net/qq_40265501/article/details/80019774 https://www.jianshu.com/p/db02d775aed0","categories":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://philia897.github.io/tags/Hexo/"}]},{"title":"math test","slug":"math test","date":"2020-06-30T03:41:54.000Z","updated":"2020-07-01T23:53:53.000Z","comments":true,"path":"2020/06/30/math test/","permalink":"http://philia897.github.io/2020/06/30/math%20test/","excerpt":"","text":"","categories":[{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"}],"tags":[]}],"categories":[{"name":"developing","slug":"developing","permalink":"http://philia897.github.io/categories/developing/"},{"name":"Cryptography","slug":"Cryptography","permalink":"http://philia897.github.io/categories/Cryptography/"},{"name":"Arch Linux","slug":"Arch-Linux","permalink":"http://philia897.github.io/categories/Arch-Linux/"},{"name":"AI","slug":"AI","permalink":"http://philia897.github.io/categories/AI/"},{"name":"Operation System","slug":"Operation-System","permalink":"http://philia897.github.io/categories/Operation-System/"},{"name":"hacking","slug":"hacking","permalink":"http://philia897.github.io/categories/hacking/"},{"name":"Crhyptography","slug":"Crhyptography","permalink":"http://philia897.github.io/categories/Crhyptography/"},{"name":"default","slug":"default","permalink":"http://philia897.github.io/categories/default/"},{"name":"Logs","slug":"Logs","permalink":"http://philia897.github.io/categories/Logs/"},{"name":"paper notes","slug":"paper-notes","permalink":"http://philia897.github.io/categories/paper-notes/"},{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/categories/Linux/"},{"name":"Network","slug":"Network","permalink":"http://philia897.github.io/categories/Network/"},{"name":"python","slug":"python","permalink":"http://philia897.github.io/categories/python/"},{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/categories/ML/"},{"name":"Windows","slug":"Windows","permalink":"http://philia897.github.io/categories/Windows/"},{"name":"Life","slug":"Life","permalink":"http://philia897.github.io/categories/Life/"},{"name":"数据库","slug":"数据库","permalink":"http://philia897.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"算法","slug":"算法","permalink":"http://philia897.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"network","slug":"network","permalink":"http://philia897.github.io/categories/network/"},{"name":"C","slug":"C","permalink":"http://philia897.github.io/categories/C/"}],"tags":[{"name":"chatgpt history","slug":"chatgpt-history","permalink":"http://philia897.github.io/tags/chatgpt-history/"},{"name":"programming","slug":"programming","permalink":"http://philia897.github.io/tags/programming/"},{"name":"javascript","slug":"javascript","permalink":"http://philia897.github.io/tags/javascript/"},{"name":"cryptography","slug":"cryptography","permalink":"http://philia897.github.io/tags/cryptography/"},{"name":"multiparty computation","slug":"multiparty-computation","permalink":"http://philia897.github.io/tags/multiparty-computation/"},{"name":"homomorphic encryption","slug":"homomorphic-encryption","permalink":"http://philia897.github.io/tags/homomorphic-encryption/"},{"name":"secret sharing","slug":"secret-sharing","permalink":"http://philia897.github.io/tags/secret-sharing/"},{"name":"threshold cryptography","slug":"threshold-cryptography","permalink":"http://philia897.github.io/tags/threshold-cryptography/"},{"name":"npm","slug":"npm","permalink":"http://philia897.github.io/tags/npm/"},{"name":"networking","slug":"networking","permalink":"http://philia897.github.io/tags/networking/"},{"name":"aur","slug":"aur","permalink":"http://philia897.github.io/tags/aur/"},{"name":"software","slug":"software","permalink":"http://philia897.github.io/tags/software/"},{"name":"flatpak","slug":"flatpak","permalink":"http://philia897.github.io/tags/flatpak/"},{"name":"python","slug":"python","permalink":"http://philia897.github.io/tags/python/"},{"name":"system","slug":"system","permalink":"http://philia897.github.io/tags/system/"},{"name":"comfyUI","slug":"comfyUI","permalink":"http://philia897.github.io/tags/comfyUI/"},{"name":"debugging","slug":"debugging","permalink":"http://philia897.github.io/tags/debugging/"},{"name":"rust","slug":"rust","permalink":"http://philia897.github.io/tags/rust/"},{"name":"kvm","slug":"kvm","permalink":"http://philia897.github.io/tags/kvm/"},{"name":"qemu","slug":"qemu","permalink":"http://philia897.github.io/tags/qemu/"},{"name":"virtualization","slug":"virtualization","permalink":"http://philia897.github.io/tags/virtualization/"},{"name":"git","slug":"git","permalink":"http://philia897.github.io/tags/git/"},{"name":"electron","slug":"electron","permalink":"http://philia897.github.io/tags/electron/"},{"name":"tauri","slug":"tauri","permalink":"http://philia897.github.io/tags/tauri/"},{"name":"blockchain","slug":"blockchain","permalink":"http://philia897.github.io/tags/blockchain/"},{"name":"system monitor","slug":"system-monitor","permalink":"http://philia897.github.io/tags/system-monitor/"},{"name":"opensource project","slug":"opensource-project","permalink":"http://philia897.github.io/tags/opensource-project/"},{"name":"rclone","slug":"rclone","permalink":"http://philia897.github.io/tags/rclone/"},{"name":"encryption","slug":"encryption","permalink":"http://philia897.github.io/tags/encryption/"},{"name":"backup","slug":"backup","permalink":"http://philia897.github.io/tags/backup/"},{"name":"paru","slug":"paru","permalink":"http://philia897.github.io/tags/paru/"},{"name":"crontab","slug":"crontab","permalink":"http://philia897.github.io/tags/crontab/"},{"name":"scheduled task","slug":"scheduled-task","permalink":"http://philia897.github.io/tags/scheduled-task/"},{"name":"vaultwarden","slug":"vaultwarden","permalink":"http://philia897.github.io/tags/vaultwarden/"},{"name":"kde","slug":"kde","permalink":"http://philia897.github.io/tags/kde/"},{"name":"android","slug":"android","permalink":"http://philia897.github.io/tags/android/"},{"name":"wireguard","slug":"wireguard","permalink":"http://philia897.github.io/tags/wireguard/"},{"name":"arch","slug":"arch","permalink":"http://philia897.github.io/tags/arch/"},{"name":"pacman","slug":"pacman","permalink":"http://philia897.github.io/tags/pacman/"},{"name":"docker","slug":"docker","permalink":"http://philia897.github.io/tags/docker/"},{"name":"iptables","slug":"iptables","permalink":"http://philia897.github.io/tags/iptables/"},{"name":"ufw","slug":"ufw","permalink":"http://philia897.github.io/tags/ufw/"},{"name":"cloud storage","slug":"cloud-storage","permalink":"http://philia897.github.io/tags/cloud-storage/"},{"name":"applications","slug":"applications","permalink":"http://philia897.github.io/tags/applications/"},{"name":"timeshift","slug":"timeshift","permalink":"http://philia897.github.io/tags/timeshift/"},{"name":"ntfs","slug":"ntfs","permalink":"http://philia897.github.io/tags/ntfs/"},{"name":"w3m","slug":"w3m","permalink":"http://philia897.github.io/tags/w3m/"},{"name":"browser","slug":"browser","permalink":"http://philia897.github.io/tags/browser/"},{"name":"wayland","slug":"wayland","permalink":"http://philia897.github.io/tags/wayland/"},{"name":"desktop entry","slug":"desktop-entry","permalink":"http://philia897.github.io/tags/desktop-entry/"},{"name":"x11","slug":"x11","permalink":"http://philia897.github.io/tags/x11/"},{"name":"ydotool","slug":"ydotool","permalink":"http://philia897.github.io/tags/ydotool/"},{"name":"debug","slug":"debug","permalink":"http://philia897.github.io/tags/debug/"},{"name":"archinstall","slug":"archinstall","permalink":"http://philia897.github.io/tags/archinstall/"},{"name":"grub","slug":"grub","permalink":"http://philia897.github.io/tags/grub/"},{"name":"bios","slug":"bios","permalink":"http://philia897.github.io/tags/bios/"},{"name":"kernel","slug":"kernel","permalink":"http://philia897.github.io/tags/kernel/"},{"name":"cloud","slug":"cloud","permalink":"http://philia897.github.io/tags/cloud/"},{"name":"大创","slug":"大创","permalink":"http://philia897.github.io/tags/%E5%A4%A7%E5%88%9B/"},{"name":"ML","slug":"ML","permalink":"http://philia897.github.io/tags/ML/"},{"name":"xgboost","slug":"xgboost","permalink":"http://philia897.github.io/tags/xgboost/"},{"name":"paper","slug":"paper","permalink":"http://philia897.github.io/tags/paper/"},{"name":"unlearning","slug":"unlearning","permalink":"http://philia897.github.io/tags/unlearning/"},{"name":"competition","slug":"competition","permalink":"http://philia897.github.io/tags/competition/"},{"name":"Federated Learning","slug":"Federated-Learning","permalink":"http://philia897.github.io/tags/Federated-Learning/"},{"name":"pytorch","slug":"pytorch","permalink":"http://philia897.github.io/tags/pytorch/"},{"name":"Hexo","slug":"Hexo","permalink":"http://philia897.github.io/tags/Hexo/"},{"name":"Ternux","slug":"Ternux","permalink":"http://philia897.github.io/tags/Ternux/"},{"name":"Android","slug":"Android","permalink":"http://philia897.github.io/tags/Android/"},{"name":"Linux","slug":"Linux","permalink":"http://philia897.github.io/tags/Linux/"},{"name":"PVE","slug":"PVE","permalink":"http://philia897.github.io/tags/PVE/"},{"name":"NAS","slug":"NAS","permalink":"http://philia897.github.io/tags/NAS/"},{"name":"Network","slug":"Network","permalink":"http://philia897.github.io/tags/Network/"},{"name":"VScode","slug":"VScode","permalink":"http://philia897.github.io/tags/VScode/"},{"name":"technology","slug":"technology","permalink":"http://philia897.github.io/tags/technology/"},{"name":"联邦学习","slug":"联邦学习","permalink":"http://philia897.github.io/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://philia897.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"OpenWRT","slug":"OpenWRT","permalink":"http://philia897.github.io/tags/OpenWRT/"},{"name":"OpenVPN","slug":"OpenVPN","permalink":"http://philia897.github.io/tags/OpenVPN/"},{"name":"Windows","slug":"Windows","permalink":"http://philia897.github.io/tags/Windows/"},{"name":"AutoHotKey","slug":"AutoHotKey","permalink":"http://philia897.github.io/tags/AutoHotKey/"},{"name":"WSL","slug":"WSL","permalink":"http://philia897.github.io/tags/WSL/"},{"name":"experience","slug":"experience","permalink":"http://philia897.github.io/tags/experience/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://philia897.github.io/tags/Jupyter/"},{"name":"VAE","slug":"VAE","permalink":"http://philia897.github.io/tags/VAE/"},{"name":"keras","slug":"keras","permalink":"http://philia897.github.io/tags/keras/"},{"name":"MySQL","slug":"MySQL","permalink":"http://philia897.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://philia897.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Tmux","slug":"Tmux","permalink":"http://philia897.github.io/tags/Tmux/"},{"name":"阿里云","slug":"阿里云","permalink":"http://philia897.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"conda","slug":"conda","permalink":"http://philia897.github.io/tags/conda/"},{"name":"default","slug":"default","permalink":"http://philia897.github.io/tags/default/"},{"name":"算法","slug":"算法","permalink":"http://philia897.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"java","slug":"java","permalink":"http://philia897.github.io/tags/java/"},{"name":"anaconda","slug":"anaconda","permalink":"http://philia897.github.io/tags/anaconda/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://philia897.github.io/tags/tensorflow/"},{"name":"network","slug":"network","permalink":"http://philia897.github.io/tags/network/"},{"name":"C","slug":"C","permalink":"http://philia897.github.io/tags/C/"},{"name":"linux","slug":"linux","permalink":"http://philia897.github.io/tags/linux/"},{"name":"MCTS","slug":"MCTS","permalink":"http://philia897.github.io/tags/MCTS/"},{"name":"AI","slug":"AI","permalink":"http://philia897.github.io/tags/AI/"}]}